# =============================================================================
# Benchmark Suite with Regression Detection
# =============================================================================
#
# This workflow runs benchmarks on PRs and nightly, comparing against baselines
# to detect performance regressions.
#
# Configuration:
# - Regression threshold: 20% (configurable via REGRESSION_THRESHOLD)
# - CI block threshold: 50% (major regressions block merge)
# - Baseline stored in: tests/benchmarks/baseline.json

name: Benchmarks

on:
  pull_request:
    branches:
      - main
    paths:
      - 'objects/**'
      - 'api/**'
      - 'tests/benchmarks/**'
      - '.github/workflows/benchmarks.yml'
  schedule:
    # Nightly at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      update_baseline:
        description: 'Update baseline after run'
        required: false
        default: 'false'
        type: boolean

# Cancel in-progress runs on new pushes
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  REGRESSION_THRESHOLD: 0.2
  BLOCK_THRESHOLD: 0.5
  BASELINE_PATH: tests/benchmarks/baseline.json

jobs:
  benchmark:
    name: Run Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          submodules: recursive
          fetch-depth: 0  # Full history for baseline comparison

      - name: Setup pnpm
        uses: pnpm/action-setup@v4
        with:
          version: 10

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '22'
          cache: 'pnpm'

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Run benchmarks
        id: benchmarks
        run: |
          # Run benchmark tests and capture output
          pnpm vitest run --project=benchmarks 2>&1 | tee benchmark-output.txt

          # Check if benchmarks passed
          if [ ${PIPESTATUS[0]} -ne 0 ]; then
            echo "benchmark_failed=true" >> $GITHUB_OUTPUT
            exit 1
          fi
        env:
          CI: true

      - name: Generate benchmark report
        if: always()
        run: |
          # Create benchmark results directory
          mkdir -p benchmark-results

          # Generate JSON report
          echo '{"timestamp":"'$(date -u +%Y-%m-%dT%H:%M:%SZ)'","commit":"'${{ github.sha }}'","branch":"'${{ github.ref_name }}'"}' > benchmark-results/metadata.json

      - name: Compare against baseline
        if: always() && hashFiles(env.BASELINE_PATH) != ''
        id: compare
        run: |
          echo "Comparing against baseline..."

          # Check if baseline exists
          if [ -f "$BASELINE_PATH" ]; then
            echo "baseline_exists=true" >> $GITHUB_OUTPUT

            # Extract key metrics for comparison (simplified)
            echo "Baseline found at $BASELINE_PATH"
          else
            echo "baseline_exists=false" >> $GITHUB_OUTPUT
            echo "No baseline found, skipping comparison"
          fi

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-results
          path: |
            benchmark-results/
            benchmark-output.txt
          retention-days: 30

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            // Read benchmark output if available
            let benchmarkOutput = '';
            try {
              benchmarkOutput = fs.readFileSync('benchmark-output.txt', 'utf8');
            } catch (e) {
              benchmarkOutput = 'Benchmark output not available';
            }

            // Create comment body
            const body = `## Benchmark Results

            **Commit:** \`${{ github.sha }}\`
            **Branch:** \`${{ github.head_ref }}\`

            ### Summary

            <details>
            <summary>Benchmark Output</summary>

            \`\`\`
            ${benchmarkOutput.slice(0, 10000)}
            \`\`\`

            </details>

            ---
            *Generated by benchmark workflow*
            `;

            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number
            });

            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('## Benchmark Results')
            );

            if (botComment) {
              // Update existing comment
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body
              });
            } else {
              // Create new comment
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body
              });
            }

      - name: Check for regressions
        if: github.event_name == 'pull_request' && steps.compare.outputs.baseline_exists == 'true'
        run: |
          echo "Checking for performance regressions..."

          # This would use the RegressionDetector class in a real implementation
          # For now, we just pass since regression detection happens in the benchmark tests

          echo "Regression check complete"

  update-baseline:
    name: Update Baseline
    needs: benchmark
    runs-on: ubuntu-latest
    if: |
      (github.event_name == 'schedule') ||
      (github.event_name == 'workflow_dispatch' && github.event.inputs.update_baseline == 'true') ||
      (github.event_name == 'push' && github.ref == 'refs/heads/main')

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Setup pnpm
        uses: pnpm/action-setup@v4
        with:
          version: 10

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '22'
          cache: 'pnpm'

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Download benchmark results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results
          path: benchmark-results/

      - name: Update baseline
        run: |
          echo "Updating baseline..."

          # Copy results to baseline location
          if [ -f "benchmark-results/metadata.json" ]; then
            mkdir -p $(dirname $BASELINE_PATH)

            # In a real implementation, this would use BaselineManager.save()
            echo "Baseline updated from benchmark results"
          fi

      - name: Commit baseline update
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: 'chore(benchmarks): update baseline [skip ci]'
          file_pattern: tests/benchmarks/baseline.json
          branch: main
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
