---
title: anthropic.do
description: Claude on the Edge — A fully managed Anthropic service running on Cloudflare Workers with caching, rate limiting, and multi-tenant support.
---

# anthropic.do

**Claude on the Edge** — A managed Anthropic service that runs entirely on Cloudflare Workers, with built-in caching, rate limiting, usage tracking, and conversation memory.

```typescript
import { Claude } from 'anthropic.do'

const claude = new Claude('https://your-worker.workers.dev')

// Simple chat
const response = await claude.chat('What is the capital of France?')
console.log(response.text)
// "The capital of France is Paris."

// With conversation memory
const session = claude.session('user-123')
await session.chat('My name is Alice')
await session.chat('What is my name?') // Remembers context
```

<Callout type="info">
Looking for direct API access? See [@dotdo/anthropic](/docs/integrations/anthropic/package) for a drop-in replacement SDK.
</Callout>

## Why anthropic.do?

Traditional AI integrations require managing API keys, implementing caching, handling rate limits, and tracking usage. anthropic.do handles all of that:

- **Zero Key Management** — API keys stored securely in Workers secrets
- **Global Edge Caching** — Identical requests served from cache, reducing costs
- **Automatic Rate Limiting** — Per-tenant limits with graceful degradation
- **Usage Tracking** — Token usage per tenant, per model, per day
- **Conversation Memory** — Persistent chat sessions in Durable Objects
- **Serverless Economics** — Pay only for actual AI usage

## Features

### Core AI

| Feature | Description |
|---------|-------------|
| **Messages API** | Full Anthropic Messages API compatibility |
| **Streaming** | Real-time streaming responses |
| **Tool Use** | Function calling with automatic execution |
| **Vision** | Image analysis with Claude's vision models |
| **All Models** | Access to all Claude models (Opus, Sonnet, Haiku) |

### Edge Features

| Feature | Description |
|---------|-------------|
| **Response Caching** | Cache identical requests at the edge |
| **Rate Limiting** | Per-tenant, per-minute/hour/day limits |
| **Usage Tracking** | Token usage analytics and billing data |
| **Conversation Memory** | Persistent sessions in Durable Objects |
| **Multi-Tenant** | Isolated tenants with separate limits and usage |

### Connectivity

| Feature | Description |
|---------|-------------|
| **HTTP/REST** | Standard REST API with JSON |
| **WebSocket** | Persistent connections for streaming |
| **Service Bindings** | Zero-latency Worker-to-Worker calls |
| **MCP Protocol** | Model Context Protocol for AI agents |

## Installation

```bash
npm install anthropic.do
```

## Quick Start

### Deploy to Cloudflare Workers

```typescript
// src/index.ts
import { AnthropicEntrypoint, AnthropicService } from 'anthropic.do'

export { AnthropicService }
export default AnthropicEntrypoint
```

```jsonc
// wrangler.jsonc
{
  "name": "my-anthropic-do",
  "main": "src/index.ts",
  "compatibility_date": "2025-01-01",
  "compatibility_flags": ["nodejs_compat"],
  "durable_objects": {
    "bindings": [{ "name": "ANTHROPIC_SERVICE", "class_name": "AnthropicService" }]
  },
  "migrations": [{ "tag": "v1", "new_sqlite_classes": ["AnthropicService"] }],
  "vars": {
    "ANTHROPIC_API_KEY": "sk-ant-xxx"
  }
}
```

```bash
# Set your API key securely
npx wrangler secret put ANTHROPIC_API_KEY

# Deploy
npx wrangler deploy
```

### Client Usage

```typescript
import { Claude } from 'anthropic.do'

const claude = new Claude('https://your-anthropic.workers.dev')

// Simple message
const response = await claude.messages.create({
  model: 'claude-sonnet-4-20250514',
  max_tokens: 1024,
  messages: [{ role: 'user', content: 'Hello!' }],
})

console.log(response.content[0].text)
```

## Examples

### Response Caching

Cache identical requests to reduce costs and latency:

```typescript
// First request - hits Anthropic API
const response1 = await claude.messages.create({
  model: 'claude-sonnet-4-20250514',
  max_tokens: 1024,
  messages: [{ role: 'user', content: 'What is 2+2?' }],
}, { cache: true, cacheTtl: 3600 })

// Second identical request - served from cache
const response2 = await claude.messages.create({
  model: 'claude-sonnet-4-20250514',
  max_tokens: 1024,
  messages: [{ role: 'user', content: 'What is 2+2?' }],
}, { cache: true })

console.log(response2.cached) // true
```

### Conversation Sessions

Persistent conversation memory across requests:

```typescript
const session = claude.session('user-123')

// Messages are automatically accumulated
await session.chat('My favorite color is blue')
await session.chat('I live in San Francisco')

// Claude remembers the full conversation
const response = await session.chat('What do you know about me?')
// "You mentioned your favorite color is blue and you live in San Francisco."

// Session persists in Durable Object storage
// Resume later with the same session ID
```

### Multi-Tenant Usage

Isolate tenants with separate limits and tracking:

```typescript
// Configure per-tenant limits
const claude = new Claude('https://your-anthropic.workers.dev', {
  tenant: 'acme-corp',
  rateLimits: {
    requestsPerMinute: 60,
    requestsPerDay: 10000,
    tokensPerDay: 1000000,
  },
})

// Usage is tracked per tenant
const usage = await claude.getUsage('acme-corp')
console.log(usage)
// {
//   today: { requests: 150, inputTokens: 50000, outputTokens: 25000 },
//   thisMonth: { requests: 3200, inputTokens: 1200000, outputTokens: 600000 },
// }
```

### Rate Limiting

Automatic rate limiting with graceful degradation:

```typescript
try {
  const response = await claude.messages.create({
    model: 'claude-sonnet-4-20250514',
    max_tokens: 1024,
    messages: [{ role: 'user', content: 'Hello!' }],
  })
} catch (error) {
  if (error.code === 'RATE_LIMITED') {
    console.log('Rate limited, retry after:', error.retryAfter)
    // Automatic retry-after header
  }
}
```

### Streaming Responses

Real-time streaming for chat interfaces:

```typescript
const stream = await claude.messages.create({
  model: 'claude-sonnet-4-20250514',
  max_tokens: 1024,
  messages: [{ role: 'user', content: 'Write a story' }],
  stream: true,
})

for await (const event of stream) {
  if (event.type === 'content_block_delta') {
    process.stdout.write(event.delta.text)
  }
}
```

### Tool Use with Auto-Execution

Define tools that execute automatically:

```typescript
const response = await claude.messages.create({
  model: 'claude-sonnet-4-20250514',
  max_tokens: 1024,
  tools: [
    {
      name: 'get_weather',
      description: 'Get weather for a location',
      input_schema: {
        type: 'object',
        properties: {
          location: { type: 'string' },
        },
        required: ['location'],
      },
      // Auto-execute handler
      execute: async (input) => {
        const weather = await fetchWeather(input.location)
        return { temperature: weather.temp, conditions: weather.conditions }
      },
    },
  ],
  messages: [{ role: 'user', content: 'What is the weather in NYC?' }],
})

// Tool is automatically called and result incorporated
console.log(response.content[0].text)
// "The weather in New York City is 72F and sunny."
```

### MCP Protocol Integration

Connect to AI agents using Model Context Protocol:

```typescript
import { createMcpServer, createAnthropicAdapter } from 'anthropic.do/mcp'

const server = createMcpServer({
  resources: [
    {
      name: 'database',
      description: 'Query the application database',
      handler: async (query) => {
        return await db.query(query)
      },
    },
  ],
})

// Use with Anthropic Claude
const adapter = createAnthropicAdapter({ server })
const tools = await adapter.getTools()
```

## Architecture

```
+--------------------------------------------------------------------------+
|                         Client Applications                               |
+----------+----------+----------+------------------------------------------+
|   REST   |WebSocket |  Service | MCP Protocol                             |
|   API    |Streaming | Binding  | AI Agents                                |
+----------+----------+----------+------------------------------------------+
|                      anthropic.do Worker (Edge)                           |
+--------------------------------------------------------------------------+
|  Request Router | Rate Limiter | Cache Layer | Usage Tracker              |
+--------------------------------------------------------------------------+
|                    Durable Objects (Session Storage)                      |
+----------------------------------+---------------------------------------+
|         Anthropic API            |        Edge Cache (KV)                |
|    (Claude Models)               |   (Response Caching)                  |
+----------------------------------+---------------------------------------+
```

anthropic.do acts as an intelligent proxy:

1. **Request Routing** — Route requests based on tenant, model, and features
2. **Rate Limiting** — Enforce per-tenant limits before hitting Anthropic
3. **Cache Layer** — Serve cached responses for identical requests
4. **Usage Tracking** — Record all token usage for billing and analytics
5. **Session Storage** — Persist conversation state in Durable Objects

## Configuration

### Basic Setup

```jsonc
// wrangler.jsonc
{
  "name": "my-anthropic-do",
  "main": "src/index.ts",
  "compatibility_date": "2025-01-01",
  "durable_objects": {
    "bindings": [
      { "name": "ANTHROPIC_SERVICE", "class_name": "AnthropicService" },
      { "name": "SESSIONS", "class_name": "SessionStore" }
    ]
  },
  "kv_namespaces": [
    { "binding": "CACHE", "id": "xxx" }
  ]
}
```

### With Custom Rate Limits

```typescript
// src/config.ts
export const config = {
  defaultLimits: {
    requestsPerMinute: 30,
    requestsPerHour: 500,
    requestsPerDay: 5000,
    tokensPerDay: 500000,
  },
  tierLimits: {
    free: {
      requestsPerDay: 100,
      tokensPerDay: 50000,
    },
    pro: {
      requestsPerDay: 10000,
      tokensPerDay: 1000000,
    },
    enterprise: {
      requestsPerDay: Infinity,
      tokensPerDay: Infinity,
    },
  },
  cache: {
    enabled: true,
    defaultTtl: 3600, // 1 hour
    maxSize: '100MB',
  },
}
```

### With Analytics

```typescript
// Export usage to analytics
import { AnthropicService } from 'anthropic.do'

export class MyAnthropicService extends AnthropicService {
  async afterRequest(request, response, usage) {
    // Send to your analytics system
    await this.env.ANALYTICS.writeDataPoint({
      blobs: [request.tenant, request.model],
      doubles: [usage.inputTokens, usage.outputTokens],
      indexes: [request.tenant],
    })
  }
}
```

## API Reference

### Claude Client

```typescript
const claude = new Claude(endpoint, options?)

// Messages API (Anthropic-compatible)
claude.messages.create(params, options?)

// Session management
claude.session(sessionId) // Get persistent session
claude.clearSession(sessionId) // Clear session history

// Usage and analytics
claude.getUsage(tenant?) // Get usage statistics
claude.getRateLimitStatus(tenant?) // Check current limits

// Admin
claude.setRateLimits(tenant, limits) // Update tenant limits
claude.invalidateCache(pattern?) // Clear cached responses
```

### Session

```typescript
const session = claude.session('user-123')

// Chat with memory
session.chat(message, options?)

// Manage history
session.getHistory() // Get conversation history
session.clearHistory() // Clear history
session.setSystemPrompt(prompt) // Set system prompt

// Export/import
session.export() // Export conversation
session.import(data) // Import conversation
```

## Service Bindings (Zero Latency)

Connect from other Workers without HTTP overhead:

```typescript
// In your consuming worker
export default {
  async fetch(request: Request, env: Env) {
    // Zero-latency call to anthropic.do
    const response = await env.ANTHROPIC.messages.create({
      model: 'claude-sonnet-4-20250514',
      max_tokens: 1024,
      messages: [{ role: 'user', content: 'Hello!' }],
    })

    return Response.json(response)
  },
}
```

```jsonc
// wrangler.jsonc for consuming worker
{
  "services": [
    { "binding": "ANTHROPIC", "service": "my-anthropic-do" }
  ]
}
```

## Pricing Optimization

anthropic.do helps optimize your Anthropic costs:

| Strategy | Savings |
|----------|---------|
| Response caching | 50-80% for repetitive queries |
| Prompt caching | 30-50% for similar prompts |
| Model routing | Use Haiku for simple tasks |
| Rate limiting | Prevent runaway costs |
| Usage tracking | Identify optimization opportunities |

### Automatic Model Selection

```typescript
const response = await claude.messages.create({
  // Let anthropic.do choose the best model
  model: 'auto',
  max_tokens: 1024,
  messages: [{ role: 'user', content: 'What is 2+2?' }],
})

// Simple math -> routes to Haiku (cheapest)
// Complex analysis -> routes to Sonnet
// Creative writing -> routes to Opus
```

## Development

```bash
# Install dependencies
npm install

# Run locally
npm run dev

# Run tests
npm test

# Deploy
npm run deploy
```

## Related

- [@dotdo/anthropic](/docs/integrations/anthropic/package) - Drop-in Anthropic SDK
- [OpenAI Integration](/docs/integrations/openai) - OpenAI API compatibility
- [Named Agents](/docs/agents/named-agents) - Priya, Ralph, Tom, Mark, Sally, Quinn
- [Durable Objects](/docs/architecture/durable-objects) - DO-backed storage
