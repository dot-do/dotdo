---
title: clickhouse.do
description: ClickHouse on the Edge - A fully managed OLAP database running on Cloudflare Workers with real-time analytics and time-series support.
---

# clickhouse.do

**ClickHouse on the Edge** - A ClickHouse-compatible OLAP database that runs entirely on Cloudflare Workers, with native time-series support, materialized views, and real-time analytics.

```typescript
import { ClickHouseClient } from 'clickhouse.do'

const client = new ClickHouseClient('https://your-worker.workers.dev')

await client.query(`
  SELECT toStartOfHour(timestamp) as hour, count(*) as events
  FROM events
  WHERE timestamp > now() - INTERVAL 24 HOUR
  GROUP BY hour
  ORDER BY hour
`)
```

<Callout type="info">
Looking for an in-memory implementation for testing? See [@dotdo/clickhouse](/docs/integrations/clickhouse/package) for a zero-dependency analytics engine.
</Callout>

## Why clickhouse.do?

Traditional OLAP databases require dedicated infrastructure, complex scaling, and careful tuning. clickhouse.do eliminates all of that by running directly on Cloudflare's edge network:

- **Zero Infrastructure** - No servers to manage, no cluster configuration, no cold starts
- **Global by Default** - Analytics at the edge, close to your users
- **ClickHouse Compatible** - Drop-in replacement for most ClickHouse operations
- **Time-Series Native** - Built-in support for time-series data, TTL, and retention
- **Serverless Economics** - Pay only for what you use, scale to zero

## Features

### Core Analytics

| Feature | Description |
|---------|-------------|
| **MergeTree Engines** | Full MergeTree family with merge/compaction: `MergeTree`, `ReplacingMergeTree`, `SummingMergeTree`, `AggregatingMergeTree`, `CollapsingMergeTree` |
| **SQL Queries** | Full ClickHouse SQL including JOINs, subqueries, CTEs, window functions, CUBE/ROLLUP |
| **Aggregate Functions** | 100+ functions: count, sum, avg, quantile, uniq, argMax, topK, histogram, and more |
| **Materialized Views** | Persistent pre-aggregated views with automatic refresh |
| **TTL Policies** | Active time-to-live with automatic data expiration |

### Real-Time Features

| Feature | Description |
|---------|-------------|
| **Streaming Inserts** | High-throughput batch inserts with automatic buffering |
| **Live Queries** | Real-time query results via WebSocket subscriptions |
| **CDC Integration** | Change data capture from MongoDB, PostgreSQL via Pipelines |
| **Metrics Collection** | Built-in collectors for application metrics and traces |

### Connectivity

| Feature | Description |
|---------|-------------|
| **Wire Protocol** | Connect with ClickHouse native clients, DBeaver, DataGrip |
| **HTTP Interface** | Standard ClickHouse HTTP API with JSON/TSV/CSV formats |
| **Service Bindings** | Zero-latency Worker-to-Worker communication |
| **Grafana Plugin** | Native Grafana data source for dashboards |

## Installation

```bash
npm install clickhouse.do
```

## Quick Start

### Deploy to Cloudflare Workers

```typescript
// src/index.ts
import { ClickHouseEntrypoint, ClickHouseDatabase } from 'clickhouse.do'

export { ClickHouseDatabase }
export default ClickHouseEntrypoint
```

```jsonc
// wrangler.jsonc
{
  "name": "my-clickhouse.do",
  "main": "src/index.ts",
  "compatibility_date": "2025-01-01",
  "compatibility_flags": ["nodejs_compat"],
  "durable_objects": {
    "bindings": [{ "name": "CLICKHOUSE_DB", "class_name": "ClickHouseDatabase" }]
  },
  "migrations": [{ "tag": "v1", "new_sqlite_classes": ["ClickHouseDatabase"] }]
}
```

```bash
npx wrangler deploy
```

### Local Development

```bash
# Start a local server
npx clickhouse.do serve --port 8123

# Connect with clickhouse-client
clickhouse-client --host localhost --port 8123
```

## Examples

### Time-Series Analytics

```typescript
// Create time-series table with TTL
await client.query(`
  CREATE TABLE metrics (
    timestamp DateTime,
    host String,
    metric String,
    value Float64
  ) ENGINE = MergeTree()
  ORDER BY (metric, host, timestamp)
  TTL timestamp + INTERVAL 30 DAY
`)

// Insert metrics
await client.insert('metrics', [
  { timestamp: new Date(), host: 'server1', metric: 'cpu_usage', value: 45.2 },
  { timestamp: new Date(), host: 'server1', metric: 'memory_mb', value: 4096 },
  { timestamp: new Date(), host: 'server2', metric: 'cpu_usage', value: 62.8 },
])

// Query with time bucketing
const result = await client.query(`
  SELECT
    toStartOfMinute(timestamp) as minute,
    host,
    avg(value) as avg_value,
    max(value) as max_value
  FROM metrics
  WHERE metric = 'cpu_usage'
    AND timestamp > now() - INTERVAL 1 HOUR
  GROUP BY minute, host
  ORDER BY minute DESC
`)
```

### Materialized Views

```typescript
// Create source table
await client.query(`
  CREATE TABLE events (
    timestamp DateTime,
    user_id UInt64,
    event_type String,
    properties String
  ) ENGINE = MergeTree()
  ORDER BY (timestamp, user_id)
`)

// Create materialized view for hourly aggregates
await client.query(`
  CREATE MATERIALIZED VIEW events_hourly
  ENGINE = SummingMergeTree()
  ORDER BY (hour, event_type)
  AS SELECT
    toStartOfHour(timestamp) as hour,
    event_type,
    count() as event_count,
    uniq(user_id) as unique_users
  FROM events
  GROUP BY hour, event_type
`)

// Insert data - materialized view updates automatically
await client.insert('events', events)

// Query from materialized view (fast, pre-aggregated)
const stats = await client.query(`
  SELECT * FROM events_hourly
  WHERE hour > now() - INTERVAL 24 HOUR
  ORDER BY hour DESC
`)
```

### Window Functions

```typescript
// Ranking and percentiles
const ranked = await client.query(`
  SELECT
    user_id,
    revenue,
    row_number() OVER (ORDER BY revenue DESC) as rank,
    percent_rank() OVER (ORDER BY revenue) as percentile
  FROM orders
  WHERE order_date > today() - 30
`)

// Running totals
const cumulative = await client.query(`
  SELECT
    order_date,
    revenue,
    sum(revenue) OVER (ORDER BY order_date ROWS UNBOUNDED PRECEDING) as cumulative_revenue,
    avg(revenue) OVER (ORDER BY order_date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) as moving_avg_7d
  FROM daily_sales
`)
```

### CUBE and ROLLUP

```typescript
// ROLLUP for hierarchical subtotals
const rollup = await client.query(`
  SELECT
    category,
    product,
    sum(amount) as total,
    count() as orders
  FROM sales
  GROUP BY ROLLUP(category, product)
  ORDER BY category, product
`)
// Returns: (category, product), (category, NULL), (NULL, NULL)

// CUBE for all dimension combinations
const cube = await client.query(`
  SELECT
    region,
    category,
    sum(revenue) as total
  FROM sales
  GROUP BY CUBE(region, category)
`)
// Returns all 4 combinations: (region, category), (region, NULL), (NULL, category), (NULL, NULL)
```

### Dashboard Backend

```typescript
import { ClickHouseClient } from 'clickhouse.do'

export default {
  async fetch(request: Request, env: Env): Promise<Response> {
    const client = new ClickHouseClient(env)
    const url = new URL(request.url)

    if (url.pathname === '/api/dashboard/overview') {
      const [users, revenue, events] = await Promise.all([
        client.query(`SELECT uniq(user_id) as count FROM events WHERE timestamp > today()`),
        client.query(`SELECT sum(amount) as total FROM orders WHERE created_at > today()`),
        client.query(`SELECT count() as count FROM events WHERE timestamp > now() - INTERVAL 1 HOUR`),
      ])

      return Response.json({
        activeUsers: users.rows[0].count,
        todayRevenue: revenue.rows[0].total,
        eventsLastHour: events.rows[0].count,
      })
    }

    if (url.pathname === '/api/dashboard/timeseries') {
      const result = await client.query(`
        SELECT
          toStartOfHour(timestamp) as hour,
          count() as events,
          uniq(user_id) as users
        FROM events
        WHERE timestamp > now() - INTERVAL 24 HOUR
        GROUP BY hour
        ORDER BY hour
      `)

      return Response.json(result.rows)
    }

    return new Response('Not Found', { status: 404 })
  },
}
```

### CDC from MongoDB

```typescript
// Configure MongoDB change stream to ClickHouse
import { createCDCPipeline } from 'clickhouse.do/cdc'

const pipeline = createCDCPipeline({
  source: {
    type: 'mongodb',
    uri: env.MONGODB_URI,
    database: 'app',
    collection: 'orders',
  },
  destination: {
    clickhouse: client,
    table: 'orders_raw',
    transform: (doc) => ({
      id: doc._id.toString(),
      user_id: doc.userId,
      amount: doc.amount,
      status: doc.status,
      created_at: doc.createdAt,
    }),
  },
})

await pipeline.start()
```

### Real-Time Subscriptions

```typescript
// Subscribe to live query results
const subscription = client.subscribe(`
  SELECT
    toStartOfSecond(timestamp) as second,
    count() as events
  FROM events
  WHERE timestamp > now() - INTERVAL 1 MINUTE
  GROUP BY second
  ORDER BY second DESC
  LIMIT 60
`, {
  interval: 1000, // Refresh every second
  onData: (rows) => {
    updateChart(rows)
  },
})

// Later: unsubscribe
subscription.stop()
```

## Architecture

```
┌─────────────────────────────────────────────────────────────────────────┐
│                           Client Applications                           │
├─────────────────┬─────────────────┬─────────────────┬───────────────────┤
│   ClickHouse    │   HTTP/JSON     │   WebSocket     │  Service Binding  │
│   Wire Protocol │   REST API      │   Live Queries  │  Worker-to-Worker │
├─────────────────┴─────────────────┴─────────────────┴───────────────────┤
│                       clickhouse.do Worker (Edge)                       │
├─────────────────────────────────────────────────────────────────────────┤
│  SQL Parser  │  Query Planner  │  Aggregation Engine  │  Mat Views      │
├─────────────────────────────────────────────────────────────────────────┤
│                      Durable Objects (SQLite Storage)                   │
├──────────────────────────┬──────────────────────────────────────────────┤
│     R2 (Cold Storage)    │           Pipelines (CDC)                    │
│    Iceberg Table Format  │    MongoDB, PostgreSQL Change Streams        │
└──────────────────────────┴──────────────────────────────────────────────┘
```

clickhouse.do translates ClickHouse SQL to SQLite at runtime:

1. **SQL Parsing** - ClickHouse SQL parsed and validated
2. **Query Planning** - Optimized execution plan with index selection
3. **Durable Object Storage** - Each database runs as an isolated Durable Object with SQLite
4. **Edge Execution** - Queries execute at the edge, close to your users
5. **Optional Integrations** - R2 for cold storage, Pipelines for CDC, Vectorize for ML

## Connectivity Options

### Wire Protocol (ClickHouse Compatible)

Connect using ClickHouse native clients, DBeaver, or DataGrip:

```bash
# Local development
npx clickhouse.do serve --port 9000

# Connect with clickhouse-client
clickhouse-client --host localhost --port 9000

# Connect with DBeaver
# Driver: ClickHouse
# Host: localhost, Port: 9000
```

### HTTP Interface

```typescript
// Standard ClickHouse HTTP API
const response = await fetch('https://your-worker.workers.dev/', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: 'SELECT * FROM events LIMIT 10 FORMAT JSON',
})

const data = await response.json()
```

### Service Bindings (Zero Latency)

```typescript
// In your consuming worker
export default {
  async fetch(request: Request, env: Env) {
    const result = await env.CLICKHOUSE.query(`
      SELECT count() as total FROM events
    `)
    return Response.json(result.rows[0])
  },
}
```

## Configuration

### With R2 Cold Storage

```jsonc
{
  "r2_buckets": [
    { "binding": "ANALYTICS_BUCKET", "bucket_name": "analytics-cold" }
  ],
  "vars": {
    "COLD_STORAGE_ENABLED": "true",
    "COLD_STORAGE_THRESHOLD_DAYS": "30"
  }
}
```

### With CDC Pipelines

```jsonc
{
  "pipelines": [
    { "binding": "MONGODB_CDC", "pipeline": "mongodb-changes" }
  ],
  "vars": {
    "CDC_ENABLED": "true"
  }
}
```

### With Grafana

```jsonc
{
  "vars": {
    "GRAFANA_ENABLED": "true",
    "GRAFANA_API_KEY": "your-api-key"
  }
}
```

## Common Patterns

### Log Analytics

```typescript
// Create log table with TTL
await client.query(`
  CREATE TABLE logs (
    timestamp DateTime,
    level String,
    service String,
    message String,
    trace_id String
  ) ENGINE = MergeTree()
  ORDER BY (service, timestamp)
  TTL timestamp + INTERVAL 7 DAY
`)

// Error rate by service
const errorRates = await client.query(`
  SELECT
    service,
    countIf(level = 'error') as errors,
    count() as total,
    round(errors / total * 100, 2) as error_rate
  FROM logs
  WHERE timestamp > now() - INTERVAL 1 HOUR
  GROUP BY service
  ORDER BY error_rate DESC
`)

// Trace analysis
const trace = await client.query(`
  SELECT * FROM logs
  WHERE trace_id = '${traceId}'
  ORDER BY timestamp
`)
```

### E-commerce Analytics

```typescript
// Funnel analysis
const funnel = await client.query(`
  WITH
    countIf(event = 'view_product') as views,
    countIf(event = 'add_to_cart') as adds,
    countIf(event = 'checkout') as checkouts,
    countIf(event = 'purchase') as purchases
  SELECT
    views,
    adds,
    checkouts,
    purchases,
    round(adds / views * 100, 2) as view_to_cart,
    round(purchases / checkouts * 100, 2) as checkout_conversion
  FROM events
  WHERE timestamp > today() - 7
`)

// Revenue cohort analysis
const cohorts = await client.query(`
  SELECT
    toStartOfWeek(first_order) as cohort_week,
    dateDiff('week', first_order, order_date) as weeks_since_first,
    sum(amount) as revenue,
    uniq(user_id) as users
  FROM orders
  JOIN (
    SELECT user_id, min(order_date) as first_order
    FROM orders
    GROUP BY user_id
  ) USING user_id
  GROUP BY cohort_week, weeks_since_first
  ORDER BY cohort_week, weeks_since_first
`)
```

### IoT Metrics

```typescript
// Downsampling for long-term storage
await client.query(`
  CREATE MATERIALIZED VIEW metrics_hourly
  ENGINE = SummingMergeTree()
  ORDER BY (device_id, hour)
  AS SELECT
    device_id,
    toStartOfHour(timestamp) as hour,
    avg(temperature) as avg_temp,
    max(temperature) as max_temp,
    min(temperature) as min_temp,
    count() as readings
  FROM device_metrics
  GROUP BY device_id, hour
`)

// Anomaly detection
const anomalies = await client.query(`
  SELECT
    device_id,
    timestamp,
    temperature,
    avg(temperature) OVER (
      PARTITION BY device_id
      ORDER BY timestamp
      ROWS BETWEEN 60 PRECEDING AND CURRENT ROW
    ) as moving_avg,
    abs(temperature - moving_avg) as deviation
  FROM device_metrics
  WHERE timestamp > now() - INTERVAL 1 HOUR
  HAVING deviation > 10
  ORDER BY deviation DESC
`)
```

## Development

```bash
# Install dependencies
npm install

# Run tests
npm test

# Build
npm run build

# Local development
npm run dev
```

## Related

- [@dotdo/clickhouse](/docs/integrations/clickhouse/package) - In-memory ClickHouse for testing
- [mongo.do](/docs/integrations/mongo/service) - MongoDB on the edge
- [kafka.do](/docs/integrations/kafka/service) - Kafka-compatible streaming
- [Durable Objects](/docs/architecture/durable-objects) - DO-backed storage
