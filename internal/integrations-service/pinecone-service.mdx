---
title: pinecone.do
description: Pinecone on the Edge — A fully managed Pinecone-compatible vector database running on Cloudflare Workers with HNSW indexing and global deployment.
---

# pinecone.do

**Pinecone on the Edge** — A Pinecone-compatible vector database that runs entirely on Cloudflare Workers, with native HNSW indexing, persistent storage, and global edge deployment.

```typescript
import { Pinecone } from 'pinecone.do'

const pinecone = new Pinecone('https://your-worker.workers.dev')
const index = pinecone.index('products')

// It's just Pinecone
await index.upsert([
  { id: 'vec1', values: embedding, metadata: { category: 'electronics' } },
])

const results = await index.query({
  vector: queryEmbedding,
  topK: 10,
  filter: { category: { $eq: 'electronics' } },
  includeMetadata: true,
})
```

<Callout type="info">
Looking for an in-memory implementation for testing? See [@dotdo/pinecone](/docs/integrations/pinecone/package) for a zero-dependency vector store.
</Callout>

## Why pinecone.do?

Traditional vector databases require infrastructure management, connection pooling, and careful scaling. pinecone.do eliminates all of that by running directly on Cloudflare's edge network:

- **Zero Infrastructure** — No servers to manage, no connection limits, no cold starts
- **Global by Default** — Vectors live at the edge, close to your users (300+ cities)
- **Pinecone Compatible** — Drop-in replacement for Pinecone SDK operations
- **Edge-Native HNSW** — State-of-the-art approximate nearest neighbor search
- **Serverless Economics** — Pay only for what you use, scale to zero

| Pinecone Cloud | pinecone.do |
|----------------|-------------|
| Managed vector database | Runs entirely on the edge |
| Per-vector pricing | Flat resource pricing |
| Network latency to clusters | Edge-local HNSW in DO |
| Cold start delays | 0ms cold starts (V8 isolates) |
| Separate infrastructure | Unified with your DO data |
| Regional deployment | Global (300+ cities) |

**This is a reimplementation.** Your data lives in Durable Object storage with EdgeVec HNSW indexes - not proxied to Pinecone servers.

## Features

### Core Vector Operations

| Feature | Description |
|---------|-------------|
| **Upsert** | Insert or update vectors with metadata |
| **Query** | Similarity search with metadata filters |
| **Fetch** | Retrieve vectors by ID |
| **Delete** | Remove vectors by ID or filter |
| **Update** | Modify vector values or metadata |
| **Index Stats** | Vector counts, dimension info, fullness |

### HNSW Indexing

| Feature | Description |
|---------|-------------|
| **Configurable M** | Max connections per node (default: 16) |
| **efConstruction** | Build-time search width (default: 200) |
| **Search-time ef** | Query-time candidate list size |
| **Distance Metrics** | Cosine, Euclidean (L2), Dot Product |

### Edge Architecture

| Feature | Description |
|---------|-------------|
| **Durable Storage** | SQLite-backed persistence in Durable Objects |
| **R2 Backup** | Automatic backup to R2 as Parquet files |
| **Quantization** | Product and Scalar quantization for memory efficiency |
| **Sharding** | Horizontal scaling across multiple DOs |
| **0ms Cold Starts** | V8 isolate architecture |

## Installation

```bash
npm install pinecone.do
```

## Quick Start

### Deploy to Cloudflare Workers

```typescript
// src/index.ts
import { PineconeEntrypoint, PineconeDO } from 'pinecone.do'

export { PineconeDO }
export default PineconeEntrypoint
```

```jsonc
// wrangler.jsonc
{
  "name": "my-pinecone.do",
  "main": "src/index.ts",
  "compatibility_date": "2025-01-01",
  "compatibility_flags": ["nodejs_compat"],
  "durable_objects": {
    "bindings": [{ "name": "PINECONE_DO", "class_name": "PineconeDO" }]
  },
  "migrations": [{ "tag": "v1", "new_sqlite_classes": ["PineconeDO"] }],
  "r2_buckets": [{ "binding": "VECTORS_BACKUP", "bucket_name": "pinecone-backup" }]
}
```

```bash
npx wrangler deploy
```

### Create Index and Search

```typescript
import { Pinecone } from 'pinecone.do'

const pinecone = new Pinecone('https://your-worker.workers.dev')

// Create an index
await pinecone.createIndex({
  name: 'products',
  dimension: 1536,
  metric: 'cosine',
  spec: {
    hnsw: {
      m: 16,
      efConstruction: 200,
    },
  },
})

// Get the index
const index = pinecone.index('products')

// Upsert vectors with metadata
await index.upsert([
  {
    id: 'product-1',
    values: embedding1,
    metadata: {
      name: 'MacBook Pro',
      category: 'electronics',
      price: 2499,
      inStock: true,
    },
  },
  {
    id: 'product-2',
    values: embedding2,
    metadata: {
      name: 'AirPods Pro',
      category: 'electronics',
      price: 249,
      inStock: true,
    },
  },
])

// Query with filters
const results = await index.query({
  vector: queryEmbedding,
  topK: 10,
  filter: {
    $and: [
      { category: { $eq: 'electronics' } },
      { price: { $lt: 500 } },
    ],
  },
  includeMetadata: true,
})
```

## Examples

### RAG Pipeline with Embeddings

```typescript
import { Pinecone } from 'pinecone.do'
import { AI } from '@cloudflare/ai'

export default {
  async fetch(request: Request, env: Env) {
    const ai = new AI(env.AI)
    const pinecone = new Pinecone(env.PINECONE_URL)
    const index = pinecone.index('documents')

    const { query } = await request.json()

    // Generate embedding for query
    const { data } = await ai.run('@cf/baai/bge-base-en-v1.5', { text: [query] })
    const queryEmbedding = data[0]

    // Search for similar documents
    const results = await index.query({
      vector: queryEmbedding,
      topK: 5,
      includeMetadata: true,
    })

    // Build context from results
    const context = results.matches
      .map(m => m.metadata?.content)
      .join('\n\n')

    // Generate response with context
    const response = await ai.run('@cf/meta/llama-2-7b-chat-int8', {
      messages: [
        { role: 'system', content: `Use this context to answer: ${context}` },
        { role: 'user', content: query },
      ],
    })

    return Response.json({ answer: response.response, sources: results.matches })
  },
}
```

### Batch Ingestion

```typescript
// Upsert in batches (automatically chunked)
const vectors = await generateEmbeddings(documents) // Large dataset

await index.upsert(vectors, {
  batchSize: 100, // Vectors per batch
})

// Check index stats
const stats = await index.describeIndexStats()
console.log(`Total vectors: ${stats.totalVectorCount}`)
```

### Namespace Isolation

```typescript
const index = pinecone.index('products')

// Different namespaces for different tenants
const tenant1 = index.namespace('tenant-1')
const tenant2 = index.namespace('tenant-2')

// Data is isolated between namespaces
await tenant1.upsert([{ id: 'product-1', values: embedding1, metadata: { name: 'Widget' } }])
await tenant2.upsert([{ id: 'product-1', values: embedding2, metadata: { name: 'Gadget' } }])

// Queries only see their namespace's data
const results = await tenant1.query({ vector: queryVector, topK: 10 })
```

### Hybrid Search (Vector + Filters)

```typescript
// Complex metadata filtering
const results = await index.query({
  vector: queryEmbedding,
  topK: 20,
  filter: {
    $and: [
      { category: { $in: ['electronics', 'computers'] } },
      { price: { $gte: 100, $lte: 1000 } },
      { inStock: { $eq: true } },
      {
        $or: [
          { brand: { $eq: 'Apple' } },
          { rating: { $gte: 4.5 } },
        ],
      },
    ],
  },
  includeMetadata: true,
  includeValues: false,
})
```

## Architecture

```
+-------------------------------------------------------------+
|                    Your Application                          |
|                                                              |
|  const index = pinecone.index('products')                    |
|  await index.query({ vector: [...], topK: 10 })              |
+-------------------------------------------------------------+
                              |
                              v
+-------------------------------------------------------------+
|                    pinecone.do Worker                        |
|                    (Edge - 300+ cities)                      |
+-------------------------------------------------------------+
|                                                              |
|  +-------------------+  +-------------------+                |
|  |   EdgeVecDO       |  |   EdgeVecDO       |  (Sharding)   |
|  |   (Durable Object)|  |   (Durable Object)|               |
|  +-------------------+  +-------------------+                |
|  |                   |  |                   |                |
|  | ┌───────────────┐ |  | ┌───────────────┐ |                |
|  | │  HNSW Index   │ |  | │  HNSW Index   │ |                |
|  | │  (in-memory)  │ |  | │  (in-memory)  │ |                |
|  | └───────────────┘ |  | └───────────────┘ |                |
|  |                   |  |                   |                |
|  | ┌───────────────┐ |  | ┌───────────────┐ |                |
|  | │ SQLite Storage│ |  | │ SQLite Storage│ |                |
|  | │ (persistence) │ |  | │ (persistence) │ |                |
|  | └───────────────┘ |  | └───────────────┘ |                |
|  +-------------------+  +-------------------+                |
|                                                              |
+-------------------------------------------------------------+
                              |
                              v
+-------------------------------------------------------------+
|                    R2 Backup (Parquet)                       |
|                    (Disaster recovery)                       |
+-------------------------------------------------------------+
```

### EdgeVec Features

- **HNSW algorithm**: State-of-the-art approximate nearest neighbor search
- **Configurable parameters**: Tune M and efConstruction for speed/recall tradeoff
- **Multiple metrics**: Cosine, Euclidean (L2), and Dot Product
- **Filtered search**: Metadata filters applied during search
- **Batch operations**: Efficient bulk upsert and delete
- **Persistence**: Automatic SQLite persistence with R2 backup
- **Quantization**: Product and Scalar quantization for memory efficiency

## Scaling Considerations

| Limit | Value | Notes |
|-------|-------|-------|
| DO storage | ~1GB per Durable Object | Per-shard limit |
| Max vectors (1536D) | ~150k vectors per DO | Dimension-dependent |
| Query latency | Sub-millisecond | Typical workloads |
| Global replication | DO replication | Automatic |

### Horizontal Scaling

For larger datasets, pinecone.do automatically shards across multiple Durable Objects:

```typescript
// Large-scale deployment
await pinecone.createIndex({
  name: 'large-index',
  dimension: 1536,
  metric: 'cosine',
  spec: {
    shards: 10, // Distribute across 10 DOs
    hnsw: { m: 16, efConstruction: 200 },
  },
})

// Queries are automatically routed to the right shard
const results = await index.query({
  vector: queryEmbedding,
  topK: 100,
})
```

## Configuration

### With Workers AI Embeddings

```jsonc
{
  "ai": { "binding": "AI" },
  "vars": {
    "EMBEDDING_MODEL": "@cf/baai/bge-base-en-v1.5",
    "AUTO_EMBED": "true"
  }
}
```

```typescript
// Auto-embed text on upsert
await index.upsert([
  {
    id: 'doc-1',
    text: 'Introduction to machine learning', // Auto-embedded
    metadata: { topic: 'ml' },
  },
])

// Auto-embed query text
const results = await index.query({
  text: 'neural networks tutorial', // Auto-embedded
  topK: 10,
})
```

### With R2 Backup

```jsonc
{
  "r2_buckets": [
    { "binding": "VECTORS_BACKUP", "bucket_name": "pinecone-backup" }
  ],
  "vars": {
    "BACKUP_INTERVAL": "3600000",
    "BACKUP_FORMAT": "parquet"
  }
}
```

## Connectivity Options

### HTTP RPC

```typescript
// Direct HTTP calls
const response = await fetch('https://your-worker.workers.dev/rpc', {
  method: 'POST',
  body: JSON.stringify({
    method: 'query',
    params: {
      index: 'products',
      vector: queryEmbedding,
      topK: 10,
    },
  }),
})
```

### Service Bindings (Zero Latency)

```typescript
// In your consuming worker
export default {
  async fetch(request: Request, env: Env) {
    const results = await env.PINECONE.query({
      index: 'products',
      vector: queryEmbedding,
      topK: 10,
    })
    return Response.json(results)
  },
}
```

## Limitations

What's different from Pinecone Cloud:

| Feature | Status | Notes |
|---------|--------|-------|
| Upsert/Query/Fetch | Implemented | Full compatibility |
| Metadata filters | Implemented | All operators supported |
| Namespaces | Implemented | Partition vectors |
| Index management | Implemented | Create/delete/describe |
| Batch operations | Implemented | Auto-chunking |
| Collections | Not implemented | |
| Inference API | Not implemented | Use Workers AI instead |
| Serverless spec | Not implemented | All indexes use EdgeVec |
| Pod configuration | Not implemented | |

## Related

- [@dotdo/pinecone](/docs/integrations/pinecone/package) - In-memory Pinecone for testing
- [Elasticsearch Integration](/docs/integrations/elasticsearch) - Full-text + vector hybrid search
- [Vector Search](/docs/database/vector) - Semantic similarity search
- [Durable Objects](/docs/architecture/durable-objects) - DO-backed storage
