---
title: "@dotdo/openai"
description: Drop-in replacement for the OpenAI SDK with edge compatibility and automatic retries.
---

# @dotdo/openai

Drop-in replacement for the official `openai` SDK. Your existing OpenAI code works unchanged - just swap the import.

```typescript
// Before: OpenAI
import OpenAI from 'openai'

// After: dotdo
import OpenAI from '@dotdo/openai'

// Code stays the same
const client = new OpenAI({ apiKey: 'sk-xxx' })
const completion = await client.chat.completions.create({
  model: 'gpt-4',
  messages: [{ role: 'user', content: 'Hello' }],
})
```

<Callout type="info">
Looking for a managed service with multi-provider routing and observability? See [openai.do](/docs/integrations/openai/service) for production deployments.
</Callout>

## Why @dotdo/openai?

| OpenAI SDK | @dotdo/openai |
|------------|---------------|
| Node.js only | Edge-compatible (Workers, Deno, Bun) |
| No built-in retries | Automatic retries with exponential backoff |
| Manual error handling | Typed errors with request IDs |

**This is a reimplementation.** The SDK provides 100% API compatibility while adding edge runtime support and automatic retries.

## Installation

```bash
npm install @dotdo/openai
```

## Features

### Implemented

**Chat Completions**
- `chat.completions.create()` - Chat completions with GPT-4, GPT-3.5-turbo, etc.
- Streaming support via async iterables
- Function/tool calling
- Vision (image inputs)
- JSON mode

**Embeddings**
- `embeddings.create()` - Text embeddings with ada-002, text-embedding-3-small/large

**Images**
- `images.generate()` - DALL-E image generation
- `images.edit()` - Image editing with masks
- `images.createVariation()` - Image variations

**Models**
- `models.list()` - List available models
- `models.retrieve()` - Get model details
- `models.del()` - Delete fine-tuned models

**Assistants API**
- `AssistantsClient` with in-memory storage backend
- Assistant CRUD operations
- Thread management
- Message handling
- Run execution with tool calls
- Polling helpers (`runs.poll()`, `runs.createAndPoll()`)

### Not Yet Implemented

- Fine-tuning API
- Files API
- Batch API
- Audio (Whisper, TTS)
- Moderation

## Quick Start

### Chat Completions

```typescript
import OpenAI from '@dotdo/openai'

const client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY })

const completion = await client.chat.completions.create({
  model: 'gpt-4',
  messages: [
    { role: 'system', content: 'You are a helpful assistant.' },
    { role: 'user', content: 'What is the capital of France?' },
  ],
})

console.log(completion.choices[0].message.content)
// "The capital of France is Paris."
```

### Streaming

```typescript
const stream = await client.chat.completions.create({
  model: 'gpt-4',
  messages: [{ role: 'user', content: 'Tell me a story' }],
  stream: true,
})

for await (const chunk of stream) {
  const content = chunk.choices[0]?.delta?.content
  if (content) {
    process.stdout.write(content)
  }
}
```

### Embeddings

```typescript
const embedding = await client.embeddings.create({
  model: 'text-embedding-ada-002',
  input: 'Hello world',
})

console.log(embedding.data[0].embedding)
// [0.123, -0.456, 0.789, ...]

// Batch embeddings
const embeddings = await client.embeddings.create({
  model: 'text-embedding-3-small',
  input: ['Hello', 'World', 'How are you?'],
})
```

### Function Calling

```typescript
const completion = await client.chat.completions.create({
  model: 'gpt-4',
  messages: [{ role: 'user', content: 'What is the weather in San Francisco?' }],
  tools: [{
    type: 'function',
    function: {
      name: 'get_weather',
      description: 'Get the current weather for a location',
      parameters: {
        type: 'object',
        properties: {
          location: {
            type: 'string',
            description: 'City name',
          },
          unit: {
            type: 'string',
            enum: ['celsius', 'fahrenheit'],
          },
        },
        required: ['location'],
      },
    },
  }],
})

const message = completion.choices[0].message

if (message.tool_calls) {
  const toolCall = message.tool_calls[0]
  console.log(toolCall.function.name)
  // "get_weather"
  console.log(JSON.parse(toolCall.function.arguments))
  // { location: "San Francisco", unit: "fahrenheit" }
}
```

### Vision

```typescript
const completion = await client.chat.completions.create({
  model: 'gpt-4-vision-preview',
  messages: [
    {
      role: 'user',
      content: [
        { type: 'text', text: 'What is in this image?' },
        {
          type: 'image_url',
          image_url: { url: 'https://example.com/image.jpg' },
        },
      ],
    },
  ],
})
```

### Image Generation

```typescript
const response = await client.images.generate({
  model: 'dall-e-3',
  prompt: 'A sunset over mountains',
  size: '1024x1024',
  quality: 'hd',
})

console.log(response.data[0].url)
```

## Edge Runtime Compatibility

Unlike the official SDK, `@dotdo/openai` works in edge runtimes:

```typescript
// Cloudflare Workers
export default {
  async fetch(request: Request, env: Env) {
    const client = new OpenAI({ apiKey: env.OPENAI_API_KEY })

    const completion = await client.chat.completions.create({
      model: 'gpt-4',
      messages: [{ role: 'user', content: 'Hello from the edge!' }],
    })

    return Response.json(completion)
  },
}
```

```typescript
// Deno
import OpenAI from '@dotdo/openai'

const client = new OpenAI({ apiKey: Deno.env.get('OPENAI_API_KEY') })
```

## Custom Endpoints

By default, requests go to OpenAI's API. You can also configure custom endpoints:

```typescript
// Default: OpenAI API
const client = new OpenAI({ apiKey: 'sk-xxx' })

// Azure OpenAI
const azureClient = new OpenAI({
  apiKey: process.env.AZURE_OPENAI_KEY,
  baseURL: 'https://your-resource.openai.azure.com/openai/deployments/your-deployment',
  defaultHeaders: {
    'api-version': '2024-02-01',
  },
})

// Local/self-hosted (vLLM, LocalAI, etc.)
const localClient = new OpenAI({
  apiKey: 'not-needed',
  baseURL: 'http://localhost:8000/v1',
})
```

## Multi-Provider Routing

For production use, route requests across multiple providers with automatic failover using `createOpenAIWithRuntime`:

```typescript
import { createOpenAIWithRuntime } from '@dotdo/openai'

const client = createOpenAIWithRuntime({
  providers: [
    { name: 'openai', apiKey: process.env.OPENAI_API_KEY },
    { name: 'anthropic', apiKey: process.env.ANTHROPIC_API_KEY },
  ],
  strategy: 'priority',  // 'priority' | 'round-robin' | 'least-latency' | 'cost-optimized'
  fallback: {
    enabled: true,
    maxAttempts: 2,
  },
})

// Same API - requests auto-route with failover
const completion = await client.chat.completions.create({
  model: 'gpt-4',
  messages: [{ role: 'user', content: 'Hello' }],
})

// Get routing statistics
const stats = client.getStats()
console.log(stats)
// {
//   totalRequests: 100,
//   totalTokens: 50000,
//   byProvider: {
//     openai: { requests: 95, tokens: 47500, costUsd: 1.50, errors: 2, averageLatencyMs: 450 },
//     anthropic: { requests: 5, tokens: 2500, costUsd: 0.08, errors: 0, averageLatencyMs: 380 }
//   }
// }
```

### Routing Strategies

| Strategy | Description |
|----------|-------------|
| `priority` | Use first provider, failover to next (default) |
| `round-robin` | Distribute requests evenly across providers |
| `least-latency` | Route to provider with lowest average latency |
| `cost-optimized` | Route based on model pricing (basic implementation) |

### Supported Providers

Currently supported providers for multi-provider routing:
- `openai` - OpenAI API
- `anthropic` - Anthropic Claude API

## Assistants API

The `AssistantsClient` provides an OpenAI Assistants-compatible API with an in-memory storage backend:

```typescript
import { AssistantsClient } from '@dotdo/openai'

const client = new AssistantsClient({
  apiKey: process.env.OPENAI_API_KEY,
})

// Create an assistant
const assistant = await client.assistants.create({
  name: 'Math Tutor',
  instructions: 'You are a helpful math tutor.',
  model: 'gpt-4-turbo',
  tools: [{ type: 'function', function: { name: 'calculate', parameters: {} } }],
})

// Create a thread
const thread = await client.threads.create()

// Add a message
await client.messages.create(thread.id, {
  role: 'user',
  content: 'Solve x^2 + 2x + 1 = 0',
})

// Create and poll for completion (convenience method)
const run = await client.runs.createAndPoll(thread.id, {
  assistant_id: assistant.id,
})

// Get messages
const messages = await client.messages.list(thread.id)
console.log(messages.data[0].content)
```

### Tool Handling

When a run requires tool outputs, you can submit them:

```typescript
// Check if run requires action
if (run.status === 'requires_action') {
  const toolCalls = run.required_action?.submit_tool_outputs.tool_calls

  const toolOutputs = toolCalls.map((tc) => ({
    tool_call_id: tc.id,
    output: JSON.stringify({ result: 'computed value' }),
  }))

  await client.runs.submitToolOutputs(thread.id, run.id, {
    tool_outputs: toolOutputs,
  })
}
```

<Callout type="warn">
The Assistants API uses local in-memory storage by default. Data does not persist across restarts.
</Callout>

## TypeScript Types

Full TypeScript support with OpenAI-compatible types:

```typescript
import type {
  // Chat
  ChatCompletion,
  ChatCompletionChunk,
  ChatCompletionCreateParams,
  ChatCompletionMessage,
  ChatCompletionMessageParam,
  ChatCompletionTool,
  ChatCompletionToolChoiceOption,

  // Embeddings
  Embedding,
  EmbeddingCreateParams,
  CreateEmbeddingResponse,

  // Images
  ImageCreateParams,
  ImagesResponse,

  // Models
  Model,
  ModelListResponse,

  // Common
  Usage,
  RequestOptions,

  // Assistants (selected types)
  Assistant,
  Thread,
  AssistantMessage,
  Run,
  RunStatus,
} from '@dotdo/openai'

// Client classes and errors
import { OpenAI, OpenAIError, AssistantsClient } from '@dotdo/openai'

// Multi-provider routing
import { createOpenAIWithRuntime } from '@dotdo/openai'
```

## Error Handling

```typescript
import OpenAI, { OpenAIError } from '@dotdo/openai'

try {
  const completion = await client.chat.completions.create({
    model: 'gpt-4',
    messages: [{ role: 'user', content: 'Hello' }],
  })
} catch (error) {
  if (error instanceof OpenAIError) {
    console.error('Status:', error.status)
    console.error('Type:', error.type)
    console.error('Code:', error.code)
    console.error('Message:', error.message)
    console.error('Request ID:', error.requestId)

    // Handle specific errors
    if (error.status === 429) {
      console.error('Rate limited - retry later')
    } else if (error.status === 401) {
      console.error('Invalid API key')
    }
  }
}
```

## Configuration

```typescript
const client = new OpenAI({
  // Required
  apiKey: 'sk-xxx',

  // Optional
  organization: 'org-xxx',
  baseURL: 'https://api.openai.com',
  timeout: 600000,  // 10 minutes (default)
  maxRetries: 2,    // Automatic retries (default)

  // Custom headers
  defaultHeaders: {
    'X-Custom-Header': 'value',
  },

  // Custom fetch (for testing/mocking)
  fetch: customFetch,
})
```

## Migration from Official SDK

### Package Change

```bash
# Remove
npm uninstall openai

# Install
npm install @dotdo/openai
```

### Import Change

```typescript
// Before
import OpenAI from 'openai'

// After
import OpenAI from '@dotdo/openai'
```

All method signatures and types are identical - no code changes required.

## Testing with @dotdo/openai

The package is ideal for unit tests - works without network access when mocking:

```typescript
import OpenAI from '@dotdo/openai'
import { vi } from 'vitest'

describe('AI Service', () => {
  let client: OpenAI

  beforeEach(() => {
    client = new OpenAI({
      apiKey: 'test-key',
      fetch: vi.fn().mockResolvedValue(
        new Response(JSON.stringify({
          choices: [{ message: { content: 'Mocked response' } }]
        }))
      ),
    })
  })

  it('should generate completion', async () => {
    const result = await client.chat.completions.create({
      model: 'gpt-4',
      messages: [{ role: 'user', content: 'Hello' }],
    })
    expect(result.choices[0].message.content).toBe('Mocked response')
  })
})
```

## Related

- [openai.do](/docs/integrations/openai/service) - Managed AI gateway for production
- [Anthropic](/docs/integrations/anthropic) - Claude API integration
- [Named Agents](/docs/agents/named-agents) - Priya, Ralph, Tom, Mark, Sally, Quinn
- [Agent SDK](/docs/agents) - Multi-provider agent system
