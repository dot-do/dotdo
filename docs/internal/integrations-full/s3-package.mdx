---
title: "@dotdo/s3"
description: Drop-in replacement for @aws-sdk/client-s3, backed by Cloudflare R2 on Durable Objects.
---

# @dotdo/s3

Drop-in replacement for the AWS S3 SDK. Your existing `@aws-sdk/client-s3` code works unchanged - just swap the import.

```typescript
// Before: AWS S3
import { S3Client, PutObjectCommand, GetObjectCommand } from '@aws-sdk/client-s3'

// After: dotdo
import { S3Client, PutObjectCommand, GetObjectCommand } from '@dotdo/s3'

// Code stays the same
const client = new S3Client({ region: 'auto' })
await client.send(new PutObjectCommand({
  Bucket: 'my-bucket',
  Key: 'file.txt',
  Body: 'Hello, World!'
}))
```

<Callout type="info">
Looking for managed production storage? See [s3.do](/docs/integrations/s3/service) for a fully managed S3-compatible edge service.
</Callout>

## Why @dotdo/s3?

| AWS S3 | @dotdo/s3 |
|--------|-----------|
| Regional latency | Edge-local (300+ cities) |
| Per-request pricing | Flat resource pricing |
| Cross-region data transfer fees | No egress fees (R2) |
| Cold start for Lambda | 0ms cold starts (V8 isolates) |
| Separate infrastructure | Unified with your DO data |

**This is backed by Cloudflare R2.** Your data lives in R2 buckets with S3-compatible API - not proxied to AWS. R2 has zero egress fees.

## Installation

```bash
npm install @dotdo/s3
```

## Features

### Implemented

**Bucket Commands**
- `CreateBucketCommand` - Create a bucket
- `DeleteBucketCommand` - Delete an empty bucket
- `HeadBucketCommand` - Check if bucket exists
- `ListBucketsCommand` - List all buckets with pagination

**Object Commands**
- `PutObjectCommand` - Upload objects
- `GetObjectCommand` - Download objects (with range support)
- `HeadObjectCommand` - Get object metadata
- `DeleteObjectCommand` - Delete single object
- `DeleteObjectsCommand` - Batch delete
- `ListObjectsV2Command` - List objects with pagination
- `CopyObjectCommand` - Copy objects (with metadata directive)

**Multipart Upload Commands**
- `CreateMultipartUploadCommand` - Start multipart upload
- `UploadPartCommand` - Upload part
- `CompleteMultipartUploadCommand` - Complete multipart
- `AbortMultipartUploadCommand` - Abort multipart
- `ListPartsCommand` - List uploaded parts
- `ListMultipartUploadsCommand` - List in-progress uploads

**CORS Configuration**
- `PutBucketCorsCommand` - Configure CORS rules
- `GetBucketCorsCommand` - Get CORS configuration
- `DeleteBucketCorsCommand` - Remove CORS configuration

**Lifecycle Configuration**
- `PutBucketLifecycleConfigurationCommand` - Configure lifecycle rules
- `GetBucketLifecycleConfigurationCommand` - Get lifecycle rules
- `DeleteBucketLifecycleCommand` - Remove lifecycle rules

**Versioning**
- `PutBucketVersioningCommand` - Enable/suspend versioning
- `GetBucketVersioningCommand` - Get versioning status
- `ListObjectVersionsCommand` - List object versions

**Presigned URLs**
- `getSignedUrl()` - Generate presigned URLs (AWS SDK compatible)
- `getSignedGetUrl()` - Convenience method for downloads
- `getSignedPutUrl()` - Convenience method for uploads
- `getSignedDeleteUrl()` - Convenience method for deletes
- `getSignedHeadUrl()` - Convenience method for metadata checks
- AWS Signature V4 signing
- Expiration control (up to 7 days)
- Custom header support

### Not Yet Implemented

- Object tagging
- Replication
- S3 Select
- Inventory
- Object Lock

## Quick Start

### Configure

Add R2 bucket binding to your `wrangler.toml`:

```toml
[[r2_buckets]]
binding = "R2_BUCKET"
bucket_name = "my-bucket"
```

### Upload Files

```typescript
import { S3Client, PutObjectCommand } from '@dotdo/s3'

const client = new S3Client({
  region: 'auto',
  // R2 binding injected automatically in Workers
})

// Upload text
await client.send(new PutObjectCommand({
  Bucket: 'my-bucket',
  Key: 'hello.txt',
  Body: 'Hello, World!',
  ContentType: 'text/plain',
}))

// Upload JSON
await client.send(new PutObjectCommand({
  Bucket: 'my-bucket',
  Key: 'data.json',
  Body: JSON.stringify({ name: 'John', age: 30 }),
  ContentType: 'application/json',
}))

// Upload with metadata
await client.send(new PutObjectCommand({
  Bucket: 'my-bucket',
  Key: 'document.pdf',
  Body: pdfBuffer,
  ContentType: 'application/pdf',
  Metadata: {
    'uploaded-by': 'user-123',
    'original-name': 'quarterly-report.pdf',
  },
}))
```

### Download Files

```typescript
import { S3Client, GetObjectCommand } from '@dotdo/s3'

const client = new S3Client({ region: 'auto' })

// Get object
const response = await client.send(new GetObjectCommand({
  Bucket: 'my-bucket',
  Key: 'hello.txt',
}))

// Read as text
const text = await response.Body?.transformToString()
console.log(text) // "Hello, World!"

// Read as bytes
const bytes = await response.Body?.transformToByteArray()

// Stream response
const stream = response.Body?.transformToWebStream()

// Access metadata
console.log(response.ContentType)     // "text/plain"
console.log(response.ContentLength)   // 13
console.log(response.ETag)            // "abc123..."
console.log(response.Metadata)        // { "uploaded-by": "user-123" }
```

### Check If Object Exists

```typescript
import { S3Client, HeadObjectCommand } from '@dotdo/s3'

const client = new S3Client({ region: 'auto' })

try {
  const response = await client.send(new HeadObjectCommand({
    Bucket: 'my-bucket',
    Key: 'file.txt',
  }))
  console.log('Size:', response.ContentLength)
  console.log('Last Modified:', response.LastModified)
} catch (error) {
  if (error.name === 'NotFound') {
    console.log('Object does not exist')
  }
}
```

### Delete Files

```typescript
import { S3Client, DeleteObjectCommand, DeleteObjectsCommand } from '@dotdo/s3'

const client = new S3Client({ region: 'auto' })

// Delete single object
await client.send(new DeleteObjectCommand({
  Bucket: 'my-bucket',
  Key: 'old-file.txt',
}))

// Batch delete
await client.send(new DeleteObjectsCommand({
  Bucket: 'my-bucket',
  Delete: {
    Objects: [
      { Key: 'file1.txt' },
      { Key: 'file2.txt' },
      { Key: 'folder/file3.txt' },
    ],
  },
}))
```

### List Objects

```typescript
import { S3Client, ListObjectsV2Command } from '@dotdo/s3'

const client = new S3Client({ region: 'auto' })

// List all objects
const response = await client.send(new ListObjectsV2Command({
  Bucket: 'my-bucket',
}))

for (const object of response.Contents ?? []) {
  console.log(`${object.Key} - ${object.Size} bytes`)
}

// List with prefix (folder-like)
const uploads = await client.send(new ListObjectsV2Command({
  Bucket: 'my-bucket',
  Prefix: 'uploads/',
  Delimiter: '/',
}))

// Objects in uploads/
for (const object of uploads.Contents ?? []) {
  console.log('File:', object.Key)
}

// "Subdirectories" in uploads/
for (const prefix of uploads.CommonPrefixes ?? []) {
  console.log('Folder:', prefix.Prefix)
}

// Pagination with error handling and progress
async function listAllObjects(
  client: S3Client,
  bucket: string,
  options?: {
    prefix?: string
    maxKeys?: number
    onProgress?: (count: number, lastKey: string) => void
    resumeToken?: string  // Resume from previous run
  }
) {
  const allObjects: { Key: string; Size: number }[] = []
  let continuationToken = options?.resumeToken
  let totalObjects = 0
  let retryCount = 0
  const maxRetries = 3

  do {
    try {
      const page = await client.send(new ListObjectsV2Command({
        Bucket: bucket,
        Prefix: options?.prefix,
        MaxKeys: options?.maxKeys ?? 1000,
        ContinuationToken: continuationToken,
      }))

      for (const object of page.Contents ?? []) {
        allObjects.push({ Key: object.Key!, Size: object.Size ?? 0 })
        totalObjects++
      }

      // Progress callback with current count and last processed key
      if (options?.onProgress && page.Contents?.length) {
        const lastKey = page.Contents[page.Contents.length - 1].Key!
        options.onProgress(totalObjects, lastKey)
      }

      continuationToken = page.IsTruncated ? page.NextContinuationToken : undefined
      retryCount = 0 // Reset on success

      // Rate limiting for large buckets
      if (page.IsTruncated) {
        await new Promise(resolve => setTimeout(resolve, 100))
      }
    } catch (error) {
      if (retryCount < maxRetries && isRetryableError(error)) {
        retryCount++
        const delay = Math.min(1000 * Math.pow(2, retryCount), 10000)
        console.warn(`Retry ${retryCount}/${maxRetries} after ${delay}ms`)
        await new Promise(resolve => setTimeout(resolve, delay))
        continue
      }
      // Store continuationToken for resume on failure
      console.error('Failed at token:', continuationToken)
      throw error
    }
  } while (continuationToken)

  return allObjects
}

function isRetryableError(error: unknown): boolean {
  if (error instanceof Error) {
    return ['NetworkError', 'TimeoutError', 'ThrottlingException']
      .includes(error.name) || (error as any).$fault === 'server'
  }
  return false
}

// Usage
const objects = await listAllObjects(client, 'my-bucket', {
  prefix: 'uploads/',
  onProgress: (count, lastKey) => {
    console.log(`Processed ${count} objects, last: ${lastKey}`)
  },
})
```

### Copy Objects

```typescript
import { S3Client, CopyObjectCommand } from '@dotdo/s3'

const client = new S3Client({ region: 'auto' })

// Copy within same bucket
await client.send(new CopyObjectCommand({
  Bucket: 'my-bucket',
  CopySource: 'my-bucket/original.txt',
  Key: 'backup/original.txt',
}))

// Copy with new metadata
await client.send(new CopyObjectCommand({
  Bucket: 'my-bucket',
  CopySource: 'my-bucket/file.txt',
  Key: 'file-copy.txt',
  MetadataDirective: 'REPLACE',
  ContentType: 'text/plain',
  Metadata: {
    'copied-at': new Date().toISOString(),
  },
}))
```

## Presigned URLs

Generate temporary URLs for direct uploads/downloads without exposing credentials.

### Download URL

```typescript
import { S3Client, GetObjectCommand, getSignedUrl } from '@dotdo/s3'

const client = new S3Client({ region: 'auto' })

// Generate download URL (expires in 1 hour)
const url = await getSignedUrl(client, new GetObjectCommand({
  Bucket: 'my-bucket',
  Key: 'file.pdf',
}), {
  expiresIn: 3600, // 1 hour
})

// Return URL to client for direct download
return new Response(JSON.stringify({ downloadUrl: url }))
```

### Upload URL

```typescript
import { S3Client, PutObjectCommand, getSignedUrl } from '@dotdo/s3'

const client = new S3Client({ region: 'auto' })

// Generate upload URL
const url = await getSignedUrl(client, new PutObjectCommand({
  Bucket: 'my-bucket',
  Key: `uploads/${crypto.randomUUID()}.jpg`,
  ContentType: 'image/jpeg',
}), {
  expiresIn: 300, // 5 minutes
})

// Client can PUT directly to this URL
return new Response(JSON.stringify({ uploadUrl: url }))
```

### Client-Side Upload

```typescript
// Frontend code
async function uploadFile(file: File) {
  // Get presigned URL from your API
  const { uploadUrl } = await fetch('/api/upload-url', {
    method: 'POST',
    body: JSON.stringify({ filename: file.name, contentType: file.type }),
  }).then(r => r.json())

  // Upload directly to R2
  await fetch(uploadUrl, {
    method: 'PUT',
    body: file,
    headers: {
      'Content-Type': file.type,
    },
  })
}
```

## Multipart Uploads

For large files (> 5MB), use multipart uploads for reliability and resumability.

```typescript
import {
  S3Client,
  CreateMultipartUploadCommand,
  UploadPartCommand,
  CompleteMultipartUploadCommand,
  AbortMultipartUploadCommand,
} from '@dotdo/s3'

const client = new S3Client({ region: 'auto' })

async function uploadLargeFile(bucket: string, key: string, data: ArrayBuffer) {
  const partSize = 10 * 1024 * 1024 // 10MB parts
  const parts: { ETag: string; PartNumber: number }[] = []

  // Start multipart upload
  const { UploadId } = await client.send(new CreateMultipartUploadCommand({
    Bucket: bucket,
    Key: key,
  }))

  try {
    // Upload parts
    for (let i = 0; i * partSize < data.byteLength; i++) {
      const start = i * partSize
      const end = Math.min(start + partSize, data.byteLength)
      const partData = data.slice(start, end)

      const { ETag } = await client.send(new UploadPartCommand({
        Bucket: bucket,
        Key: key,
        UploadId,
        PartNumber: i + 1,
        Body: partData,
      }))

      parts.push({ ETag: ETag!, PartNumber: i + 1 })
    }

    // Complete upload
    await client.send(new CompleteMultipartUploadCommand({
      Bucket: bucket,
      Key: key,
      UploadId,
      MultipartUpload: { Parts: parts },
    }))

  } catch (error) {
    // Abort on failure
    await client.send(new AbortMultipartUploadCommand({
      Bucket: bucket,
      Key: key,
      UploadId,
    }))
    throw error
  }
}
```

## Connecting to Real AWS S3

If you need to interact with actual AWS S3 buckets:

```typescript
import { S3Client, GetObjectCommand } from '@dotdo/s3'

// Connect to real AWS S3
const awsClient = new S3Client({
  region: 'us-east-1',
  credentials: {
    accessKeyId: env.AWS_ACCESS_KEY_ID,
    secretAccessKey: env.AWS_SECRET_ACCESS_KEY,
  },
})

// Fetch from AWS S3
const response = await awsClient.send(new GetObjectCommand({
  Bucket: 'my-aws-bucket',
  Key: 'data.json',
}))
```

## SDK Compatibility

Works with S3-compatible libraries:

```typescript
// Works with multer-s3
import multer from 'multer'
import multerS3 from 'multer-s3'
import { S3Client } from '@dotdo/s3'

const upload = multer({
  storage: multerS3({
    s3: new S3Client({ region: 'auto' }),
    bucket: 'uploads',
    key: (req, file, cb) => {
      cb(null, `${Date.now()}-${file.originalname}`)
    },
  }),
})
```

## Error Handling

The S3 compat layer provides all standard S3 error types:

```typescript
import {
  S3Client,
  GetObjectCommand,
  S3ServiceException,
  NoSuchKey,
  NoSuchBucket,
  BucketAlreadyExists,
  BucketNotEmpty,
  NoSuchUpload,
  AccessDenied,
} from '@dotdo/s3'

const client = new S3Client({ region: 'auto' })

try {
  await client.send(new GetObjectCommand({
    Bucket: 'my-bucket',
    Key: 'missing.txt',
  }))
} catch (error) {
  // Type-specific error handling
  if (error instanceof NoSuchKey) {
    console.log('Object not found')
  } else if (error instanceof NoSuchBucket) {
    console.log('Bucket not found')
  } else if (error instanceof AccessDenied) {
    console.log('Permission denied')
  } else if (error instanceof S3ServiceException) {
    // Generic S3 error
    console.log('S3 error:', error.name, error.message)
    console.log('Fault:', error.$fault) // 'client' or 'server'
  }
}
```

All error classes extend `S3ServiceException` with:
- `name` - Error type (e.g., `NoSuchKey`)
- `message` - Human-readable description
- `$fault` - Either `'client'` (4xx) or `'server'` (5xx, retryable)
- `$metadata` - Response metadata including `httpStatusCode`

## Migration from AWS S3

### Package Change

```bash
# Remove AWS SDK
npm uninstall @aws-sdk/client-s3 @aws-sdk/s3-request-presigner

# Install dotdo S3
npm install @dotdo/s3
```

### Import Change

```typescript
// Before
import { S3Client, PutObjectCommand } from '@aws-sdk/client-s3'
import { getSignedUrl } from '@aws-sdk/s3-request-presigner'

// After
import { S3Client, PutObjectCommand } from '@dotdo/s3'
import { getSignedUrl } from '@dotdo/s3/presigner'
```

### Configuration

```typescript
// Before: AWS S3
const client = new S3Client({
  region: 'us-east-1',
  credentials: {
    accessKeyId: process.env.AWS_ACCESS_KEY_ID,
    secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY,
  },
})

// After: dotdo (R2 binding auto-injected)
const client = new S3Client({
  region: 'auto', // R2 is region-less
})
```

## Testing with @dotdo/s3

@dotdo/s3 is ideal for testing S3 operations without AWS infrastructure:

```typescript
import { S3Client, PutObjectCommand, GetObjectCommand } from '@dotdo/s3'
import { describe, it, expect, beforeEach } from 'vitest'

describe('FileStorage', () => {
  let client: S3Client

  beforeEach(() => {
    // Fresh client for each test
    client = new S3Client({ region: 'auto' })
  })

  it('should upload and retrieve a file', async () => {
    await client.send(new PutObjectCommand({
      Bucket: 'test-bucket',
      Key: 'test.txt',
      Body: 'Hello, World!',
    }))

    const response = await client.send(new GetObjectCommand({
      Bucket: 'test-bucket',
      Key: 'test.txt',
    }))

    const text = await response.Body?.transformToString()
    expect(text).toBe('Hello, World!')
  })
})
```

## Types

Full TypeScript support with AWS SDK-compatible types:

```typescript
import type {
  // Client config
  S3ClientConfig,
  ExtendedS3ClientConfig,
  ResponseMetadata,
  StreamingBody,

  // Bucket operations
  CreateBucketCommandInput,
  CreateBucketCommandOutput,
  DeleteBucketCommandInput,
  DeleteBucketCommandOutput,
  HeadBucketCommandInput,
  HeadBucketCommandOutput,
  ListBucketsCommandInput,
  ListBucketsCommandOutput,

  // Object operations
  PutObjectCommandInput,
  PutObjectCommandOutput,
  GetObjectCommandInput,
  GetObjectCommandOutput,
  HeadObjectCommandInput,
  HeadObjectCommandOutput,
  DeleteObjectCommandInput,
  DeleteObjectCommandOutput,
  DeleteObjectsCommandInput,
  DeleteObjectsCommandOutput,
  CopyObjectCommandInput,
  CopyObjectCommandOutput,
  ListObjectsV2CommandInput,
  ListObjectsV2CommandOutput,

  // Multipart operations
  CreateMultipartUploadCommandInput,
  CreateMultipartUploadCommandOutput,
  UploadPartCommandInput,
  UploadPartCommandOutput,
  CompleteMultipartUploadCommandInput,
  CompleteMultipartUploadCommandOutput,
  AbortMultipartUploadCommandInput,
  AbortMultipartUploadCommandOutput,
  ListPartsCommandInput,
  ListPartsCommandOutput,
  ListMultipartUploadsCommandInput,
  ListMultipartUploadsCommandOutput,

  // CORS types
  CORSRule,
  CORSConfiguration,

  // Lifecycle types
  LifecycleRule,
  LifecycleConfiguration,

  // Versioning types
  VersioningStatus,
  ObjectVersion,

  // Storage class
  StorageClass,
} from '@dotdo/s3'
```

## Related

- [s3.do](/docs/integrations/s3/service) - Managed S3 on the edge
- [Storage Overview](/docs/storage) - All storage options
- [Hot Tier Storage](/docs/storage/hot-tier) - High-performance storage with Durable Objects
- [Warm Tier Storage](/docs/storage/warm-tier) - R2 object storage
