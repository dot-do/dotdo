---
title: Google Cloud Storage
description: Drop-in replacement for @google-cloud/storage with edge compatibility, R2 backend, and signed URL support.
---

# Google Cloud Storage

Drop-in replacement for the Google Cloud Storage SDK (`@google-cloud/storage`). Your existing GCS code works unchanged - just swap the import.

```typescript
// Before: Google Cloud Storage SDK
import { Storage } from '@google-cloud/storage'

// After: dotdo
import { Storage } from '@dotdo/gcs'

// Code stays the same
const storage = new Storage({
  projectId: 'my-project',
  credentials: { client_email: '...', private_key: '...' },
})

const [bucket] = await storage.createBucket('my-bucket')
const file = bucket.file('hello.txt')
await file.save('Hello World')
```

## Why @dotdo/gcs?

| @google-cloud/storage | @dotdo/gcs |
|----------------------|------------|
| Node.js runtime required | Edge-compatible (Cloudflare Workers) |
| GCS only | In-memory or R2 backend |
| Requires GCP credentials | Local testing without credentials |
| External dependency | Zero network latency in local mode |
| Single region | Global edge deployment |

**This is a compatibility layer.** It can either run entirely locally using in-memory storage (for testing) or use R2 as the backend (for production). The API surface matches the official Google Cloud Storage SDK.

## Features

### Implemented

**Storage**
- `storage.createBucket()` - Create buckets
- `storage.getBuckets()` - List buckets
- `storage.bucket()` - Get bucket reference

**Bucket Operations**
- `bucket.exists()` - Check if bucket exists
- `bucket.delete()` - Delete bucket
- `bucket.getFiles()` - List files
- `bucket.file()` - Get file reference
- `bucket.setLifecycle()` - Set lifecycle rules
- `bucket.createNotification()` - Create Pub/Sub notification

**File Operations**
- `file.save()` - Upload content
- `file.download()` - Download content
- `file.delete()` - Delete file
- `file.copy()` - Copy file
- `file.move()` - Move file
- `file.exists()` - Check existence
- `file.getMetadata()` - Get metadata
- `file.setMetadata()` - Update metadata

**Streaming**
- `file.createReadStream()` - Stream downloads
- `file.createWriteStream()` - Stream uploads

**Signed URLs**
- `file.getSignedUrl()` - Generate signed URLs (v2 and v4)
- `generateSignedPostPolicyV4()` - Signed POST policies

**ACLs**
- `bucket.acl` - Bucket ACL management
- `file.acl` - Object ACL management
- `bucket.iam` - IAM policy management

### Not Yet Implemented

- HMAC key management
- Bucket locking
- Requester pays

## Quick Start

### Basic Usage

```typescript
import { Storage } from '@dotdo/gcs'

const storage = new Storage({
  projectId: 'my-project',
})

// Create a bucket
const [bucket] = await storage.createBucket('my-bucket', {
  location: 'US',
  storageClass: 'STANDARD',
})

// Upload a file
const file = bucket.file('documents/hello.txt')
await file.save('Hello World', {
  contentType: 'text/plain',
  metadata: {
    custom: 'value',
  },
})

// Download a file
const [content] = await file.download()
console.log(content.toString()) // 'Hello World'
```

### List and Query Files

```typescript
import { Storage } from '@dotdo/gcs'

const storage = new Storage({ projectId: 'my-project' })
const bucket = storage.bucket('my-bucket')

// List all files
const [files] = await bucket.getFiles()
for (const file of files) {
  console.log(file.name)
}

// List with prefix (like a directory)
const [docsFiles] = await bucket.getFiles({
  prefix: 'documents/',
  delimiter: '/',
})

// Paginated listing
const [filesPage, nextQuery] = await bucket.getFiles({
  maxResults: 100,
})
```

### Streaming Uploads and Downloads

```typescript
import { Storage } from '@dotdo/gcs'

const storage = new Storage({ projectId: 'my-project' })
const bucket = storage.bucket('my-bucket')

// Streaming upload
const writeStream = bucket.file('large-file.bin').createWriteStream({
  contentType: 'application/octet-stream',
  resumable: true,
})

// Pipe data to the stream
inputStream.pipe(writeStream)

writeStream.on('finish', () => {
  console.log('Upload complete')
})

// Streaming download
const readStream = bucket.file('large-file.bin').createReadStream()
readStream.pipe(outputStream)
```

### Copy and Move Files

```typescript
import { Storage } from '@dotdo/gcs'

const storage = new Storage({ projectId: 'my-project' })
const bucket = storage.bucket('my-bucket')

const sourceFile = bucket.file('original.txt')

// Copy within same bucket
await sourceFile.copy(bucket.file('copy.txt'))

// Copy to different bucket
const destBucket = storage.bucket('other-bucket')
await sourceFile.copy(destBucket.file('copy.txt'))

// Move (copy + delete original)
await sourceFile.move(bucket.file('renamed.txt'))
```

## Signed URLs

Generate signed URLs for temporary access:

```typescript
import { Storage } from '@dotdo/gcs'

const storage = new Storage({
  projectId: 'my-project',
  credentials: {
    client_email: 'service-account@project.iam.gserviceaccount.com',
    private_key: '-----BEGIN PRIVATE KEY-----\n...\n-----END PRIVATE KEY-----\n',
  },
})

const file = storage.bucket('my-bucket').file('private.pdf')

// V4 signed URL (recommended)
const [url] = await file.getSignedUrl({
  version: 'v4',
  action: 'read',
  expires: Date.now() + 15 * 60 * 1000, // 15 minutes
})

console.log('Download URL:', url)

// Signed URL for upload
const [uploadUrl] = await file.getSignedUrl({
  version: 'v4',
  action: 'write',
  expires: Date.now() + 60 * 60 * 1000, // 1 hour
  contentType: 'application/pdf',
})

// Signed POST policy for browser uploads
const [policy] = await generateSignedPostPolicyV4({
  bucket: 'my-bucket',
  file: 'uploads/${filename}',
  expires: Date.now() + 60 * 60 * 1000,
  conditions: [
    ['content-length-range', 0, 10485760], // Max 10MB
    ['starts-with', '$Content-Type', 'image/'],
  ],
})

// Use policy.url and policy.fields in HTML form
```

## Lifecycle Management

Configure automatic object lifecycle:

```typescript
import { Storage } from '@dotdo/gcs'

const storage = new Storage({ projectId: 'my-project' })
const bucket = storage.bucket('my-bucket')

// Set lifecycle rules
await bucket.setLifecycle([
  // Delete objects after 30 days
  {
    action: { type: 'Delete' },
    condition: { age: 30 },
  },
  // Move to NEARLINE after 7 days
  {
    action: { type: 'SetStorageClass', storageClass: 'NEARLINE' },
    condition: { age: 7 },
  },
  // Delete non-current versions after 90 days
  {
    action: { type: 'Delete' },
    condition: {
      isLive: false,
      numNewerVersions: 3,
    },
  },
])

// Get current lifecycle rules
const [metadata] = await bucket.getMetadata()
console.log(metadata.lifecycle)
```

## Notifications

Set up Pub/Sub notifications for bucket events:

```typescript
import { Storage } from '@dotdo/gcs'

const storage = new Storage({ projectId: 'my-project' })
const bucket = storage.bucket('my-bucket')

// Create notification
const [notification] = await bucket.createNotification({
  topic: 'projects/my-project/topics/bucket-events',
  eventTypes: [
    'OBJECT_FINALIZE',
    'OBJECT_DELETE',
    'OBJECT_ARCHIVE',
    'OBJECT_METADATA_UPDATE',
  ],
  objectNamePrefix: 'uploads/',
  payloadFormat: 'JSON_API_V1',
})

console.log('Notification ID:', notification.id)

// List notifications
const [notifications] = await bucket.getNotifications()

// Delete notification
await notification.delete()
```

## ACL Management

Manage access control:

```typescript
import { Storage } from '@dotdo/gcs'

const storage = new Storage({ projectId: 'my-project' })
const file = storage.bucket('my-bucket').file('document.pdf')

// Add ACL entry
await file.acl.add({
  entity: 'user-email@example.com',
  role: 'READER',
})

// Make file public
await file.makePublic()

// Get ACL
const [acl] = await file.acl.get()
console.log(acl)

// Remove ACL entry
await file.acl.delete({
  entity: 'user-email@example.com',
})
```

## Backend Configuration

### In-Memory Backend (Testing)

```typescript
import { Storage, resetToMemoryBackend } from '@dotdo/gcs'

// Ensure memory backend for tests
resetToMemoryBackend()

const storage = new Storage({ projectId: 'test-project' })

// All operations use in-memory storage
const [bucket] = await storage.createBucket('test-bucket')
await bucket.file('test.txt').save('test content')
```

### R2 Backend (Production)

```typescript
import { Storage, R2Backend, setDefaultBackend } from '@dotdo/gcs'

// Configure R2 backend
const r2Backend = new R2Backend({
  bucket: env.R2_BUCKET,
  prefix: 'gcs/', // Optional prefix for all objects
})

setDefaultBackend(r2Backend)

const storage = new Storage({ projectId: 'my-project' })
// Operations now use R2
```

## Error Handling

```typescript
import {
  Storage,
  NotFoundError,
  BucketNotFoundError,
  FileNotFoundError,
  ConflictError,
  isRetryableError,
} from '@dotdo/gcs'

const storage = new Storage({ projectId: 'my-project' })

try {
  await storage.bucket('missing').file('test.txt').download()
} catch (error) {
  if (error instanceof FileNotFoundError) {
    console.log('File does not exist')
  } else if (error instanceof BucketNotFoundError) {
    console.log('Bucket does not exist')
  } else if (isRetryableError(error)) {
    // Retry the operation
  }
}
```

## Types

```typescript
import type {
  // Storage options
  StorageOptions,
  ExtendedStorageOptions,
  StorageClass,

  // Bucket types
  CreateBucketOptions,
  GetBucketsOptions,
  BucketMetadata,

  // File types
  GetFilesOptions,
  FileMetadata,
  SaveOptions,
  CopyOptions,
  DownloadOptions,

  // Signed URL types
  SignedUrlConfig,
  GenerateSignedPostPolicyV4Options,
  SignedPostPolicyV4,

  // ACL types
  PredefinedBucketAcl,
  PredefinedObjectAcl,
  BucketAccessControl,
  ObjectAccessControl,

  // Lifecycle types
  LifecycleRule,
  LifecycleAction,
  LifecycleCondition,

  // Notification types
  NotificationConfig,
  NotificationMetadata,
} from '@dotdo/gcs'
```

## Migration from @google-cloud/storage

### 1. Update imports

```typescript
// Before
import { Storage } from '@google-cloud/storage'

// After
import { Storage } from '@dotdo/gcs'
```

### 2. Code changes

Most code works unchanged. Check for:

- **Credentials**: Can use in-memory mode without credentials for testing
- **Node.js streams**: Use web streams for edge compatibility
- **Local files**: Replace filesystem operations with R2

## Next Steps

<Cards>
  <Card title="S3 Integration" href="/docs/integrations/s3">
    AWS S3 compatibility layer with R2 backend.
  </Card>
  <Card title="Storage SDKs" href="/docs/compat/storage">
    Overview of all storage compatibility layers.
  </Card>
  <Card title="Workflows" href="/docs/workflows">
    Build durable file processing workflows.
  </Card>
</Cards>
