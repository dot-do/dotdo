---
title: OpenAI
description: Drop-in replacement for the OpenAI SDK with edge compatibility and automatic retries.
---

# OpenAI

Drop-in replacement for the official `openai` SDK. Your existing OpenAI code works unchanged - just swap the import.

```typescript
// Before: OpenAI
import OpenAI from 'openai'

// After: dotdo
import OpenAI from '@dotdo/openai'

// Code stays the same
const client = new OpenAI({ apiKey: 'sk-xxx' })
const completion = await client.chat.completions.create({
  model: 'gpt-4',
  messages: [{ role: 'user', content: 'Hello' }],
})
```

## Why @dotdo/openai?

| OpenAI SDK | openai.do |
|------------|-----------|
| Node.js only | Edge-compatible (Cloudflare Workers, Deno, Bun) |
| No built-in retries | Automatic retries with exponential backoff |
| Manual error handling | Typed errors with request IDs |

**This is a reimplementation.** The SDK provides 100% API compatibility while adding edge runtime support and automatic retries.

## Features

### Implemented

**Chat Completions**
- `chat.completions.create()` - Chat completions with GPT-4, GPT-3.5-turbo, etc.
- Streaming support via async iterables
- Function/tool calling
- Vision (image inputs)
- JSON mode

**Embeddings**
- `embeddings.create()` - Text embeddings with ada-002, text-embedding-3-small/large

**Images**
- `images.generate()` - DALL-E image generation
- `images.edit()` - Image editing with masks
- `images.createVariation()` - Image variations

**Models**
- `models.list()` - List available models
- `models.retrieve()` - Get model details
- `models.del()` - Delete fine-tuned models

**Assistants API**
- `AssistantsClient` with in-memory storage backend
- Assistant CRUD operations
- Thread management
- Message handling
- Run execution with tool calls
- Polling helpers (`runs.poll()`, `runs.createAndPoll()`)

### Not Yet Implemented

- Fine-tuning API
- Files API
- Batch API
- Audio (Whisper, TTS)
- Moderation

## Quick Start

### Install

```bash
npm install @dotdo/openai
```

### Chat Completions

```typescript
import OpenAI from '@dotdo/openai'

const client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY })

const completion = await client.chat.completions.create({
  model: 'gpt-4',
  messages: [
    { role: 'system', content: 'You are a helpful assistant.' },
    { role: 'user', content: 'What is the capital of France?' },
  ],
})

console.log(completion.choices[0].message.content)
// "The capital of France is Paris."
```

### Streaming

```typescript
const stream = await client.chat.completions.create({
  model: 'gpt-4',
  messages: [{ role: 'user', content: 'Tell me a story' }],
  stream: true,
})

for await (const chunk of stream) {
  const content = chunk.choices[0]?.delta?.content
  if (content) {
    process.stdout.write(content)
  }
}
```

#### Streaming with Error Handling

For production use, add proper error handling and timeout support:

```typescript
import OpenAI, { OpenAIError } from '@dotdo/openai'

const client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY })

// Create AbortController for timeout
const controller = new AbortController()
const timeoutId = setTimeout(() => controller.abort(), 30000) // 30s timeout

try {
  const stream = await client.chat.completions.create({
    model: 'gpt-4',
    messages: [{ role: 'user', content: 'Tell me a story' }],
    stream: true,
  }, {
    signal: controller.signal,
  })

  for await (const chunk of stream) {
    const content = chunk.choices[0]?.delta?.content
    if (content) {
      process.stdout.write(content)
    }
  }
} catch (error) {
  if (error instanceof Error && error.name === 'AbortError') {
    console.error('Stream timed out or was cancelled')
  } else if (error instanceof OpenAIError) {
    console.error(`OpenAI error [${error.status}]: ${error.message}`)
    // Handle specific errors (rate limit, auth, etc.)
    if (error.status === 429) {
      console.error('Rate limited - implement backoff')
    }
  } else {
    console.error('Stream error:', error)
  }
  throw error
} finally {
  clearTimeout(timeoutId)
}
```

### Embeddings

```typescript
const embedding = await client.embeddings.create({
  model: 'text-embedding-ada-002',
  input: 'Hello world',
})

console.log(embedding.data[0].embedding)
// [0.123, -0.456, 0.789, ...]

// Batch embeddings
const embeddings = await client.embeddings.create({
  model: 'text-embedding-3-small',
  input: ['Hello', 'World', 'How are you?'],
})
```

### Function Calling

```typescript
const completion = await client.chat.completions.create({
  model: 'gpt-4',
  messages: [{ role: 'user', content: 'What is the weather in San Francisco?' }],
  tools: [{
    type: 'function',
    function: {
      name: 'get_weather',
      description: 'Get the current weather for a location',
      parameters: {
        type: 'object',
        properties: {
          location: {
            type: 'string',
            description: 'City name',
          },
          unit: {
            type: 'string',
            enum: ['celsius', 'fahrenheit'],
          },
        },
        required: ['location'],
      },
    },
  }],
})

const message = completion.choices[0].message

if (message.tool_calls) {
  const toolCall = message.tool_calls[0]
  console.log(toolCall.function.name)
  // "get_weather"
  console.log(JSON.parse(toolCall.function.arguments))
  // { location: "San Francisco", unit: "fahrenheit" }
}
```

### Vision

```typescript
const completion = await client.chat.completions.create({
  model: 'gpt-4-vision-preview',
  messages: [
    {
      role: 'user',
      content: [
        { type: 'text', text: 'What is in this image?' },
        {
          type: 'image_url',
          image_url: { url: 'https://example.com/image.jpg' },
        },
      ],
    },
  ],
})
```

### Image Generation

```typescript
const response = await client.images.generate({
  model: 'dall-e-3',
  prompt: 'A sunset over mountains',
  size: '1024x1024',
  quality: 'hd',
})

console.log(response.data[0].url)
```

## Inbound - Using openai.do

Use `@dotdo/openai` as a drop-in replacement for the official SDK. Your existing code works unchanged.

```typescript
// Works with any OpenAI client code
import OpenAI from '@dotdo/openai'

const client = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
  organization: 'org-xxx',  // Optional
  baseURL: 'https://api.openai.com',  // Default
  timeout: 60000,  // Request timeout (ms)
  maxRetries: 2,  // Automatic retries
})
```

### Edge Runtime Compatibility

Unlike the official SDK, `@dotdo/openai` works in edge runtimes:

```typescript
// Cloudflare Workers
export default {
  async fetch(request: Request, env: Env) {
    const client = new OpenAI({ apiKey: env.OPENAI_API_KEY })

    const completion = await client.chat.completions.create({
      model: 'gpt-4',
      messages: [{ role: 'user', content: 'Hello from the edge!' }],
    })

    return Response.json(completion)
  },
}
```

```typescript
// Deno
import OpenAI from '@dotdo/openai'

const client = new OpenAI({ apiKey: Deno.env.get('OPENAI_API_KEY') })
```

## Outbound - Connecting to OpenAI

By default, requests go to OpenAI's API. You can also configure custom endpoints:

```typescript
// Default: OpenAI API
const client = new OpenAI({ apiKey: 'sk-xxx' })

// Azure OpenAI
const azureClient = new OpenAI({
  apiKey: process.env.AZURE_OPENAI_KEY,
  baseURL: 'https://your-resource.openai.azure.com/openai/deployments/your-deployment',
  defaultHeaders: {
    'api-version': '2024-02-01',
  },
})

// Local/self-hosted (vLLM, LocalAI, etc.)
const localClient = new OpenAI({
  apiKey: 'not-needed',
  baseURL: 'http://localhost:8000/v1',
})
```

## Multi-Provider Routing

For production use, route requests across multiple providers with automatic failover using `createOpenAIWithRuntime`:

```typescript
import { createOpenAIWithRuntime } from '@dotdo/openai'

const client = createOpenAIWithRuntime({
  providers: [
    { name: 'openai', apiKey: process.env.OPENAI_API_KEY },
    { name: 'anthropic', apiKey: process.env.ANTHROPIC_API_KEY },
  ],
  strategy: 'priority',  // 'priority' | 'round-robin' | 'least-latency' | 'cost-optimized'
  fallback: {
    enabled: true,
    maxAttempts: 2,
  },
})

// Same API - requests auto-route with failover
const completion = await client.chat.completions.create({
  model: 'gpt-4',
  messages: [{ role: 'user', content: 'Hello' }],
})

// Get routing statistics
const stats = client.getStats()
console.log(stats)
// {
//   totalRequests: 100,
//   totalTokens: 50000,
//   totalInputTokens: 35000,
//   totalOutputTokens: 15000,
//   totalCostUsd: 1.58,
//   averageLatencyMs: 445,
//   successRate: 1,
//   byProvider: {
//     openai: { requests: 95, tokens: 47500, costUsd: 1.50, errors: 2, averageLatencyMs: 450 },
//     anthropic: { requests: 5, tokens: 2500, costUsd: 0.08, errors: 0, averageLatencyMs: 380 }
//   },
//   byModel: {
//     'gpt-4': { requests: 80, inputTokens: 28000, outputTokens: 12000, costUsd: 1.35, averageLatencyMs: 480 }
//   }
// }
```

### Routing Strategies

| Strategy | Description |
|----------|-------------|
| `priority` | Use first provider, failover to next (default) |
| `round-robin` | Distribute requests evenly across providers |
| `least-latency` | Route to provider with lowest average latency |
| `cost-optimized` | Route based on model pricing (basic implementation) |

### Supported Providers

Currently supported providers for multi-provider routing:
- `openai` - OpenAI API
- `anthropic` - Anthropic Claude API

## Agent Integration

Named agents from `agents.do` can use OpenAI (or Anthropic) as their backend. By default, agents use the `ANTHROPIC_API_KEY` or `OPENAI_API_KEY` environment variable.

```typescript
import { priya, ralph, tom } from 'agents.do'

// Named agents work with OpenAI or Anthropic
const spec = await priya`define the MVP for ${hypothesis}`
let app = await ralph`build ${spec}`

// Tom can review and approve
const review = await tom.approve(app)
if (review.approved) {
  console.log('Code approved!')
}
```

### Named Agent Personas

| Agent | Role | Description |
|-------|------|-------------|
| Priya | Product | Specs, roadmaps, user stories |
| Ralph | Engineering | Builds and implements code |
| Tom | Tech Lead | Architecture, code review |
| Mark | Marketing | Content, launches, campaigns |
| Sally | Sales | Outreach, demos, closing |
| Quinn | QA | Testing, quality assurance |

### Configuring Agents

Configure agents to use specific models and parameters:

```typescript
import { ralph } from 'agents.do'

// Use GPT-4 with custom settings
const codeRalph = ralph.withConfig({
  model: 'gpt-4-turbo',
  temperature: 0.2,
  maxTokens: 4096,
})

const code = await codeRalph`implement a REST API for ${spec}`

// Streaming (simulated chunking of response)
for await (const chunk of ralph.stream`build ${feature}`) {
  process.stdout.write(chunk)
}
```

Note: The `.stream` method currently simulates streaming by chunking the full response. True provider-level streaming is planned.

## Assistants API

The `AssistantsClient` provides an OpenAI Assistants-compatible API with an in-memory storage backend. You can import it as a class or use the factory function:

```typescript
// Class import
import { AssistantsClient } from '@dotdo/openai'
const client = new AssistantsClient({ apiKey: process.env.OPENAI_API_KEY })

// Factory function (equivalent)
import { createAssistantsClient } from '@dotdo/openai'
const client = createAssistantsClient({ apiKey: process.env.OPENAI_API_KEY })
```

The client exposes four resource namespaces: `assistants`, `threads`, `messages`, and `runs`:

```typescript
import { AssistantsClient } from '@dotdo/openai'

const client = new AssistantsClient({
  apiKey: process.env.OPENAI_API_KEY,
})

// Create an assistant via client.assistants
const assistant = await client.assistants.create({
  name: 'Math Tutor',
  instructions: 'You are a helpful math tutor.',
  model: 'gpt-4-turbo',
  tools: [{ type: 'function', function: { name: 'calculate', parameters: {} } }],
})

// Create a thread via client.threads
const thread = await client.threads.create()

// Add a message via client.messages
await client.messages.create(thread.id, {
  role: 'user',
  content: 'Solve x^2 + 2x + 1 = 0',
})

// Create and poll for completion via client.runs
const run = await client.runs.createAndPoll(thread.id, {
  assistant_id: assistant.id,
})

// Get messages
const messages = await client.messages.list(thread.id)
console.log(messages.data[0].content)
```

### Tool Handling

When a run requires tool outputs, you can submit them:

```typescript
// Check if run requires action
if (run.status === 'requires_action') {
  const toolCalls = run.required_action?.submit_tool_outputs.tool_calls

  const toolOutputs = toolCalls.map((tc) => ({
    tool_call_id: tc.id,
    output: JSON.stringify({ result: 'computed value' }),
  }))

  await client.runs.submitToolOutputs(thread.id, run.id, {
    tool_outputs: toolOutputs,
  })
}
```

Note: The Assistants API uses local in-memory storage by default. Data does not persist across restarts.

## TypeScript Types

Full TypeScript support with OpenAI-compatible types:

```typescript
import type {
  // Chat
  ChatCompletion,
  ChatCompletionChunk,
  ChatCompletionCreateParams,
  ChatCompletionMessage,
  ChatCompletionMessageParam,
  ChatCompletionTool,
  ChatCompletionToolChoiceOption,

  // Embeddings
  Embedding,
  EmbeddingCreateParams,
  CreateEmbeddingResponse,

  // Images
  ImageCreateParams,
  ImagesResponse,

  // Models
  Model,
  ModelListResponse,

  // Common
  Usage,
  RequestOptions,

  // Assistants (selected types)
  Assistant,
  Thread,
  AssistantMessage,
  Run,
  RunStatus,
} from '@dotdo/openai'

// Client classes and errors
import {
  OpenAI,
  OpenAIError,
  AssistantsClient,
  createAssistantsClient,  // Factory function
  AssistantsAPIError,      // Assistants-specific error class
} from '@dotdo/openai'

// Multi-provider routing
import { createOpenAIWithRuntime } from '@dotdo/openai'
```

## Error Handling

```typescript
import OpenAI, { OpenAIError } from '@dotdo/openai'

try {
  const completion = await client.chat.completions.create({
    model: 'gpt-4',
    messages: [{ role: 'user', content: 'Hello' }],
  })
} catch (error) {
  if (error instanceof OpenAIError) {
    console.error('Status:', error.status)
    console.error('Type:', error.type)
    console.error('Code:', error.code)
    console.error('Message:', error.message)
    console.error('Request ID:', error.requestId)

    // Handle specific errors
    if (error.status === 429) {
      console.error('Rate limited - retry later')
    } else if (error.status === 401) {
      console.error('Invalid API key')
    }
  }
}
```

### Assistants API Errors

The Assistants API throws `AssistantsAPIError` for assistants-specific errors:

```typescript
import { AssistantsClient, AssistantsAPIError } from '@dotdo/openai'

try {
  const assistant = await client.assistants.retrieve('invalid-id')
} catch (error) {
  if (error instanceof AssistantsAPIError) {
    console.error('Status:', error.status)   // 404
    console.error('Code:', error.code)       // 'not_found'
    console.error('Type:', error.type)       // 'invalid_request_error'
    console.error('Message:', error.message)
  }
}
```

## Configuration

```typescript
const client = new OpenAI({
  // Required
  apiKey: 'sk-xxx',

  // Optional
  organization: 'org-xxx',
  baseURL: 'https://api.openai.com',
  timeout: 600000,  // 10 minutes (default)
  maxRetries: 2,    // Automatic retries (default)

  // Custom headers
  defaultHeaders: {
    'X-Custom-Header': 'value',
  },

  // Custom fetch (for testing/mocking)
  fetch: customFetch,
})
```

## Migration from Official SDK

### Package Change

```bash
# Remove
npm uninstall openai

# Install
npm install @dotdo/openai
```

### Import Change

```typescript
// Before
import OpenAI from 'openai'

// After
import OpenAI from '@dotdo/openai'
```

All method signatures and types are identical - no code changes required.

## Related

- [Anthropic](/docs/integrations/anthropic) - Claude API integration
- [Named Agents](/docs/agents/named-agents) - Priya, Ralph, Tom, Mark, Sally, Quinn
- [Agent SDK](/docs/agents) - Multi-provider agent system
