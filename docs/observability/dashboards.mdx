---
title: Dashboards and Alerting
description: Cloudflare Analytics integration, custom dashboards, and production alerting
---

# Dashboards and Alerting

You have the data. Now you need to see it. dotdo integrates with Cloudflare Analytics and supports custom dashboards for your observability stack.

## Cloudflare Analytics

Built-in analytics are available in the Cloudflare dashboard with zero configuration.

### Workers Analytics

Navigate to **Workers & Pages > Analytics** to see:

- Request volume and error rates
- CPU time and duration percentiles
- Geographic distribution
- Status code breakdown

### Durable Objects Analytics

Navigate to **Workers & Pages > Durable Objects** for:

- Active DO count
- Storage usage per DO
- Requests per DO
- WebSocket connections

### R2 Analytics

For storage metrics, see **R2 > Analytics**:

- Object count and storage size
- Read/write operations
- Egress bandwidth (free!)

## Custom Dashboards

### GraphQL API

Query your metrics programmatically:

```typescript
const query = `
  query {
    viewer {
      accounts(filter: { accountTag: $accountId }) {
        workersInvocationsAdaptive(
          filter: { datetime_geq: $startTime, datetime_leq: $endTime }
          limit: 1000
        ) {
          dimensions {
            scriptName
            status
          }
          sum {
            requests
            errors
          }
          quantiles {
            cpuTimeP50
            cpuTimeP99
          }
        }
      }
    }
  }
`

const response = await fetch('https://api.cloudflare.com/client/v4/graphql', {
  method: 'POST',
  headers: {
    'Authorization': `Bearer ${CF_API_TOKEN}`,
    'Content-Type': 'application/json'
  },
  body: JSON.stringify({ query, variables: { accountId, startTime, endTime } })
})
```

### Export to External Systems

Send metrics to your observability platform:

```typescript
export class MyStartup extends Startup {
  observability = {
    metrics: {
      exporters: [
        {
          type: 'prometheus',
          endpoint: '/metrics',  // Expose Prometheus endpoint
          prefix: 'dotdo_'
        },
        {
          type: 'datadog',
          apiKey: process.env.DD_API_KEY,
          site: 'datadoghq.com'
        },
        {
          type: 'newrelic',
          licenseKey: process.env.NR_LICENSE_KEY,
          accountId: process.env.NR_ACCOUNT_ID
        }
      ]
    }
  }
}
```

Supported exporters:
- Prometheus (pull-based)
- Datadog
- New Relic
- Grafana Cloud
- Honeycomb
- OpenTelemetry Collector

## Building Dashboards

### Essential Panels

Every dotdo dashboard should have:

**System Health**
```typescript
// Error rate
$.metrics.query({
  metric: 'api.requests',
  labels: { status: { match: '5*' } },
  rate: true
})

// Request latency p99
$.metrics.query({
  metric: 'api.requests.duration',
  percentile: 99
})

// Active DOs
$.metrics.query({
  metric: 'do.active',
  aggregation: 'max'
})
```

**Agent Performance**
```typescript
// Agent call rate
$.metrics.query({
  metric: 'agent.calls',
  groupBy: 'agent',
  rate: true
})

// Agent latency by agent
$.metrics.query({
  metric: 'agent.duration',
  groupBy: 'agent',
  percentile: 99
})

// Token consumption
$.metrics.query({
  metric: 'agent.tokens',
  groupBy: 'agent',
  aggregation: 'sum'
})
```

**Business Metrics**
```typescript
// Orders per minute
$.metrics.query({
  metric: 'orders.created',
  rate: true,
  interval: '1m'
})

// Revenue (if tracking)
$.metrics.query({
  metric: 'revenue',
  aggregation: 'sum',
  groupBy: 'plan'
})

// HUNCH summary
$.hunch.summary({ timeRange: { last: '24h' } })
```

### Grafana Dashboard

Example Grafana JSON for a dotdo dashboard:

```json
{
  "title": "dotdo Overview",
  "panels": [
    {
      "title": "Request Rate",
      "type": "graph",
      "targets": [
        {
          "expr": "rate(dotdo_api_requests_total[5m])",
          "legendFormat": "{{status}}"
        }
      ]
    },
    {
      "title": "Agent Latency p99",
      "type": "graph",
      "targets": [
        {
          "expr": "histogram_quantile(0.99, rate(dotdo_agent_duration_bucket[5m]))",
          "legendFormat": "{{agent}}"
        }
      ]
    },
    {
      "title": "Error Rate",
      "type": "stat",
      "targets": [
        {
          "expr": "sum(rate(dotdo_api_requests_total{status=~\"5..\"}[5m])) / sum(rate(dotdo_api_requests_total[5m]))"
        }
      ],
      "thresholds": {
        "steps": [
          { "value": 0, "color": "green" },
          { "value": 0.01, "color": "yellow" },
          { "value": 0.05, "color": "red" }
        ]
      }
    }
  ]
}
```

## Alerting

### Built-in Alerts

Define alerts in your configuration:

```typescript
export class MyStartup extends Startup {
  observability = {
    alerts: [
      {
        name: 'high-error-rate',
        description: 'Error rate exceeds 5%',
        metric: 'api.requests',
        condition: {
          expression: 'rate(status=~"5.*") / rate(*) > 0.05',
          window: '5m',
          threshold: 'for 2m'  // Must persist for 2 minutes
        },
        severity: 'critical',
        notify: ['pagerduty:critical', 'slack:#incidents']
      },
      {
        name: 'slow-agents',
        description: 'Agent p99 latency exceeds 30s',
        metric: 'agent.duration',
        condition: {
          percentile: 99,
          threshold: 30000,
          window: '10m'
        },
        severity: 'warning',
        notify: ['slack:#alerts']
      },
      {
        name: 'high-token-usage',
        description: 'Token consumption spike',
        metric: 'agent.tokens',
        condition: {
          rateChange: 2.0,  // 2x normal rate
          window: '1h'
        },
        severity: 'info',
        notify: ['email:ops@example.com.ai']
      }
    ]
  }
}
```

### Alert Channels

Configure notification channels:

```typescript
export class MyStartup extends Startup {
  observability = {
    alertChannels: {
      slack: {
        webhookUrl: process.env.SLACK_WEBHOOK,
        defaultChannel: '#alerts'
      },
      pagerduty: {
        routingKey: process.env.PAGERDUTY_KEY,
        severity: {
          critical: 'critical',
          warning: 'warning',
          info: 'info'
        }
      },
      email: {
        smtp: {
          host: 'smtp.example.com.ai',
          port: 587,
          user: process.env.SMTP_USER,
          password: process.env.SMTP_PASS
        },
        from: 'alerts@example.com.ai'
      },
      webhook: {
        url: 'https://example.com.ai/alerts',
        headers: { 'Authorization': 'Bearer ...' }
      }
    }
  }
}
```

### Alert Format

Alerts include full context:

```json
{
  "alert": "high-error-rate",
  "severity": "critical",
  "status": "firing",
  "startsAt": "2024-01-15T14:32:00Z",
  "description": "Error rate exceeds 5%",
  "value": 0.073,
  "threshold": 0.05,
  "labels": {
    "environment": "production",
    "region": "us-west"
  },
  "annotations": {
    "dashboard": "https://dash.example.com.ai/errors",
    "runbook": "https://docs.example.com.ai/runbooks/high-error-rate"
  }
}
```

### Silencing

Suppress alerts during maintenance:

```typescript
// Silence for 1 hour
await $.alerts.silence({
  matchers: [{ name: 'high-error-rate' }],
  duration: '1h',
  reason: 'Scheduled maintenance'
})

// Silence by label
await $.alerts.silence({
  matchers: [{ labels: { region: 'eu-west' } }],
  until: '2024-01-16T06:00:00Z',
  reason: 'EU datacenter maintenance'
})
```

## On-Call Integration

### PagerDuty

```typescript
export class MyStartup extends Startup {
  observability = {
    oncall: {
      provider: 'pagerduty',
      serviceId: process.env.PAGERDUTY_SERVICE_ID,
      escalationPolicy: process.env.PAGERDUTY_POLICY_ID,

      routing: [
        { match: { severity: 'critical' }, escalate: 'immediately' },
        { match: { severity: 'warning' }, escalate: 'after 15m' }
      ]
    }
  }
}
```

### Opsgenie

```typescript
export class MyStartup extends Startup {
  observability = {
    oncall: {
      provider: 'opsgenie',
      apiKey: process.env.OPSGENIE_KEY,
      team: 'platform-team',

      priority: {
        critical: 'P1',
        warning: 'P3',
        info: 'P5'
      }
    }
  }
}
```

## Debugging Workflows

### Incident Response

When an alert fires:

```typescript
// 1. Get recent errors
const errors = await $.logs.search({
  level: 'error',
  timeRange: { last: '15m' }
})

// 2. Find affected traces
const traces = await $.trace.search({
  status: 'error',
  timeRange: { last: '15m' }
})

// 3. Check metric trends
const errorTrend = await $.metrics.query({
  metric: 'api.requests',
  labels: { status: { match: '5*' } },
  interval: '1m',
  timeRange: { last: '1h' }
})

// 4. Identify root cause
const analysis = await $.analyze.incident({
  alerts: ['high-error-rate'],
  timeRange: { last: '30m' }
})
// Returns correlated logs, metrics, and traces
```

### Live Tail

Watch logs in real-time:

```bash
npx dotdo logs --follow --filter 'level:error'
npx dotdo logs --follow --filter 'agent:priya'
npx dotdo logs --follow --filter 'customerId:cus_123'
```

### Trace Viewer

Open a trace in the browser:

```bash
npx dotdo trace tr_abc123
# Opens: https://dash.dotdo.dev/traces/tr_abc123
```

## Best Practices

### Set Meaningful Thresholds

```typescript
// Good: Based on SLOs
{
  name: 'slo-breach',
  description: 'Latency SLO at risk',
  condition: {
    percentile: 99,
    threshold: 1000,  // 1s = our SLO
    window: '5m'
  }
}

// Bad: Arbitrary threshold
{
  name: 'slow-requests',
  condition: {
    percentile: 99,
    threshold: 500,  // Why 500ms?
    window: '1m'     // Too short, too noisy
  }
}
```

### Avoid Alert Fatigue

```typescript
// Good: Actionable, infrequent
alerts: [
  { name: 'critical-outage', severity: 'critical', /* fires ~0/month */ },
  { name: 'degraded-service', severity: 'warning', /* fires ~2/week */ }
]

// Bad: Too many, too sensitive
alerts: [
  { name: 'any-error', severity: 'critical', /* fires 50x/day */ }
]
```

### Document Runbooks

Link alerts to runbooks:

```typescript
{
  name: 'database-connection-pool-exhausted',
  annotations: {
    runbook: 'https://docs.example.com.ai/runbooks/db-pool',
    dashboard: 'https://grafana.example.com.ai/d/db-health'
  }
}
```

## Related

- [Logging](/docs/observability/logging) - Structured logs for debugging
- [Metrics](/docs/observability/metrics) - Counters, gauges, and histograms
- [Tracing](/docs/observability/tracing) - Distributed traces
