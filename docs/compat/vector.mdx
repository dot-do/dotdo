---
title: Vector
description: API-compatible vector database SDK for Pinecone, backed by edge-native HNSW.
---

# Vector Compat SDKs

Pinecone-compatible vector API, rebuilt on Durable Objects with edge-native HNSW. Embeddings that scale with your agents.

## The Problem

When 10,000 AI agents start generating and querying embeddings:

- **Pinecone** rate limits per namespace
- Cloud vector databases add latency at the edge
- Cold starts on serverless vector services

The solution: edge-native vector storage using HNSW (Hierarchical Navigable Small World) graphs backed by SQLite. Hot vectors in libsql (F32_BLOB), with optional tiering to Cloudflare Vectorize or Parquet for cold storage.

## Quick Start

```typescript
import { Pinecone } from '@dotdo/pinecone'

const pinecone = new Pinecone({ apiKey: process.env.PINECONE_KEY })
const index = pinecone.index('products')

// Upsert vectors
await index.upsert([
  {
    id: 'doc-1',
    values: embedding, // 1536 dimensions for text-embedding-3-small
    metadata: { title: 'Product Guide', category: 'docs' },
  },
])

// Query
const results = await index.query({
  vector: queryEmbedding,
  topK: 10,
  filter: { category: 'docs' },
  includeMetadata: true,
})
```

## Available SDK

### @dotdo/pinecone

Drop-in replacement for `@pinecone-database/pinecone`, backed by dotdo's edgevec HNSW implementation.

```typescript
import { Pinecone } from '@dotdo/pinecone'
import { embed } from 'dotdo/ai'

const pinecone = new Pinecone({
  apiKey: process.env.PINECONE_KEY,
})

// Get or create index
const index = pinecone.index('knowledge-base')

// Upsert vectors
await index.upsert([
  {
    id: 'doc-1',
    values: await embed('How to reset password'),
    metadata: {
      title: 'Password Reset Guide',
      category: 'support',
      lastUpdated: '2024-01-15',
    },
  },
  {
    id: 'doc-2',
    values: await embed('Account settings overview'),
    metadata: {
      title: 'Account Settings',
      category: 'settings',
      lastUpdated: '2024-01-10',
    },
  },
])

// Query with metadata filter
const results = await index.query({
  vector: await embed('how do I change my password'),
  topK: 5,
  filter: {
    category: { $eq: 'support' },
  },
  includeMetadata: true,
  includeValues: false,
})

for (const match of results.matches) {
  console.log(`${match.id}: ${match.score}`)
  console.log(match.metadata)
}

// Namespace isolation
const ns = index.namespace('tenant-123')
await ns.upsert([{ id: 'doc-1', values: embedding }])
const nsResults = await ns.query({ vector: queryEmbedding, topK: 5 })

// Fetch vectors by ID
const vectors = await index.fetch(['doc-1', 'doc-2'])

// Delete vectors
await index.deleteOne('doc-1')
await index.deleteMany(['doc-2', 'doc-3'])
await index.deleteAll() // Clear entire index

// Describe index
const stats = await index.describeIndexStats()
console.log(`Total vectors: ${stats.totalRecordCount}`)
```

Filter operators:
- `$eq`, `$ne` - Equality
- `$gt`, `$gte`, `$lt`, `$lte` - Comparison
- `$in`, `$nin` - Set membership
- `$and`, `$or` - Boolean

## Architecture

### Tiered Vector Storage

Vectors tier automatically based on access patterns:

```
┌─────────────────────────────────────────────────────────────────┐
│                         Query                                    │
│              index.query({ vector, topK: 10 })                   │
└─────────────────────────────────────────────────────────────────┘
                                │
                                ▼
┌─────────────────────────────────────────────────────────────────┐
│                      VectorRouter                                │
│         Strategy: cascade | parallel | smart                     │
└─────────────────────────────────────────────────────────────────┘
                                │
                    ┌───────────┼───────────┐
                    ▼           ▼           ▼
               ┌────────┐  ┌────────┐  ┌────────┐
               │  Hot   │  │  Warm  │  │  Cold  │
               │ libsql │  │Vectorize│ │Parquet │
               │ < 10ms │  │ < 50ms │  │< 200ms │
               └────────┘  └────────┘  └────────┘
```

### Storage Tiers

```typescript
const router = new VectorRouter(env, {
  tiers: {
    hot: {
      engine: 'libsql',
      dimensions: 1536,
      metric: 'cosine',
    },
    warm: {
      engine: 'vectorize',
      dimensions: 1536,
      metric: 'cosine',
    },
    cold: {
      engine: 'parquet',
      dimensions: 1536,
      metric: 'cosine',
    },
  },
  routing: {
    strategy: 'cascade', // Try hot first, fall back to colder
    fallback: true,
  },
})
```

### Routing Strategies

**Cascade** (default): Try hot tier first, fall back if results insufficient.

```typescript
const router = new VectorRouter(env, {
  routing: { strategy: 'cascade', fallback: true },
})
```

**Parallel**: Query all tiers simultaneously, merge results.

```typescript
const router = new VectorRouter(env, {
  routing: { strategy: 'parallel' },
})
```

**Smart**: Use query characteristics to pick best tier.

```typescript
const router = new VectorRouter(env, {
  routing: {
    strategy: 'smart',
    // Hot for real-time queries
    // Cold for batch analytics
  },
})
```

## Distance Metrics

All compat SDKs support three distance metrics:

### Cosine Similarity

Best for normalized embeddings (OpenAI, Cohere).

```typescript
// Range: -1 to 1 (higher = more similar)
const similarity = cosineSimilarity(a, b)
```

### Euclidean Distance

Best for unnormalized vectors or spatial data.

```typescript
// Range: 0 to infinity (lower = more similar)
const distance = euclideanDistance(a, b)
```

### Dot Product

Best for maximum inner product search (MIPS).

```typescript
// Range: -infinity to infinity (higher = more similar)
const product = dotProduct(a, b)
```

## Embedding Models

Common embedding dimensions:

| Model | Provider | Dimensions | Use Case |
|-------|----------|------------|----------|
| text-embedding-3-small | OpenAI | 1536 | General purpose |
| text-embedding-3-large | OpenAI | 3072 | High accuracy |
| embed-english-v3.0 | Cohere | 1024 | English text |
| embed-multilingual-v3.0 | Cohere | 1024 | Multilingual |
| all-MiniLM-L6-v2 | HuggingFace | 384 | Fast, lightweight |
| bge-large-en-v1.5 | BAAI | 1024 | High quality |

### Generate Embeddings

```typescript
import { embed } from 'dotdo/ai'

// Using OpenAI
const embedding = await embed('Your text here', {
  model: 'text-embedding-3-small',
})

// Using Workers AI
const embedding = await embed('Your text here', {
  model: '@cf/baai/bge-large-en-v1.5',
})

// Batch embedding
const embeddings = await embedBatch([
  'First document',
  'Second document',
  'Third document',
])
```

## Patterns

### RAG (Retrieval-Augmented Generation)

```typescript
import { embed } from 'dotdo/ai'

async function rag(question: string) {
  // 1. Embed the question
  const questionEmbedding = await embed(question)

  // 2. Find relevant documents
  const results = await index.query({
    vector: questionEmbedding,
    topK: 5,
    includeMetadata: true,
  })

  // 3. Build context
  const context = results.matches
    .map((m) => m.metadata.content)
    .join('\n\n')

  // 4. Generate answer
  const answer = await complete({
    model: 'gpt-4o',
    messages: [
      {
        role: 'system',
        content: `Answer based on this context:\n\n${context}`,
      },
      { role: 'user', content: question },
    ],
  })

  return answer
}
```

### Semantic Cache

Cache LLM responses by semantic similarity:

```typescript
import { embed } from 'dotdo/ai'

async function cachedComplete(prompt: string) {
  const embedding = await embed(prompt)

  // Check cache
  const cached = await index.query({
    vector: embedding,
    topK: 1,
    filter: { type: 'cache' },
    includeMetadata: true,
  })

  if (cached.matches[0]?.score > 0.95) {
    return cached.matches[0].metadata.response
  }

  // Generate new response
  const response = await complete({ messages: [{ role: 'user', content: prompt }] })

  // Cache it
  await index.upsert([{
    id: `cache-${Date.now()}`,
    values: embedding,
    metadata: { type: 'cache', prompt, response },
  }])

  return response
}
```

### Hybrid Search

Combine vector similarity with keyword search:

```typescript
import { embed } from 'dotdo/ai'

async function hybridSearch(query: string) {
  const embedding = await embed(query)

  // Vector search
  const vectorResults = await vectorIndex.query({
    vector: embedding,
    topK: 20,
  })

  // Keyword search
  const keywordResults = await searchIndex.search({
    query: { match: { content: query } },
    size: 20,
  })

  // Reciprocal rank fusion
  const scores = new Map<string, number>()
  const k = 60 // RRF constant

  vectorResults.matches.forEach((match, i) => {
    const score = 1 / (k + i + 1)
    scores.set(match.id, (scores.get(match.id) || 0) + score)
  })

  keywordResults.hits.forEach((hit, i) => {
    const score = 1 / (k + i + 1)
    scores.set(hit._id, (scores.get(hit._id) || 0) + score)
  })

  return Array.from(scores.entries())
    .sort((a, b) => b[1] - a[1])
    .slice(0, 10)
}
```

### Multi-Tenant Isolation

Use namespaces for tenant isolation:

```typescript
// Each tenant gets their own namespace
const tenantIndex = index.namespace(`tenant-${tenantId}`)

// Vectors are isolated
await tenantIndex.upsert([{ id: 'doc-1', values: embedding }])

// Queries only see tenant's vectors
const results = await tenantIndex.query({
  vector: queryEmbedding,
  topK: 10,
})
```

## Performance

| Operation | Latency | Notes |
|-----------|---------|-------|
| Insert (single) | < 20ms | Hot tier |
| Insert (batch) | < 100ms | 100 vectors |
| Query (hot) | < 10ms | libsql F32_BLOB |
| Query (warm) | < 50ms | Vectorize |
| Query (cold) | < 200ms | ClickHouse ANN |

### Optimization Tips

1. **Batch inserts** - Group inserts for better throughput
2. **Use namespaces** - Isolate tenants for faster queries
3. **Filter early** - Metadata filters reduce search space
4. **Right-size topK** - Don't over-fetch results

## Migration Guide

### From Pinecone

```typescript
// Before: Pinecone
import { Pinecone } from '@pinecone-database/pinecone'
const pinecone = new Pinecone({ apiKey: 'xxx' })

// After: dotdo
import { Pinecone } from '@dotdo/pinecone'
const pinecone = new Pinecone({ apiKey: process.env.DOTDO_KEY })

// Code stays the same
const index = pinecone.index('my-index')
await index.upsert([{ id: 'doc-1', values: embedding }])
```

### From Qdrant Cloud

```typescript
// Before: Qdrant Cloud
import { QdrantClient } from '@qdrant/js-client-rest'
const client = new QdrantClient({ url: 'https://xxx.qdrant.io' })

// After: dotdo
import { QdrantClient } from '@dotdo/qdrant'
const client = new QdrantClient({ url: process.env.DOTDO_QDRANT_URL })

// Code stays the same
await client.upsert('collection', { points })
```

### From Weaviate Cloud

```typescript
// Before: Weaviate Cloud
import weaviate from 'weaviate-ts-client'
const client = weaviate.client({ host: 'xxx.weaviate.network' })

// After: dotdo
import weaviate from '@dotdo/weaviate'
const client = weaviate.client({ host: process.env.DOTDO_WEAVIATE_HOST })

// Code stays the same
await client.data.creator().withClassName('Article').do()
```

## Limitations

What's different from managed services:

- **No sparse vectors** - Dense embeddings only
- **Max 16K dimensions** - Sufficient for all common models
- **No reranking** - Implement in application code
- **Limited to 100M vectors per namespace** - Shard if needed

## Individual Integrations

Detailed guides for specific vector database services:

- [Pinecone](/docs/integrations/pinecone) - Pinecone SDK with HNSW backend
- [Weaviate](/docs/integrations/weaviate) - Weaviate client compatibility

## Related Categories

- [Database SDKs](/docs/compat/databases) - Postgres, MySQL, MongoDB
- [Messaging SDKs](/docs/compat/messaging) - Kafka, Redis, NATS
- [Search SDKs](/docs/compat/search) - Elasticsearch, Algolia, Meilisearch
