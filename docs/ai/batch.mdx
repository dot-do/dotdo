---
title: Batch Processing
description: Process multiple AI operations with immediate, flex, and deferred batch modes for cost optimization
---

# Batch Processing

The batch system processes multiple items through AI operations with different cost/speed tradeoffs. Choose the mode that matches your latency requirements and budget.

## Batch Modes

| Mode | Cost Multiplier | Budget Cost | Use Case |
|------|-----------------|-------------|----------|
| `immediate` | 1.0x | 1.0 per item | Real-time, user-facing operations |
| `flex` | 0.5x | 0.5 per item | Background processing with some urgency |
| `deferred` | 0.25x | 0.25 per item | Bulk processing, no time pressure |

## Type Definition

```typescript
type BatchMode = 'immediate' | 'flex' | 'deferred'

interface BatchResult<T> extends PromiseLike<T[]> {
  batchId: string
}

// Overloaded signatures
batch<T>(items: T[], mode: BatchMode): BatchResult<string> & Promise<string[]>
batch<T, R>(items: T[], mode: BatchMode, template: (item: T) => AIPromise<R>): Promise<R[]>
```

## Basic Usage

```typescript
import { ai } from 'dotdo/ai'

const items = ['Review 1 text', 'Review 2 text', 'Review 3 text']

// Process immediately (highest cost, lowest latency)
const results = await ai.batch(items, 'immediate')

// Process with flexibility (medium cost, medium latency)
const results = await ai.batch(items, 'flex')

// Process when convenient (lowest cost, highest latency)
const results = await ai.batch(items, 'deferred')
```

## Custom Templates

Apply a specific AI template to each item:

```typescript
import { ai, is, list } from 'dotdo/ai'

const reviews = [
  'Great product, love it!',
  'Terrible experience, never again',
  'It was okay, nothing special'
]

// Classify each review
const sentiments = await ai.batch(
  reviews,
  'immediate',
  (review) => is`Classify as positive, negative, or neutral: ${review}`
)
// Returns: ['positive', 'negative', 'neutral']

// Extract keywords from each
const keywordLists = await ai.batch(
  reviews,
  'flex',
  (review) => list`Extract keywords: ${review}`
)
// Returns: [['great', 'product', 'love'], ['terrible', 'experience'], ['okay']]
```

## Batch Result Object

The batch function returns a `BatchResult` that is also a `Promise`. The batch ID format is `batch-{timestamp}-{random}`:

```typescript
const batch = ai.batch(items, 'immediate')

// Get the batch ID for tracking
console.log(batch.batchId) // "batch-1705329876543-a1b2c3"

// Await results
const results = await batch
```

## Tracking Batch Status

Monitor batch progress with the status function. Status is tracked per-AI-instance to avoid race conditions in Cloudflare Workers:

```typescript
const batch = ai.batch(items, 'deferred')

// Check status
const status = await ai.batch.status(batch.batchId)
// Returns: 'pending' | 'processing' | 'completed'

// Status transitions:
// 'pending' -> 'processing' -> 'completed'

// Poll for completion
while (await ai.batch.status(batch.batchId) !== 'completed') {
  await new Promise(resolve => setTimeout(resolve, 1000))
}

const results = await batch
```

<Callout type="warn">
If a batch fails, status still transitions to `'completed'`. Check for errors by awaiting the batch.
</Callout>

## Mode Selection Guide

### Immediate Mode

Use for user-facing operations where latency matters:

```typescript
// Chat responses
const responses = await ai.batch(
  userMessages,
  'immediate',
  (msg) => ai`Respond to: ${msg}`
)

// Real-time classification
const categories = await ai.batch(
  incomingTickets,
  'immediate',
  (ticket) => is`Classify urgency (low/medium/high): ${ticket}`
)
```

### Flex Mode

Use for background tasks with reasonable time constraints:

```typescript
// Email summaries (within minutes)
const summaries = await ai.batch(
  emails,
  'flex',
  (email) => ai`Summarize: ${email.body}`
)

// Content moderation queue
const moderationResults = await ai.batch(
  pendingPosts,
  'flex',
  (post) => is`Is this content appropriate? ${post.content}`
)
```

### Deferred Mode

Use for bulk processing where cost is the priority:

```typescript
// Nightly data processing
const enriched = await ai.batch(
  allProducts,
  'deferred',
  (product) => ai`Generate SEO description: ${product.name}`
)

// Historical analysis
const analyses = await ai.batch(
  historicalRecords,
  'deferred',
  (record) => ai`Analyze trends in: ${JSON.stringify(record)}`
)
```

## Cost Optimization Patterns

### Mixed Mode Processing

Prioritize some items while batch processing others:

```typescript
const items = [
  { data: 'urgent item', priority: 'high' },
  { data: 'normal item', priority: 'low' },
  { data: 'another normal', priority: 'low' },
]

// Split by priority
const urgent = items.filter(i => i.priority === 'high')
const normal = items.filter(i => i.priority === 'low')

// Process urgent immediately, normal deferred
const [urgentResults, normalResults] = await Promise.all([
  ai.batch(urgent.map(i => i.data), 'immediate'),
  ai.batch(normal.map(i => i.data), 'deferred'),
])
```

### Budget-Aware Batching

Combine with budget tracking:

```typescript
import { createAI } from 'dotdo/ai'

const ai = createAI({
  budget: { limit: 100 }
})

// Deferred mode stretches budget 4x further
// 100 budget units = 400 deferred operations vs 100 immediate
const results = await ai.batch(items, 'deferred')

console.log('Spent:', ai.budget.spent)
console.log('Remaining:', ai.budget.remaining)
```

## Error Handling

Batch operations fail fast by default. Errors include the batch ID for debugging:

```typescript
try {
  const results = await ai.batch(items, 'immediate')
} catch (error) {
  // Error format: "Batch batch-123456-abc failed: <reason>"
  console.error('Batch failed:', error.message)
}
```

For partial results, process individually with `Promise.allSettled`:

```typescript
const results = await Promise.allSettled(
  items.map(item => ai`Process: ${item}`)
)

const successful = results
  .filter(r => r.status === 'fulfilled')
  .map(r => r.value)

const failed = results
  .filter(r => r.status === 'rejected')
  .map(r => r.reason)
```

Budget errors during batch processing also fail fast:

```typescript
import { createAI, AIBudgetExceededError } from 'dotdo/ai'

const ai = createAI({ budget: { limit: 5 } })

try {
  // If 10 items exceed budget, fails mid-batch
  await ai.batch(tenItems, 'immediate')
} catch (error) {
  if (error.cause instanceof AIBudgetExceededError) {
    console.log('Budget exhausted during batch')
  }
}
```

## Performance Considerations

1. **Batch size**: Larger batches are more efficient but increase memory usage
2. **Mode selection**: Choose based on actual latency requirements, not assumptions
3. **Parallelization**: Batch mode handles parallelization internally
4. **Caching**: Cached results bypass batch processing entirely

```typescript
// With caching enabled, repeated items are instant
const ai = createAI({
  cache: { enabled: true, ttl: 300000 }
})

// Second batch with same items returns cached results
const results1 = await ai.batch(items, 'immediate')
const results2 = await ai.batch(items, 'immediate') // Instant from cache
```

## Next Steps

- [Budget Tracking](./budget) - Monitor costs across batch operations
- [Caching](./caching) - Reduce costs with response caching
- [Providers](./providers) - Configure batch processing providers
