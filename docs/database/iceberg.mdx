---
title: Iceberg Storage
description: Apache Iceberg format for cold storage with fast point lookups and partition pruning
---

# Iceberg Storage

dotdo uses Apache Iceberg format for cold storage in R2. The IcebergReader provides fast point lookups (50-150ms) by navigating Iceberg metadata directly, bypassing R2 SQL for single-record retrieval.

## Architecture

```
+-------------------------------------------------+
|  DO SQLite (Hot Index Layer)                     |
|  - Bloom filters for existence checks            |
|  - Min/max stats for partition pruning           |
|  - Recent data in columnar format                |
+-----------------------+-------------------------+
                        |
                        v  (Compact when threshold reached)
+-----------------------+-------------------------+
|  R2 Iceberg (Cold Storage)                       |
|  things/                                         |
|  ├── metadata.json                               |
|  ├── manifests/                                  |
|  │   ├── manifest-list-1.avro                    |
|  │   └── manifest-file-1.avro                    |
|  └── data/                                       |
|      ├── type=User/                              |
|      │   ├── dt=2024-01-01/data-001.parquet      |
|      │   └── dt=2024-01-02/data-002.parquet      |
|      └── type=Order/                             |
|          └── ...                                 |
+-------------------------------------------------+
```

## Navigation Chain

Direct Iceberg navigation achieves 50-150ms latency vs 500ms-2s for R2 SQL:

```
metadata.json          →  ~10-50ms
    ↓
current-snapshot-id
    ↓
manifest-list.avro     →  ~10-50ms
    ↓
filter by partition (ns + type)
    ↓
manifest-file.avro     →  ~10-50ms
    ↓
column stats → which Parquet has id?
    ↓
data-file.parquet      →  ~10-50ms (optional)
```

## IcebergReader

Fast point lookups in Iceberg tables:

```typescript
import { IcebergReader } from 'db/iceberg'

const reader = new IcebergReader(env.R2)

// Find which Parquet file contains a record
const file = await reader.findFile({
  table: 'do_resources',
  partition: { ns: 'payments.do', type: 'Function' },
  id: 'charge'
})

if (file) {
  console.log(`Found in: ${file.path}`)
}

// Get the actual record data
const record = await reader.getRecord({
  table: 'do_resources',
  partition: { ns: 'payments.do', type: 'Function' },
  id: 'charge'
})

if (record) {
  console.log(record.esm)  // Compiled JavaScript
  console.log(record.dts)  // TypeScript definitions
}
```

### Configuration

```typescript
const reader = new IcebergReader(env.R2, {
  basePath: 'iceberg/',       // Base path in R2 bucket
  cacheMetadata: true,        // Cache metadata in memory
  cacheTtlMs: 60000           // Cache TTL (1 minute)
})
```

### Type-Safe Records

```typescript
interface FunctionRecord extends IcebergRecord {
  id: string
  ns: string
  type: string
  esm: string
  dts: string
  mdast?: object
  hast?: object
}

const record = await reader.getRecord<FunctionRecord>({
  table: 'do_resources',
  partition: { ns: 'payments.do', type: 'Function' },
  id: 'charge',
  columns: ['id', 'esm', 'dts']  // Optional column projection
})
```

## Partition Strategy

Tables are partitioned for efficient pruning:

```sql
CREATE TABLE do_resources (
  ns STRING,
  type STRING,
  id STRING,
  ts TIMESTAMP,
  mdx STRING,
  data STRUCT<...>,
  esm STRING,
  dts STRING,
  mdast STRUCT<...>,
  hast STRUCT<...>,
  estree STRUCT<...>,
  tsast STRUCT<...>,
  html STRING,
  markdown STRING
)
PARTITIONED BY (ns, type, visibility)
```

### Partition Order

1. **ns (namespace)** - Primary: Tenant isolation at storage level
2. **type** - Secondary: Resource type filtering
3. **visibility** - Tertiary: Access control pruning

This enables efficient manifest pruning:
- Public queries skip manifests with only user/org data
- Private queries skip manifests with only public data
- Unlisted data excluded from general listings

## Manifest Parsing

### Metadata

```typescript
interface IcebergMetadata {
  formatVersion: number
  tableUuid: string
  location: string
  lastUpdatedMs: number
  lastColumnId: number
  schemas: Schema[]
  currentSchemaId: number
  partitionSpecs: PartitionSpec[]
  defaultSpecId: number
  lastPartitionId: number
  properties: Record<string, string>
  currentSnapshotId: number | null
  snapshots: Snapshot[]
  snapshotLog: SnapshotLogEntry[]
  sortOrders: SortOrder[]
  defaultSortOrderId: number
}
```

### Snapshots

```typescript
interface Snapshot {
  snapshotId: number
  parentSnapshotId?: number
  timestampMs: number
  manifestList: string
  summary: {
    operation: 'append' | 'replace' | 'overwrite' | 'delete'
    addedDataFiles: number
    deletedDataFiles: number
    addedRecords: number
    deletedRecords: number
  }
}
```

### Manifest Files

```typescript
interface ManifestFile {
  manifestPath: string
  manifestLength: number
  partitionSpecId: number
  content: 'data' | 'deletes'
  addedSnapshotId: number
  addedDataFilesCount: number
  existingDataFilesCount: number
  deletedDataFilesCount: number
  addedRowsCount: number
  existingRowsCount: number
  deletedRowsCount: number
  partitions: PartitionFieldSummary[]
}
```

### Data File Entries

```typescript
interface DataFileEntry {
  content: 'data' | 'positionDeletes' | 'equalityDeletes'
  filePath: string
  fileFormat: 'parquet' | 'orc' | 'avro'
  partition: Record<string, unknown>
  recordCount: number
  fileSizeInBytes: number
  columnSizes?: Record<number, number>
  valueCounts?: Record<number, number>
  nullValueCounts?: Record<number, number>
  nanValueCounts?: Record<number, number>
  lowerBounds?: Record<number, Uint8Array>
  upperBounds?: Record<number, Uint8Array>
  sortOrderId?: number
}
```

## Column Statistics

Data files contain min/max statistics for efficient pruning:

```typescript
// Reading data file entry
const entry: DataFileEntry = await manifest.getDataFileEntry(index)

// Check if file might contain our ID
const idFieldId = 3  // 'id' column field ID
const idLower = entry.lowerBounds?.[idFieldId]
const idUpper = entry.upperBounds?.[idFieldId]

if (idLower && idUpper) {
  const min = decodeString(idLower)
  const max = decodeString(idUpper)

  if (targetId < min || targetId > max) {
    // Skip this file - ID definitely not here
    continue
  }
}
```

## Parquet Reading

Read Parquet files with optional column projection:

```typescript
import { ParquetReader } from 'db/iceberg/parquet'

const parquetReader = new ParquetReader(env.R2)

const records = await parquetReader.read(filePath, {
  columns: ['id', 'esm', 'dts'],  // Only read these columns
  filter: (row) => row.id === targetId,
  limit: 1
})
```

## Inverted Index

Fast ID lookups with inverted indexes:

```typescript
import { InvertedIndex } from 'db/iceberg/inverted-index'

const index = new InvertedIndex()

// Build index from manifest
for (const file of manifestFiles) {
  index.addFile(file.filePath, file.lowerBounds, file.upperBounds)
}

// Quick lookup
const candidateFiles = index.lookup('charge')
// Returns files that might contain 'charge'
```

## Puffin Statistics

Puffin files store column-level statistics:

```typescript
import { PuffinGenerator } from 'db/iceberg/puffin-generator'

const generator = new PuffinGenerator()

// Generate statistics file
const puffin = await generator.generate(dataFile, {
  includeBloom: true,
  includeHistogram: true
})

// Write to R2
await env.R2.put(puffinPath, puffin.data)
```

## Use Cases

### Event History

Retrieve DO event stream without SQL:

```typescript
const events = await reader.getRecords({
  table: 'events',
  partition: { ns: 'payments.do', type: 'Payment' },
  filter: { after: lastSeenId },
  limit: 1000
})
```

### DO Cloning

Copy state from one DO to another:

```typescript
const records = await reader.getAllRecords({
  table: 'do_resources',
  partition: { ns: 'source.do' }
})

for (const record of records) {
  await targetDO.import(record)
}
```

### Artifact Lookup

Get compiled code by ID:

```typescript
const func = await reader.getRecord<FunctionArtifact>({
  table: 'do_resources',
  partition: { ns: 'myapp.do', type: 'Function' },
  id: 'processPayment'
})

const module = await import(func.esm)
```

## Caching

Metadata is cached with configurable TTL:

```typescript
const reader = new IcebergReader(env.R2, {
  cacheMetadata: true,
  cacheTtlMs: 120000  // 2 minute cache
})

// First lookup: reads metadata from R2
await reader.getRecord({ ... })  // ~150ms

// Second lookup: uses cached metadata
await reader.getRecord({ ... })  // ~50ms
```

## Performance

| Operation | Latency | Notes |
|-----------|---------|-------|
| Point lookup (cached) | ~50ms | Metadata in cache |
| Point lookup (cold) | ~150ms | Full navigation chain |
| Batch lookup (10 records) | ~200ms | Parallelized |
| Full partition scan | ~500ms+ | Depends on size |

### Optimization Tips

1. **Use column projection** - Only request columns you need
2. **Enable metadata caching** - Reduces repeated lookups
3. **Partition wisely** - Put high-cardinality fields in partition key
4. **Compact regularly** - Smaller files = faster lookups

## Related

- [Storage Tiers](/docs/storage) - Hot, warm, and cold storage
- [Query Accelerator](/docs/database/primitives) - Bloom filters and min/max indexes
- [EdgeVec](/docs/database/edgevec) - Vector search with Parquet compaction
