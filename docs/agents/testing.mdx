---
title: Agent Testing
description: Comprehensive testing strategies for AI agents - mocks, integration, CI/CD
---

import { Callout } from 'fumadocs-ui/components/callout'
import { Tab, Tabs } from 'fumadocs-ui/components/tabs'

# Agent Testing

Testing AI agents requires balancing determinism with real-world validation. This guide covers the complete testing strategy for dotdo agents, from fast unit tests to production-like integration tests.

## Testing Philosophy

AI agents present unique testing challenges:

| Challenge | Solution |
|-----------|----------|
| Non-deterministic outputs | Mock providers for unit tests, flexible assertions for integration |
| Expensive API calls | Run integration tests selectively, use cheaper models |
| Tool side effects | Tracked tools record calls without execution |
| Conversation context | Isolated providers per test |

## Testing Utilities

Import testing utilities from `agents/testing`:

```typescript
import {
  createMockProvider,
  createIsolatedMockProvider,
  createMockTool,
  createTrackedTool,
  mockResponses,
  fixtures,
  expectAgentResult,
  collectStreamEvents,
} from 'agents/testing'
```

## Unit Testing with Mocks

### Creating Mock Providers

Mock providers return predetermined responses in sequence:

```typescript
import { describe, it, expect } from 'vitest'
import { createMockProvider, mockResponses, fixtures } from 'agents/testing'

describe('Contract Reviewer Agent', () => {
  it('returns text response', async () => {
    const provider = createMockProvider({
      responses: [
        mockResponses.text('Found 3 risky clauses in the contract.')
      ]
    })

    const agent = provider.createAgent({
      id: 'reviewer',
      name: 'Lexi',
      instructions: 'Review contracts for risks.',
      model: 'mock',
    })

    const result = await agent.run({ prompt: 'Review this NDA' })

    expect(result.text).toBe('Found 3 risky clauses in the contract.')
    expect(result.finishReason).toBe('stop')
  })
})
```

### Mock Response Types

The `mockResponses` helper creates different response types:

<Tabs items={['Text', 'Tool Call', 'Multiple Tools', 'Error']}>
  <Tab value="Text">
```typescript
// Simple text response
mockResponses.text('The analysis is complete.')

// With custom token usage
mockResponses.text('Short response', {
  promptTokens: 50,
  completionTokens: 10,
  totalTokens: 60,
})
```
  </Tab>
  <Tab value="Tool Call">
```typescript
// Single tool call
mockResponses.toolCall('searchDatabase', {
  query: 'contract templates',
  limit: 10,
})

// With accompanying text
mockResponses.toolCall('analyze', { doc: 'contract.pdf' }, {
  text: 'Let me analyze that document...',
})
```
  </Tab>
  <Tab value="Multiple Tools">
```typescript
// Parallel tool calls
mockResponses.toolCalls([
  { name: 'searchLegal', args: { query: 'liability' } },
  { name: 'searchLegal', args: { query: 'indemnity' } },
  { name: 'fetchTemplate', args: { type: 'NDA' } },
])
```
  </Tab>
  <Tab value="Error">
```typescript
// Simulate API errors
mockResponses.error('Rate limit exceeded')

// Max tokens exceeded
mockResponses.maxTokens('Partial response that got cut off...')
```
  </Tab>
</Tabs>

### Testing Tool Execution

Test that agents call tools correctly and handle results:

```typescript
import { z } from 'zod'
import { tool, hasToolCall } from 'agents'
import { createMockProvider, mockResponses, createTrackedTool } from 'agents/testing'

describe('Agent Tool Usage', () => {
  const searchTool = tool({
    name: 'search',
    description: 'Search legal database',
    inputSchema: z.object({
      query: z.string(),
      jurisdiction: z.string().optional(),
    }),
    execute: async ({ query, jurisdiction }) => ({
      results: [`Case 1 for ${query}`, `Case 2 for ${query}`],
      jurisdiction: jurisdiction ?? 'federal',
    }),
  })

  it('executes tool and continues', async () => {
    const provider = createMockProvider({
      responses: [
        mockResponses.toolCall('search', { query: 'non-compete' }),
        mockResponses.text('Based on the search results, non-competes are enforceable.'),
      ],
    })

    const agent = provider.createAgent({
      id: 'legal-agent',
      name: 'Lexi',
      instructions: 'Search before answering.',
      model: 'mock',
      tools: [searchTool],
    })

    const result = await agent.run({ prompt: 'Are non-competes enforceable?' })

    expect(result.toolCalls).toHaveLength(1)
    expect(result.toolCalls[0].name).toBe('search')
    expect(result.toolResults[0].result.results).toHaveLength(2)
    expect(result.text).toContain('enforceable')
    expect(result.steps).toBe(2)
  })

  it('stops when specific tool is called', async () => {
    const finishTool = tool({
      name: 'complete',
      description: 'Mark review as complete',
      inputSchema: z.object({ summary: z.string() }),
      execute: async ({ summary }) => ({ done: true, summary }),
    })

    const provider = createMockProvider({
      responses: [
        mockResponses.toolCall('search', { query: 'liability' }),
        mockResponses.toolCall('complete', { summary: 'Review finished' }),
      ],
    })

    const agent = provider.createAgent({
      id: 'stop-agent',
      name: 'Lexi',
      instructions: 'Search then complete.',
      model: 'mock',
      tools: [searchTool, finishTool],
      stopWhen: hasToolCall('complete'),
    })

    const result = await agent.run({ prompt: 'Review contract' })

    expect(result.toolCalls).toHaveLength(2)
    expect(result.steps).toBe(2)
  })
})
```

### Tracked Tools

Use tracked tools to record calls without complex mocking:

```typescript
import { createTrackedTool } from 'agents/testing'

describe('Tool Call Tracking', () => {
  it('records all tool invocations', async () => {
    const [searchTool, searchCalls] = createTrackedTool('search', {
      results: ['result1', 'result2']
    })

    const provider = createMockProvider({
      responses: [
        mockResponses.toolCall('search', { query: 'first' }),
        mockResponses.toolCall('search', { query: 'second' }),
        mockResponses.text('Done searching'),
      ],
    })

    const agent = provider.createAgent({
      id: 'tracker',
      name: 'Test',
      instructions: 'Search multiple times.',
      model: 'mock',
      tools: [searchTool],
    })

    await agent.run({ prompt: 'Search twice' })

    expect(searchCalls).toHaveLength(2)
    expect(searchCalls[0].input).toEqual({ query: 'first' })
    expect(searchCalls[1].input).toEqual({ query: 'second' })
    expect(searchCalls[0].timestamp).toBeInstanceOf(Date)
  })
})
```

### Testing Hooks

Verify that agent hooks are called correctly:

```typescript
import { vi } from 'vitest'

describe('Agent Hooks', () => {
  it('onPreToolUse can deny calls', async () => {
    const onPreToolUse = vi.fn().mockResolvedValue({
      action: 'deny',
      reason: 'Tool disabled during testing',
    })

    const provider = createMockProvider({
      responses: [
        mockResponses.toolCall('dangerousTool', {}),
        mockResponses.text('Handled the denial'),
      ],
    })

    const agent = provider.createAgent({
      id: 'hook-test',
      name: 'Test',
      instructions: 'Use tools.',
      model: 'mock',
      tools: [dangerousTool],
      hooks: { onPreToolUse },
    })

    const result = await agent.run({ prompt: 'Try the tool' })

    expect(onPreToolUse).toHaveBeenCalled()
    expect(result.toolResults[0].error).toBe('Tool disabled during testing')
  })

  it('onPostToolUse logs results', async () => {
    const onPostToolUse = vi.fn()

    // ... setup and run agent ...

    expect(onPostToolUse).toHaveBeenCalledWith(
      expect.objectContaining({ name: 'search' }),
      expect.objectContaining({ result: expect.any(Object) })
    )
  })
})
```

## Integration Testing

Integration tests verify agent behavior with real LLM providers.

### Conditional Test Execution

Run integration tests only when API keys are available:

```typescript
import { describe, it, expect } from 'vitest'
import { createClaudeProvider } from 'agents/providers/claude'
import { createOpenAIProvider } from 'agents/providers/openai'

describe('Real LLM Integration', () => {
  const hasClaudeKey = !!process.env.ANTHROPIC_API_KEY
  const hasOpenAIKey = !!process.env.OPENAI_API_KEY

  it.skipIf(!hasClaudeKey)('Claude generates code', async () => {
    const provider = createClaudeProvider({
      apiKey: process.env.ANTHROPIC_API_KEY,
    })

    const agent = provider.createAgent({
      id: 'code-gen',
      name: 'Ralph',
      instructions: 'Generate TypeScript code.',
      model: 'claude-sonnet-4-20250514',
    })

    const result = await agent.run({
      prompt: 'Write a function to validate email addresses'
    })

    expect(result.text).toMatch(/function|const|=>/)
    expect(result.text.toLowerCase()).toContain('email')
  }, { timeout: 30000 })

  it.skipIf(!hasOpenAIKey)('OpenAI analyzes data', async () => {
    const provider = createOpenAIProvider({
      apiKey: process.env.OPENAI_API_KEY,
    })

    const agent = provider.createAgent({
      id: 'analyst',
      name: 'Finn',
      instructions: 'Analyze financial data.',
      model: 'gpt-4o',
    })

    const result = await agent.run({
      prompt: 'What are key metrics to track for SaaS revenue?'
    })

    expect(result.text.length).toBeGreaterThan(100)
    expect(result.text.toLowerCase()).toMatch(/mrr|arr|churn|revenue/)
  }, { timeout: 30000 })
})
```

### Cost-Conscious Testing

<Callout type="warning" title="API Costs">
Integration tests incur real API costs. Use these strategies to minimize spending:
</Callout>

```typescript
describe('Cost-Conscious Integration Tests', () => {
  // Use cheaper models for most tests
  const cheapProvider = createClaudeProvider({
    apiKey: process.env.ANTHROPIC_API_KEY,
    defaultModel: 'claude-haiku-3-20240307', // Cheapest option
  })

  // Limit max steps to prevent runaway loops
  const agent = cheapProvider.createAgent({
    id: 'cost-test',
    name: 'Test',
    instructions: 'Be concise.',
    model: 'claude-haiku-3-20240307',
    maxSteps: 3, // Never run more than 3 steps
  })

  // Use short prompts
  it('answers briefly', async () => {
    const result = await agent.run({
      prompt: 'Yes or no: Is 2+2=4?'
    })

    expect(result.text.toLowerCase()).toMatch(/yes/)
  })
})
```

Cost estimates per 1M tokens:

| Provider | Model | Input | Output |
|----------|-------|-------|--------|
| Anthropic | Claude Haiku 3 | $0.25 | $1.25 |
| Anthropic | Claude Sonnet 4 | $3.00 | $15.00 |
| Anthropic | Claude Opus 4 | $15.00 | $75.00 |
| OpenAI | GPT-4o-mini | $0.15 | $0.60 |
| OpenAI | GPT-4o | $2.50 | $10.00 |

## Handling Non-Determinism

### Flexible Assertions

LLM outputs vary between runs. Use flexible matching:

```typescript
describe('Flexible Assertions', () => {
  it('checks for semantic content, not exact text', async () => {
    const result = await agent.run({
      prompt: 'List 3 benefits of unit testing'
    })

    // Bad: Exact match fails with LLMs
    // expect(result.text).toBe('1. Catches bugs early...')

    // Good: Check for expected concepts
    const benefits = ['bug', 'refactor', 'document', 'confidence', 'regression']
    const foundBenefits = benefits.filter(b =>
      result.text.toLowerCase().includes(b)
    )

    expect(foundBenefits.length).toBeGreaterThanOrEqual(2)
  })

  it('uses regex for flexible matching', async () => {
    const result = await agent.run({
      prompt: 'What is 2 + 2?'
    })

    // Match various ways of saying "4"
    expect(result.text).toMatch(/\b4\b|four|2\s*\+\s*2\s*=\s*4/)
  })
})
```

### Temperature Control

Use temperature 0 for more deterministic outputs:

```typescript
it('produces consistent output with temperature 0', async () => {
  const provider = createClaudeProvider({
    apiKey: process.env.ANTHROPIC_API_KEY,
  })

  const agent = provider.createAgent({
    id: 'deterministic',
    name: 'Test',
    instructions: 'Answer with exactly one word.',
    model: 'claude-sonnet-4-20250514',
    providerOptions: { temperature: 0 },
  })

  // Run multiple times - should be more consistent
  const results = await Promise.all([
    agent.run({ prompt: 'Is the sky blue? One word.' }),
    agent.run({ prompt: 'Is the sky blue? One word.' }),
  ])

  // Both should give similar answers
  expect(results[0].text.toLowerCase()).toMatch(/yes/)
  expect(results[1].text.toLowerCase()).toMatch(/yes/)
})
```

### Retry Patterns

Handle flaky tests with retries:

```typescript
import { retry } from 'vitest'

describe('Retry Flaky Tests', () => {
  it('retries on inconsistent output', async () => {
    await retry(async () => {
      const result = await agent.run({
        prompt: 'Generate a random number between 1-10'
      })

      // Sometimes the LLM might add extra text
      expect(result.text).toMatch(/[1-9]|10/)
    }, { retries: 3, delay: 1000 })
  })
})
```

## Snapshot and Golden File Testing

### Structure Snapshots

Snapshot test the structure of responses, not exact content:

```typescript
describe('Structure Snapshots', () => {
  it('produces expected output structure', async () => {
    const provider = createMockProvider({
      responses: [
        mockResponses.text(JSON.stringify({
          analysis: 'Contract analysis summary',
          risks: ['risk1', 'risk2'],
          recommendations: ['rec1'],
          confidence: 0.85,
        }))
      ]
    })

    const agent = provider.createAgent({
      id: 'structured',
      name: 'Lexi',
      instructions: 'Return JSON analysis.',
      model: 'mock',
    })

    const result = await agent.run({ prompt: 'Analyze contract' })
    const parsed = JSON.parse(result.text)

    // Snapshot structure, not values
    expect(parsed).toMatchObject({
      analysis: expect.any(String),
      risks: expect.any(Array),
      recommendations: expect.any(Array),
      confidence: expect.any(Number),
    })

    expect(parsed.confidence).toBeGreaterThan(0)
    expect(parsed.confidence).toBeLessThanOrEqual(1)
  })
})
```

### Golden File Pattern

Compare against baseline files for regression testing:

```typescript
import { readFileSync, writeFileSync, existsSync, mkdirSync } from 'fs'
import { join } from 'path'

const GOLDEN_DIR = join(__dirname, '__goldens__')

function compareWithGolden(
  name: string,
  actual: string,
  options: { update?: boolean; similarity?: number } = {}
) {
  const { update = false, similarity = 0.6 } = options
  const goldenPath = join(GOLDEN_DIR, `${name}.txt`)

  if (!existsSync(GOLDEN_DIR)) {
    mkdirSync(GOLDEN_DIR, { recursive: true })
  }

  if (update || !existsSync(goldenPath)) {
    writeFileSync(goldenPath, actual)
    console.log(`Updated golden file: ${name}`)
    return
  }

  const expected = readFileSync(goldenPath, 'utf-8')

  // Calculate word-level similarity
  const actualWords = new Set(actual.toLowerCase().split(/\s+/).filter(w => w.length > 2))
  const expectedWords = new Set(expected.toLowerCase().split(/\s+/).filter(w => w.length > 2))
  const intersection = [...actualWords].filter(w => expectedWords.has(w))
  const sim = intersection.length / Math.max(actualWords.size, expectedWords.size)

  expect(sim).toBeGreaterThan(similarity)
}

describe('Golden File Tests', () => {
  it('matches baseline NDA review', async () => {
    const result = await agent.run({ prompt: 'Review standard NDA clauses' })

    compareWithGolden('nda-review', result.text, {
      update: process.env.UPDATE_GOLDENS === 'true',
      similarity: 0.5, // Lower threshold for LLM variance
    })
  })
})
```

## CI/CD Integration

### GitHub Actions Workflow

```yaml
# .github/workflows/agent-tests.yml
name: Agent Tests

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

env:
  NODE_VERSION: '22'
  PNPM_VERSION: '10'

jobs:
  # Fast unit tests run on every push
  unit-tests:
    name: Unit Tests (Mocked)
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v4
      - uses: pnpm/action-setup@v4
        with:
          version: ${{ env.PNPM_VERSION }}
      - uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'pnpm'
      - run: pnpm install --frozen-lockfile
      - run: pnpm vitest run --project=agents
        env:
          AGENT_TEST_MODE: mock

  # Integration tests only on main to control costs
  integration-tests:
    name: Integration Tests (Real LLM)
    runs-on: ubuntu-latest
    timeout-minutes: 15
    if: github.ref == 'refs/heads/main'
    steps:
      - uses: actions/checkout@v4
      - uses: pnpm/action-setup@v4
        with:
          version: ${{ env.PNPM_VERSION }}
      - uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'pnpm'
      - run: pnpm install --frozen-lockfile
      - name: Run Integration Tests
        run: pnpm vitest run agents/tests/ai-integration.test.ts
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          AGENT_TEST_MODE: integration

  # Weekly comprehensive tests
  full-integration:
    name: Full Integration Suite
    runs-on: ubuntu-latest
    timeout-minutes: 30
    if: github.event_name == 'schedule'
    steps:
      - uses: actions/checkout@v4
      - uses: pnpm/action-setup@v4
        with:
          version: ${{ env.PNPM_VERSION }}
      - uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'pnpm'
      - run: pnpm install --frozen-lockfile
      - run: pnpm vitest run agents/
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          AGENT_TEST_MODE: integration
```

### API Key Management

<Callout type="info" title="Security Best Practices">
Never commit API keys. Use GitHub Secrets with these precautions:
</Callout>

1. **Create test-specific API keys** - Don't use production keys in CI
2. **Set spending limits** - Configure in provider dashboards:
   - Anthropic Console: Usage limits per API key
   - OpenAI Platform: Usage limits per project
3. **Restrict key permissions** - Disable fine-tuning, limit to specific models
4. **Rotate regularly** - Schedule key rotation every 90 days
5. **Monitor usage** - Set up alerts for unusual spending

### Test Mode Configuration

Create a configuration helper for different environments:

```typescript
// agents/testing/config.ts
import { createMockProvider, mockResponses } from 'agents/testing'
import { createClaudeProvider } from 'agents/providers/claude'
import { createOpenAIProvider } from 'agents/providers/openai'
import type { AgentProvider } from 'agents/types'

export type TestMode = 'mock' | 'integration' | 'production'

export function getTestProvider(mode?: TestMode): AgentProvider {
  const effectiveMode = mode ?? (process.env.AGENT_TEST_MODE as TestMode) ?? 'mock'

  switch (effectiveMode) {
    case 'mock':
      return createMockProvider({
        responses: [mockResponses.text('Mock response for testing')]
      })

    case 'integration':
      // Use cheapest available provider for integration tests
      if (process.env.ANTHROPIC_API_KEY) {
        return createClaudeProvider({
          apiKey: process.env.ANTHROPIC_API_KEY,
          defaultModel: 'claude-haiku-3-20240307',
        })
      }
      if (process.env.OPENAI_API_KEY) {
        return createOpenAIProvider({
          apiKey: process.env.OPENAI_API_KEY,
          defaultModel: 'gpt-4o-mini',
        })
      }
      throw new Error(
        'Integration test mode requires ANTHROPIC_API_KEY or OPENAI_API_KEY'
      )

    case 'production':
      if (!process.env.ANTHROPIC_API_KEY) {
        throw new Error('Production mode requires ANTHROPIC_API_KEY')
      }
      return createClaudeProvider({
        apiKey: process.env.ANTHROPIC_API_KEY,
        defaultModel: 'claude-sonnet-4-20250514',
      })

    default:
      throw new Error(`Unknown test mode: ${effectiveMode}`)
  }
}

// Usage in tests
describe('Multi-mode Tests', () => {
  const provider = getTestProvider()

  it('works in any mode', async () => {
    const agent = provider.createAgent({
      id: 'multi-mode',
      name: 'Test',
      instructions: 'Respond briefly.',
      model: 'mock', // Overridden by provider
    })

    const result = await agent.run({ prompt: 'Hello' })
    expect(result.text).toBeDefined()
    expect(result.text.length).toBeGreaterThan(0)
  })
})
```

## Best Practices Summary

1. **Use mock providers for unit tests** - Fast, deterministic, free
2. **Run integration tests selectively** - Only on main branch or manually
3. **Use cheap models in CI** - Claude Haiku, GPT-4o-mini
4. **Limit max steps** - Prevent runaway agent loops
5. **Use flexible assertions** - Match patterns, not exact text
6. **Set temperature to 0** - More consistent outputs
7. **Retry flaky tests** - 2-3 retries with delays
8. **Monitor API spending** - Set alerts and limits
9. **Rotate test API keys** - Security best practice
10. **Document test coverage** - Which scenarios are mocked vs real

## Related

- [Custom Agents](/docs/agents/custom-agents) - Building domain-specific agents
- [Testing Guide](/docs/guides/testing) - General testing patterns for dotdo
- [Agent Testing Utilities](/docs/guides/testing/agent-tests) - Testing utilities reference
