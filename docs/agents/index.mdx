---
title: Agent System
description: AI workers with real identity. Not chatbots. Workers.
---

# Agent System

Named agents with real identity. When Tom reviews your PR, you see `@tom-do` commenting. Not "AI Assistant".

## Provider Setup

Before using agents, configure your LLM provider. Agents require API keys from at least one provider.

### Supported Providers

| Provider | Environment Variable | Models |
|----------|---------------------|--------|
| **Anthropic (Claude)** | `ANTHROPIC_API_KEY` | claude-opus-4, claude-sonnet-4, claude-haiku-3 |
| **OpenAI** | `OPENAI_API_KEY` | gpt-4o, gpt-4o-mini, o1-preview |
| **Vercel AI** | Various | Multi-provider via `@ai-sdk/*` |
| **Devin** | `DEVIN_API_KEY` | Autonomous coding sessions |
| **Vapi (Voice)** | `VAPI_API_KEY` | Voice agents with STT/TTS |

### Environment Configuration

Create a `.env` file in your project root:

```bash
# .env
# Choose at least one provider

# Anthropic Claude (recommended for coding tasks)
ANTHROPIC_API_KEY=sk-ant-api03-...

# OpenAI (recommended for general-purpose tasks)
OPENAI_API_KEY=sk-proj-...

# Optional: Devin for autonomous coding
DEVIN_API_KEY=dv_...

# Optional: Vapi for voice agents
VAPI_API_KEY=...
```

<Callout type="warning">
Never commit `.env` files to version control. Add `.env` to your `.gitignore`.
</Callout>

### Model Selection Guide

| Model | Best For | Cost (per 1M tokens) |
|-------|----------|---------------------|
| `claude-opus-4-20250514` | Complex reasoning, architecture decisions | $15 in / $75 out |
| `claude-sonnet-4-20250514` | Balanced performance, production workloads | $3 in / $15 out |
| `claude-3-haiku-20240307` | Fast, lightweight tasks, testing | $0.25 in / $1.25 out |
| `gpt-4o` | Multimodal, vision tasks | $2.50 in / $10 out |
| `gpt-4o-mini` | Fast, cost-effective | $0.15 in / $0.60 out |

<Callout type="info">
For production, start with Claude Sonnet or GPT-4o. Use Opus/o1 for complex reasoning. Use Haiku/mini for high-volume tasks.
</Callout>

### Rate Limits and Cost Considerations

Configure spending limits in your provider dashboards:

- **Anthropic Console**: Set usage limits per API key
- **OpenAI Platform**: Set usage limits per project

For development, consider:
- Use `maxSteps` to prevent runaway agent loops
- Set `temperature: 0` for more deterministic outputs
- Use cheaper models (Haiku, mini) for testing

---

## Your First Agent

Let's build a working agent from scratch. This example creates an agent that can answer questions about code.

### Step 1: Create a Provider

```typescript
import { createClaudeProvider } from 'agents/providers'

// Create a provider with your API key
const provider = createClaudeProvider({
  apiKey: process.env.ANTHROPIC_API_KEY,
  defaultModel: 'claude-sonnet-4-20250514',
})
```

### Step 2: Define Your Agent

```typescript
const codeReviewer = provider.createAgent({
  id: 'code-reviewer',
  name: 'Code Reviewer',
  instructions: `You are a code reviewer. Analyze code for:
    - Bugs and potential issues
    - Performance concerns
    - Best practice violations
    Be constructive and specific in your feedback.`,
  model: 'claude-sonnet-4-20250514',
})
```

### Step 3: Run the Agent

```typescript
const result = await codeReviewer.run({
  prompt: `Review this function:
    function add(a, b) {
      return a + b
    }`,
})

console.log(result.text)
// "This is a simple addition function. A few observations:
//  1. No type checking - consider adding TypeScript types
//  2. No input validation - might want to handle non-numbers
//  3. Function name is clear and follows conventions..."
```

### Complete Example

Here's a full working example you can run:

```typescript
// first-agent.ts
import { createClaudeProvider } from 'agents/providers'

async function main() {
  // 1. Create provider
  const provider = createClaudeProvider({
    apiKey: process.env.ANTHROPIC_API_KEY,
  })

  // 2. Create agent
  const agent = provider.createAgent({
    id: 'my-first-agent',
    name: 'Helper',
    instructions: 'You are a helpful assistant. Be concise.',
    model: 'claude-sonnet-4-20250514',
  })

  // 3. Run agent
  const result = await agent.run({
    prompt: 'What is the capital of France?',
  })

  // 4. See the response
  console.log('Response:', result.text)
  console.log('Steps:', result.steps)
  console.log('Tokens used:', result.usage)
}

main().catch(console.error)
```

Run it:
```bash
npx tsx first-agent.ts
```

### Expected Response Format

The `AgentResult` object contains:

```typescript
interface AgentResult {
  text: string           // The agent's response text
  toolCalls: ToolCall[]  // Tools the agent called
  toolResults: ToolResult[]  // Results from tool executions
  messages: Message[]    // Full conversation history
  steps: number          // Number of agent steps taken
  finishReason: string   // Why the agent stopped ('stop', 'tool_calls', 'max_tokens')
  usage: {
    promptTokens: number
    completionTokens: number
    totalTokens: number
  }
}
```

### Adding Tools

Give your agent capabilities:

```typescript
import { z } from 'zod'

const searchTool = {
  name: 'search',
  description: 'Search for information',
  inputSchema: z.object({
    query: z.string().describe('Search query'),
  }),
  execute: async ({ query }) => {
    // Your search implementation
    return { results: [`Result for: ${query}`] }
  },
}

const agent = provider.createAgent({
  id: 'search-agent',
  name: 'Searcher',
  instructions: 'Search for information when asked.',
  model: 'claude-sonnet-4-20250514',
  tools: [searchTool],
})

const result = await agent.run({
  prompt: 'Search for TypeScript best practices',
})

// See what tools were called
console.log('Tools called:', result.toolCalls)
console.log('Tool results:', result.toolResults)
```

---

## Debugging Agents

When agents don't behave as expected, use these debugging techniques.

### Seeing Tool Calls

The result object shows exactly what tools were invoked:

```typescript
const result = await agent.run({ prompt: 'Search and summarize' })

// Log all tool calls
for (const call of result.toolCalls) {
  console.log('Tool:', call.name)
  console.log('Arguments:', JSON.stringify(call.arguments, null, 2))
}

// Log tool results
for (const toolResult of result.toolResults) {
  console.log('Tool:', toolResult.toolName)
  console.log('Result:', toolResult.result)
  if (toolResult.error) {
    console.log('Error:', toolResult.error)
  }
}
```

### Using Agent Hooks for Observability

Add hooks to log every step of agent execution:

```typescript
const agent = provider.createAgent({
  id: 'debuggable-agent',
  name: 'Debuggable',
  instructions: 'You are a helpful assistant.',
  model: 'claude-sonnet-4-20250514',
  tools: [searchTool],
  hooks: {
    onStepStart: async (stepNumber, state) => {
      console.log(`[Step ${stepNumber}] Starting...`)
      console.log('Messages so far:', state.messages.length)
    },

    onPreToolUse: async (toolCall) => {
      console.log(`[Tool] Calling ${toolCall.name}`)
      console.log('Arguments:', toolCall.arguments)
      return { action: 'allow' }
    },

    onPostToolUse: async (toolCall, result) => {
      console.log(`[Tool] ${toolCall.name} completed`)
      if (result.error) {
        console.error('Error:', result.error)
      } else {
        console.log('Result:', result.result)
      }
    },

    onStepFinish: async (step, stepNumber) => {
      console.log(`[Step ${stepNumber}] Finished`)
      console.log('Text:', step.text?.slice(0, 100) + '...')
      console.log('Tool calls:', step.toolCalls?.length ?? 0)
    },
  },
})
```

### Debugging Failed Agent Tasks

When an agent fails, check these common issues:

**1. API Key Issues**
```typescript
try {
  const result = await agent.run({ prompt: 'Hello' })
} catch (error) {
  if (error.message.includes('401') || error.message.includes('authentication')) {
    console.error('Invalid API key. Check your ANTHROPIC_API_KEY or OPENAI_API_KEY.')
  }
}
```

**2. Rate Limits**
```typescript
try {
  const result = await agent.run({ prompt: 'Hello' })
} catch (error) {
  if (error.message.includes('429') || error.message.includes('rate limit')) {
    console.error('Rate limited. Wait and retry, or use a different API key.')
  }
}
```

**3. Runaway Loops**
```typescript
// Prevent infinite loops with maxSteps
const agent = provider.createAgent({
  id: 'safe-agent',
  name: 'Safe',
  instructions: 'Complete the task.',
  model: 'claude-sonnet-4-20250514',
  maxSteps: 10, // Stop after 10 steps max
})

const result = await agent.run({ prompt: 'Complex task' })
if (result.finishReason === 'max_steps') {
  console.warn('Agent hit max steps limit - task may be incomplete')
}
```

**4. Tool Execution Errors**
```typescript
// Check tool results for errors
const result = await agent.run({ prompt: 'Search something' })

const failedTools = result.toolResults.filter(r => r.error)
if (failedTools.length > 0) {
  console.error('Some tools failed:')
  for (const failed of failedTools) {
    console.error(`  ${failed.toolName}: ${failed.error}`)
  }
}
```

### Streaming for Real-time Debugging

Stream agent output to see what's happening in real-time:

```typescript
const stream = agent.stream({ prompt: 'Write a detailed analysis' })

for await (const event of stream) {
  switch (event.type) {
    case 'text-delta':
      process.stdout.write(event.data.textDelta)
      break

    case 'tool-call-start':
      console.log(`\n[Tool Starting] ${event.data.toolName}`)
      break

    case 'tool-call-end':
      console.log(`[Tool Complete] ${event.data.toolName}`)
      break

    case 'error':
      console.error('[Error]', event.data.error)
      break

    case 'done':
      console.log('\n[Done] Steps:', event.data.finalResult.steps)
      break
  }
}
```

### Full Conversation History

Access the complete message history for debugging:

```typescript
const result = await agent.run({ prompt: 'Multi-step task' })

console.log('Full conversation:')
for (const msg of result.messages) {
  console.log(`[${msg.role}]`, typeof msg.content === 'string'
    ? msg.content.slice(0, 100)
    : JSON.stringify(msg.content))
}
```

---

## Installation

The named agents are exported from `agents.do`, a path alias that points to the named agents module in the dotdo package. Configure this in your project:

<Tabs items={['tsconfig.json', 'package.json']}>
  <Tab value="tsconfig.json">
```json title="tsconfig.json"
{
  "compilerOptions": {
    "paths": {
      "agents.do": ["./node_modules/dotdo/dist/agents/named/index.js"],
      "humans.do": ["./node_modules/dotdo/dist/lib/humans/index.js"]
    }
  }
}
```
  </Tab>
  <Tab value="package.json">
```json title="package.json"
{
  "imports": {
    "agents.do": "dotdo/agents/named",
    "humans.do": "dotdo/lib/humans"
  }
}
```
  </Tab>
</Tabs>

<Callout type="info">
**Using `npx dotdo init`?** These aliases are configured automatically. You can start importing right away.
</Callout>

Once configured, import the named agents:

```typescript
import { priya, ralph, tom, mark, sally, quinn } from 'agents.do'

export class MyStartup extends Startup {
  async launch() {
    const spec = priya`define the MVP for ${this.hypothesis}`
    let app = ralph`build ${spec}`

    do {
      app = ralph`improve ${app} per ${tom}`
    } while (!await tom.approve(app))

    mark`announce the launch`
    sally`start selling`
  }
}
```

## They're Not Chatbots

They're workers.

Each agent:

| Identity | Example |
|----------|---------|
| GitHub account | `@tom-do` |
| Email address | `tom@agents.do` |
| Avatar | Consistent across all platforms |
| Audit trail | Full history of every action |

When Tom reviews your code, you get a real PR comment from `@tom-do`. When Mark sends an email, it comes from `mark@agents.do`. Every action is traceable.

## Template Literals are RPC

This isn't string interpolation:

```typescript
ralph`build the user dashboard`
```

It's an RPC call. The template literal syntax creates a proxy that:

1. Captures your natural language instruction
2. Sends it to the agent's execution context
3. Returns a promise (or runs fire-and-forget)

No method names. No parameters. Just say what you want.

## When to Await

```typescript
// Fire-and-forget: no await
mark`announce the launch`
sally`start selling`

// Need the result: await
const spec = await priya`what should we build next?`
const code = await ralph`build ${spec}`

// Approval flow: await
const approved = await tom.approve(pullRequest)
```

If you don't need the result, don't await. The agent works in the background.

## Pipeline Execution

Agents compose with promise pipelining. One network round trip, not N.

```typescript
// This is ONE batch operation
const sprint = await priya`plan the sprint`
  .map(issue => ralph`build ${issue}`)
  .map(code => tom`review ${code}`)
  .map(pr => tom`merge ${pr}`)
```

The `.map()` isn't JavaScript's array method. It's a **Magic Map** that records your callback, sends it to the server, and replays it for each result. All in one network round trip.

## Meet Your Team

| Agent | Role | Specialty |
|-------|------|-----------|
| [Priya](/docs/agents/named-agents#priya) | Product | specs, roadmaps, priorities |
| [Ralph](/docs/agents/named-agents#ralph) | Engineering | builds what you need |
| [Tom](/docs/agents/named-agents#tom) | Tech Lead | architecture, code review |
| [Mark](/docs/agents/named-agents#mark) | Marketing | copy, content, launches |
| [Sally](/docs/agents/named-agents#sally) | Sales | outreach, demos, closing |
| [Quinn](/docs/agents/named-agents#quinn) | QA | testing, edge cases, quality |

<Cards>
  <Card title="Agent Providers" href="/docs/agents/providers">
    Claude, OpenAI, Vercel AI, Devin, and Voice providers with unified interface.
  </Card>
  <Card title="Named Agents" href="/docs/agents/named-agents">
    Meet Priya, Ralph, Tom, Mark, Sally, and Quinn.
  </Card>
  <Card title="Custom Agents" href="/docs/agents/custom-agents">
    Build domain-specific agents for your industry.
  </Card>
  <Card title="Human Actors" href="/docs/humans/">
    When AI needs human judgment - escalation, approvals, and human-in-the-loop workflows.
  </Card>
</Cards>

## Human Escalation

AI does the work. Humans make the decisions.

```typescript
import { legal, ceo } from 'humans.do'

const approved = await ceo`approve the partnership`

const escalation = this.HumanFunction({
  trigger: 'refund > $10000',
  role: 'senior-accountant',
  sla: '4 hours',
})
```

Messages route to Slack, email, SMS. Full audit trail. When the decision requires human judgment, the workflow pauses until a human responds.

## Technical Foundation

Agents run on the same infrastructure as your business:

- **V8 Isolates** for 0ms cold starts
- **Durable Objects** for persistent state
- **Cap'n Web RPC** for promise pipelining
- **300+ edge locations** worldwide

An agent is not a chatbot wrapping an LLM. It's a durable compute primitive with identity, memory, and tools.

## Technical Details

For implementation details and architecture documentation:

- **[lib README](https://github.com/dotdo/dotdo/blob/main/lib/README.md)** - Overview of lib modules including humans/, channels/, executors/, ai/

### Key Implementation Directories

| Directory | Purpose |
|-----------|---------|
| `agents/` | Multi-provider agent SDK |
| `agents/Agent.ts` | Core agent class |
| `agents/Tool.ts` | Tool definitions |
| `agents/providers/` | OpenAI, Anthropic, Google providers |
| `agents/named/` | Priya, Ralph, Tom, Mark, Sally, Quinn implementations |
| `lib/ai/` | AI gateway and tool loop agent |
| `lib/executors/` | Code, Generative, Agentic, Human executors |

## Related

- [Agent Providers](/docs/agents/providers) - Claude, OpenAI, Vercel AI, Devin, and Voice providers
- [Agent Tools](/docs/agents/tools) - Give agents capabilities with the unified tool system
- [Named Agents](/docs/agents/named-agents) - Meet Priya, Ralph, Tom, Mark, Sally, and Quinn
- [Custom Agents](/docs/agents/custom-agents) - Build domain-specific agents
- [Concepts: Functions](/docs/concepts/functions) - Function types including agentic functions
- [Humans](/docs/humans) - Human-in-the-loop escalation and approvals
