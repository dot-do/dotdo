---
title: Error Handling and Resilience
description: Retry semantics, dead letter queues, circuit breakers, and error escalation patterns in dotdo
---

import { Callout } from 'fumadocs-ui/components/callout'

# Error Handling and Resilience

dotdo provides comprehensive error handling infrastructure with configurable retry policies, dead letter queues (DLQ), circuit breakers, and an escalation chain that progresses through capability tiers. This guide covers all aspects of building resilient workflows.

## Overview

The resilience architecture consists of five integrated components:

| Component | Purpose |
|-----------|---------|
| **Execution Modes** | Three durability levels with different error behavior |
| **Retry Policies** | Configurable exponential backoff with jitter |
| **Circuit Breakers** | Prevent cascade failures to external services |
| **Dead Letter Queues** | Capture failed events for later processing |
| **Escalation Engine** | Escalate errors through capability tiers |

```
┌──────────────────────────────────────────────────────────────────────────────┐
│                        ERROR HANDLING ARCHITECTURE                            │
├──────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  EXECUTION MODES                 ERROR CLASSIFICATION                        │
│  ───────────────                 ────────────────────                        │
│                                                                              │
│  ┌──────────────┐               ┌─────────────────────────────────────┐     │
│  │   $.send()   │ Fire-forget   │  Transient (429, 500-504, timeout)  │     │
│  │   $.try()    │ Single try    │         → Retry with backoff        │     │
│  │   $.do()     │ Durable       │                                     │     │
│  └──────┬───────┘               │  Permanent (400, 401, 403, 404)     │     │
│         │                       │         → Fail immediately          │     │
│         ▼                       │                                     │     │
│  ┌──────────────┐               │  Escalatable (complex, ambiguous)   │     │
│  │   RETRY      │               │         → Escalate to next tier     │     │
│  │   POLICY     │               └─────────────────────────────────────┘     │
│  │              │                                                            │
│  │ • maxAttempts│                                                            │
│  │ • backoff    │               ESCALATION CHAIN                             │
│  │ • jitter     │               ─────────────────                            │
│  └──────┬───────┘               ┌─────┐  ┌────────────┐  ┌─────────┐  ┌─────┐│
│         │                       │Code │─▶│ Generative │─▶│ Agentic │─▶│Human││
│         │ fails                 └─────┘  └────────────┘  └─────────┘  └─────┘│
│         ▼                                                                    │
│  ┌──────────────┐               CIRCUIT BREAKER                              │
│  │   CIRCUIT    │               ───────────────                              │
│  │   BREAKER    │               ┌──────────────────────────────────────┐    │
│  │              │               │  CLOSED ─▶ OPEN ─▶ HALF-OPEN ─▶ CLOSED   │    │
│  │ • threshold  │               │  (normal)  (block)  (probe)    (recovered)│    │
│  │ • cooldown   │               └──────────────────────────────────────┘    │
│  └──────┬───────┘                                                            │
│         │ exhausted                                                          │
│         ▼                                                                    │
│  ┌──────────────┐                                                            │
│  │     DLQ      │  Dead Letter Queue                                         │
│  │              │  • Stores failed events                                    │
│  │ • replay()   │  • Retry count tracking                                    │
│  │ • purge()    │  • Manual/automatic replay                                 │
│  └──────────────┘                                                            │
│                                                                              │
└──────────────────────────────────────────────────────────────────────────────┘
```

---

## Execution Modes

The WorkflowContext (`$`) provides three execution modes with different durability and error-handling characteristics.

### Comparison Table

| Mode | Blocking | Durable | Retries | Persistence | Use Case |
|------|----------|---------|---------|-------------|----------|
| `$.send()` | No | No | No | None | Fire-and-forget notifications |
| `$.try()` | Yes | No | No | None | Quick operations, fallible checks |
| `$.do()` | Yes | Yes | Yes | SQLite | Critical business operations |

### $.send() - Fire and Forget

Non-blocking event emission. Returns immediately without waiting for handler completion. Errors are logged but not propagated.

```typescript
// Best for: telemetry, analytics, non-critical notifications
$.send('Analytics.pageView', { userId, page })
$.send('Notification.email', { to: user.email, subject: 'Welcome!' })

// Errors are captured in callbacks, not thrown
const runtime = createWorkflowRuntime({
  onStepError: (stepId, error, attempt) => {
    console.warn(`Background step ${stepId} failed:`, error)
    // Log to observability system
  }
})
```

**Key characteristics:**
- Returns `undefined` immediately
- Handler executes in background via `setImmediate()`
- Errors invoke `onStepError` callback but do not throw
- No retries, no persistence

### $.try() - Single Attempt

Blocking execution with a single attempt. Errors propagate immediately to the caller.

```typescript
// Best for: optional operations, cache lookups, quick validations
try {
  const cached = await $.try('Cache.get', { key: orderId })
  if (cached) return cached
} catch (error) {
  // Cache miss or error - continue to primary source
}

const order = await $.try('Database.query', { id: orderId })
```

**Key characteristics:**
- Blocks until completion
- Single attempt - no retries
- Errors throw immediately
- No persistence - re-executes on workflow restart

### $.do() - Durable Execution

Blocking execution with retry logic and persistence. Completed steps replay from cache on workflow restart.

```typescript
// Best for: critical business operations, payment processing, external API calls
const charge = await $.do('Payment.charge', {
  customerId,
  amount: 99.99
}, {
  retry: {
    maxAttempts: 5,
    initialDelayMs: 1000,
    backoffMultiplier: 2
  },
  stepId: 'charge-customer-payment'
})

// If workflow restarts, this step replays from cache
// Payment is guaranteed exactly-once execution
```

**Key characteristics:**
- Blocks until completion or retry exhaustion
- Retries with configurable exponential backoff
- Persists results to SQLite for replay
- Throws `WorkflowStepError` after all retries exhausted

---

## Retry Policies

Retry policies control how `$.do()` handles transient failures with exponential backoff.

### Default Configuration

```typescript
const DEFAULT_RETRY_POLICY = {
  maxAttempts: 3,        // Total attempts (not retries)
  initialDelayMs: 1000,  // 1 second initial delay
  maxDelayMs: 30000,     // Cap at 30 seconds
  backoffMultiplier: 2,  // Double delay each attempt
  jitter: true           // Add 0-25% randomness
}
```

### Delay Calculation

The delay between attempts follows exponential backoff:

```
delay = min(initialDelay × multiplier^(attempt-1), maxDelay) × (1 + random × jitter)

Example with defaults (maxAttempts=3, initial=1000ms, multiplier=2):
  Attempt 1: Execute immediately
  Attempt 2: Wait 1000ms × 2^0 = 1000ms (plus jitter)
  Attempt 3: Wait 1000ms × 2^1 = 2000ms (plus jitter)
```

### Custom Retry Policies

Configure retry behavior per operation or globally:

```typescript
// Per-operation retry policy
const result = await $.do('ExternalAPI.call', payload, {
  retry: {
    maxAttempts: 5,
    initialDelayMs: 500,
    maxDelayMs: 60000,
    backoffMultiplier: 1.5,
    jitter: true
  }
})

// Global runtime configuration
const runtime = createWorkflowRuntime({
  retryPolicy: {
    maxAttempts: 5,
    initialDelayMs: 2000,
    maxDelayMs: 30000,
    backoffMultiplier: 2,
    jitter: true
  }
})
```

### Retry-After Header Support

When calling HTTP APIs, the retry system respects `Retry-After` headers:

```typescript
import { parseRetryAfter } from 'dotdo/compat/core/retry'

// Automatically handles Retry-After header
// Either as seconds: "Retry-After: 120"
// Or as HTTP date: "Retry-After: Wed, 15 Jan 2025 10:00:00 GMT"
const response = await retryHandler.fetch(url, options)
```

### Retryable Status Codes

By default, these HTTP status codes trigger retries:

| Code | Meaning | Retry Behavior |
|------|---------|----------------|
| 429 | Too Many Requests | Retry with Retry-After header |
| 500 | Internal Server Error | Retry with backoff |
| 502 | Bad Gateway | Retry with backoff |
| 503 | Service Unavailable | Retry with backoff |
| 504 | Gateway Timeout | Retry with backoff |

Non-retryable status codes (fail immediately):

| Code | Meaning | Behavior |
|------|---------|----------|
| 400 | Bad Request | Permanent failure |
| 401 | Unauthorized | Permanent failure |
| 403 | Forbidden | Permanent failure |
| 404 | Not Found | Permanent failure |
| 422 | Unprocessable Entity | Permanent failure |

---

## Circuit Breakers

Circuit breakers prevent cascade failures when external services are degraded. They "trip" after consecutive failures, rejecting requests immediately instead of waiting for timeouts.

### Circuit Breaker States

```
┌──────────────────────────────────────────────────────────────────────────────┐
│                         CIRCUIT BREAKER STATES                                │
├──────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌─────────────────┐          failures ≥ threshold          ┌─────────────┐ │
│  │     CLOSED      │ ───────────────────────────────────▶  │    OPEN     │ │
│  │    (normal)     │                                        │  (blocked)  │ │
│  │                 │                                        │             │ │
│  │ • Requests pass │                                        │ • Requests  │ │
│  │ • Track failures│                                        │   rejected  │ │
│  │ • Reset on      │          success ≥ threshold           │ • Wait for  │ │
│  │   success       │ ◀────────────────────────────────────  │   cooldown  │ │
│  └─────────────────┘                                        └──────┬──────┘ │
│          ▲                                                         │        │
│          │                                                         │        │
│          │                     ┌─────────────────┐                │        │
│          │                     │   HALF-OPEN     │                │        │
│          │                     │    (probe)      │                │        │
│          │                     │                 │ ◀──────────────┘        │
│          │  success            │ • Allow 1 req  │   cooldown elapsed      │
│          └─────────────────────│ • Test health  │                         │
│                                │ • Fail → OPEN  │                         │
│                                └─────────────────┘                         │
│                                                                              │
└──────────────────────────────────────────────────────────────────────────────┘
```

### Basic Usage

```typescript
import { CircuitBreaker } from 'dotdo/primitives/circuit-breaker'

const circuit = new CircuitBreaker({
  failureThreshold: 5,    // Open after 5 consecutive failures
  resetTimeout: 30000,    // Wait 30 seconds before half-open
  halfOpenRequests: 1     // Allow 1 probe request
})

async function callExternalAPI() {
  return circuit.execute(async () => {
    const response = await fetch('https://api.example.com/data')
    if (!response.ok) throw new Error(`HTTP ${response.status}`)
    return response.json()
  })
}

// Circuit state tracking
circuit.onStateChange((prev, current, stats) => {
  console.log(`Circuit: ${prev} → ${current}`, stats)

  if (current === 'open') {
    alertOps('Circuit opened - external API degraded')
  }
})
```

### Advanced Configuration

```typescript
const circuit = new CircuitBreaker({
  // Core settings
  failureThreshold: 5,
  resetTimeout: 30000,
  halfOpenRequests: 1,

  // Sliding window for failure rate calculation
  slidingWindow: {
    windowSize: 60000,   // 60 second window
    minRequests: 10      // Minimum requests before tripping
  },

  // Fallback when circuit is open
  fallback: {
    handler: () => ({ cached: true, data: getCachedData() }),
    timeout: 5000        // Fallback timeout
  },

  // Health checking for recovery
  healthCheck: {
    interval: 5000,
    checker: () => pingHealthEndpoint()
  },

  // Bulkhead pattern (concurrency limiting)
  bulkhead: {
    maxConcurrent: 10,
    queueSize: 50,
    queueTimeout: 5000
  },

  // Retry before recording failure
  retryPolicy: {
    maxRetries: 2,
    baseDelay: 100,
    maxDelay: 1000,
    backoffMultiplier: 2
  }
})
```

### Circuit Breaker Registry

Manage multiple circuits for different services:

```typescript
import { CircuitBreakerRegistry } from 'dotdo/primitives/circuit-breaker'

const registry = new CircuitBreakerRegistry()

// Get or create circuits by name
const paymentCircuit = registry.getOrCreate('stripe', {
  failureThreshold: 3,
  resetTimeout: 60000
})

const emailCircuit = registry.getOrCreate('sendgrid', {
  failureThreshold: 5,
  resetTimeout: 30000
})

// Monitor all circuits
const allStats = registry.getAllStats()
console.log('Circuit health:', allStats)

// Reset all circuits (e.g., after deployment)
registry.resetAll()
```

---

## Dead Letter Queue (DLQ)

When event handlers fail after all retries, events are moved to the Dead Letter Queue for later inspection and reprocessing.

### DLQ Entry Structure

```typescript
interface DLQEntry {
  id: string              // Unique DLQ entry ID
  eventId: string         // Original event ID
  verb: string            // Event verb (e.g., 'created', 'paid')
  source: string          // Event source (e.g., 'Customer/cust_123')
  data: Record<string, unknown>  // Original event payload
  error: string           // Error message
  errorStack?: string     // Stack trace for debugging
  retryCount: number      // Number of retry attempts
  maxRetries: number      // Configured maximum retries
  lastAttemptAt: Date     // Timestamp of last retry
  createdAt: Date         // When event first failed
}
```

### Accessing the DLQ

```typescript
// The DLQ is available as a store on DO instances
class MyWorkflow extends DO {
  async checkFailedEvents() {
    // List all DLQ entries
    const failed = await this.dlq.list()

    // Filter by verb or source
    const paymentFailures = await this.dlq.list({
      verb: 'charged',
      source: 'Payment'
    })

    // Get count
    const count = await this.dlq.count()
    console.log(`${count} events in DLQ`)

    // Get specific entry
    const entry = await this.dlq.get('dlq_abc123')
  }
}
```

### Replaying Failed Events

```typescript
class MyWorkflow extends DO {
  async reprocessFailures() {
    // Replay a single event
    const result = await this.dlq.replay('dlq_abc123')
    if (result.success) {
      console.log('Event reprocessed successfully')
    } else {
      console.log('Replay failed:', result.error)
    }

    // Replay all events matching a filter
    const batchResult = await this.dlq.replayAll({
      verb: 'charged',
      maxRetries: 3  // Only retry events under max
    })
    console.log(`Replayed: ${batchResult.replayed}, Failed: ${batchResult.failed}`)
  }
}
```

### DLQ Cleanup

```typescript
class MyWorkflow extends DO {
  async cleanupDLQ() {
    // Remove a processed entry
    await this.dlq.remove('dlq_abc123')

    // Purge entries that exceeded max retries
    // These are archived to cold storage before deletion
    const purged = await this.dlq.purgeExhausted()
    console.log(`Purged ${purged} exhausted entries`)
  }
}
```

### Event Handler Integration

Event handlers automatically integrate with the DLQ:

```typescript
class OrderWorkflow extends DO {
  async initialize() {
    // Handler that might fail
    $.on.Payment.charged(async (event) => {
      const receipt = await sendReceipt(event.data)
      await updateOrder(event.data.orderId, { receiptSent: true })
    }, {
      // Handler options
      maxRetries: 3,           // Retry up to 3 times
      name: 'send-receipt',    // Handler name for debugging
      priority: 10             // Higher priority = runs first
    })
  }
}

// If handler throws after maxRetries, event goes to DLQ
// DLQ entry includes:
// - Original event data
// - Error message and stack
// - Retry count
// - Handler name
```

---

## Error Escalation Engine

The escalation engine implements an error handling chain that progressively escalates through capability tiers: Code → Generative → Agentic → Human.

### Capability Tiers

| Tier | Description | Use Case |
|------|-------------|----------|
| **Code** | Deterministic code execution | Retry, fallback values |
| **Generative** | AI-powered code generation | Generate fixes, parse errors |
| **Agentic** | Autonomous AI agents | Complex problem solving |
| **Human** | Human operator intervention | Approval, manual resolution |

### Error Classification

Errors are classified to determine the appropriate handling strategy:

```typescript
import { classifyError } from 'dotdo/workflows/errors'

const classified = classifyError(error, {
  currentTier: 'code',
  context: { orderId, customerId }
})

console.log(classified)
// {
//   classification: 'transient',  // or 'permanent', 'escalatable', 'recoverable'
//   severity: 'medium',           // 'low', 'medium', 'high', 'critical'
//   retryable: true,
//   maxRetries: 3,
//   retryDelay: 1000,
//   currentTier: 'code',
//   nextTier: 'generative',       // if escalatable
//   reason: 'Transient error: ETIMEDOUT'
// }
```

**Classification Types:**

| Classification | Behavior | Examples |
|----------------|----------|----------|
| `transient` | Retry with backoff | Network timeout, 429, 503 |
| `permanent` | Fail immediately | 400, 401, 403, 404 |
| `escalatable` | Escalate to next tier | Complex logic failure, ambiguous input |
| `recoverable` | Limited retries | Rate limit with known reset |

### Using the Escalation Engine

```typescript
import { createEscalationEngine } from 'dotdo/workflows/errors'

const engine = createEscalationEngine({
  graph,
  db,
  maxRetries: 3,
  circuitBreaker: {
    failureThreshold: 5,
    cooldownPeriod: 30000
  },
  deadLetterHandler: async (error, chain) => {
    await notifyOps(error, chain)
    await dlq.add({ error, chain })
  },
  onEscalation: (event) => {
    metrics.record('escalation', event)
  }
})

// Escalate an error through the chain
const result = await engine.escalate(error, {
  startTier: 'code',
  handlers: {
    code: async (err) => {
      // Try automatic fix
      return tryAutoFix(err)
    },
    generative: async (err) => {
      // Use AI to generate a fix
      return await ai.generateFix(err)
    },
    agentic: async (err) => {
      // Deploy an agent to resolve
      return await agent.resolve(err)
    },
    human: async (err) => {
      // Create a ticket for human review
      return await createTicket(err)
    }
  },
  tierTimeout: 30000,   // 30s per tier
  globalTimeout: 120000 // 2min total
})

if (result.exhausted) {
  console.log('All tiers exhausted, sent to DLQ')
} else {
  console.log(`Resolved by ${result.resolvedByTier}:`, result.resolution)
}
```

### Escalation Metrics

```typescript
const metrics = await engine.getMetrics()
// {
//   totalEscalations: 150,
//   resolvedAtTier: {
//     code: 120,
//     generative: 20,
//     agentic: 8,
//     human: 2
//   },
//   exhaustedCount: 5,
//   averageEscalationDepth: 1.3,
//   averageDuration: 2500
// }
```

---

## Error Propagation in Pipelines

Promise pipelines handle errors differently based on the execution mode.

### Pipeline Error Behavior

```typescript
// Fire-and-forget: errors are swallowed
priya`generate spec for ${idea}`  // No await, error logged but not thrown

// Chained operations: error propagates at await
const spec = priya`generate spec`
const app = ralph`build ${spec}`    // Still a promise
const deployed = await tom`ship ${app}`  // Error throws here

// Pipeline promises support error handling
const result = await priya`generate spec`
  .catch(error => {
    // Handle error
    return fallbackSpec
  })
```

### Handling Pipeline Failures

```typescript
// Option 1: Try-catch at the await point
try {
  const app = await ralph`build ${spec}`
} catch (error) {
  if (error.code === 'BUILD_FAILED') {
    // Specific error handling
  }
  throw error  // Re-throw for escalation
}

// Option 2: Use error handlers on RPC promises
const result = await $.Customer(id)
  .charge()
  .catch(async (error) => {
    // Log error
    await $.log('Payment failed', { error, customerId: id })
    // Return fallback
    return { success: false, reason: error.message }
  })

// Option 3: Circuit breaker for external calls
const paymentCircuit = new CircuitBreaker({ failureThreshold: 3 })
const charge = await paymentCircuit.execute(() =>
  $.Payment(id).charge({ amount })
)
```

---

## Best Practices

### 1. Choose the Right Execution Mode

```typescript
// Telemetry - fire and forget
$.send('Analytics.track', { event: 'page_view', userId })

// Cache lookup - single attempt, failure is acceptable
const cached = await $.try('Cache.get', key).catch(() => null)

// Payment - must succeed, requires durability
await $.do('Payment.charge', { customerId, amount })
```

### 2. Configure Retry Policies Per Service

```typescript
// External APIs with rate limits: longer delays
const stripePolicy = { maxAttempts: 5, initialDelayMs: 2000 }

// Internal services: quick retries
const internalPolicy = { maxAttempts: 3, initialDelayMs: 100 }

// Database operations: moderate
const dbPolicy = { maxAttempts: 3, initialDelayMs: 500 }
```

### 3. Use Circuit Breakers for External Services

```typescript
// Wrap all external service calls
const externalCircuit = new CircuitBreaker({
  failureThreshold: 5,
  resetTimeout: 30000,
  fallback: {
    handler: () => getCachedResponse()
  }
})

async function callExternalService() {
  return externalCircuit.execute(() => fetch(externalUrl))
}
```

### 4. Monitor DLQ Size

```typescript
// Scheduled job to alert on DLQ growth
$.every.hour(async () => {
  const count = await this.dlq.count()
  if (count > 100) {
    await alertOps('DLQ size exceeds threshold', { count })
  }
})
```

### 5. Implement Idempotency

```typescript
// Use step IDs for idempotent operations
await $.do('Payment.charge', payload, {
  stepId: `charge-${orderId}-${amount}`  // Deterministic step ID
})

// On replay, returns cached result instead of re-charging
```

### 6. Log Error Context

```typescript
$.on.Order.created(async (event) => {
  try {
    await processOrder(event.data)
  } catch (error) {
    // Include context in error
    error.context = {
      orderId: event.data.id,
      customerId: event.data.customerId,
      step: 'processOrder'
    }
    throw error  // Error context preserved in DLQ
  }
}, { name: 'process-order' })
```

---

## Related

<Cards>
  <Card title="Persistence Layer" href="/docs/architecture/persistence">
    Write-ahead logging, checkpoints, and tiered storage.
  </Card>
  <Card title="Workflow Context" href="/docs/sdk/workflow-context">
    The $ DSL for events, scheduling, and execution modes.
  </Card>
  <Card title="Circuit Breaker Primitive" href="/docs/primitives/circuit-breaker">
    Detailed circuit breaker API reference.
  </Card>
  <Card title="Observability" href="/docs/observability/dashboards">
    Monitoring error rates, circuit states, and DLQ metrics.
  </Card>
</Cards>
