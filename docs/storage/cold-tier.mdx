---
title: Cold Tier - ClickHouse + R2 Archive
description: Analytics, aggregations, and time-series at pennies per TB
---

# Cold Tier: ClickHouse + R2 Archive

The cold tier is your analytics engine. ClickHouse queries petabytes of data stored in R2 at a fraction of traditional data warehouse costs.

## Performance Characteristics

| Metric | Value |
|--------|-------|
| Query latency | 100ms - 10s (query dependent) |
| Storage | Unlimited (R2 Archive) |
| Query engine | ClickHouse |
| Compression | 10-20x typical |
| Cost | Pennies per TB |

## Architecture

```
Warm Tier (Iceberg) → Materialized Views → ClickHouse → R2 Archive
                           ↓
                    Pre-aggregated tables
                           ↓
                    Dashboard queries
```

ClickHouse reads directly from R2, with materialized views for common aggregations.

## Pre-Aggregated Tables

Raw events roll up into aggregated tables:

```sql
-- Raw events (warm tier)
CREATE TABLE events (
  timestamp DateTime,
  event_type String,
  customer_id String,
  amount Decimal(18, 2),
  ...
)

-- Hourly aggregates (cold tier)
CREATE MATERIALIZED VIEW events_hourly
ENGINE = SummingMergeTree()
ORDER BY (event_type, toStartOfHour(timestamp))
AS SELECT
  toStartOfHour(timestamp) as hour,
  event_type,
  count() as event_count,
  sum(amount) as total_amount,
  uniq(customer_id) as unique_customers
FROM events
GROUP BY hour, event_type
```

Dashboard queries hit the aggregated table, not raw events.

## Query Examples

### Time-Series Analytics

```typescript
const dailyRevenue = await $.analytics(`
  SELECT
    toDate(hour) as day,
    sum(total_amount) as revenue,
    sum(unique_customers) as customers
  FROM events_hourly
  WHERE hour >= today() - 30
  GROUP BY day
  ORDER BY day
`)
```

### Funnel Analysis

```typescript
const funnel = await $.analytics(`
  SELECT
    countIf(event_type = 'page_view') as views,
    countIf(event_type = 'add_to_cart') as carts,
    countIf(event_type = 'checkout') as checkouts,
    countIf(event_type = 'purchase') as purchases
  FROM events
  WHERE timestamp >= today() - 7
`)
```

### Cohort Analysis

```typescript
const cohorts = await $.analytics(`
  WITH first_purchase AS (
    SELECT customer_id, min(toDate(timestamp)) as cohort_date
    FROM events
    WHERE event_type = 'purchase'
    GROUP BY customer_id
  )
  SELECT
    cohort_date,
    dateDiff('week', cohort_date, toDate(e.timestamp)) as weeks_since,
    uniq(e.customer_id) as active_users
  FROM events e
  JOIN first_purchase fp ON e.customer_id = fp.customer_id
  WHERE e.event_type = 'purchase'
  GROUP BY cohort_date, weeks_since
  ORDER BY cohort_date, weeks_since
`)
```

## R2 Archive Storage

Older data moves to R2 Archive for long-term retention:

| Storage Class | Use Case | Cost |
|--------------|----------|------|
| R2 Standard | Last 90 days | $0.015/GB-mo |
| R2 Infrequent | 90 days - 1 year | $0.01/GB-mo |
| R2 Archive | 1+ years | $0.002/GB-mo |

At $0.002/GB-mo, storing 10 TB of historical data costs $20/month.

## Cost Comparison

Traditional analytics stack vs dotdo cold tier:

| Component | Snowflake | BigQuery | dotdo |
|-----------|-----------|----------|-------|
| Storage (10 TB) | ~$230/mo | $200/mo | $20/mo |
| Compute | Credits (~$500/mo) | On-demand ($5/TB) | Included |
| Egress | $0.05-0.12/GB | $0.12/GB | **$0** |
| **Monthly Total** | ~$800+ | ~$300+ | **~$50** |

The savings compound. Query 100 TB across your dashboards? That is $12,000/month on BigQuery egress alone. On R2? Zero.

## Compression Ratios

ClickHouse's columnar compression is aggressive:

```
Event logs:     20x compression (1 TB → 50 GB)
Time-series:    15x compression (1 TB → 67 GB)
JSON documents: 10x compression (1 TB → 100 GB)
```

Combined with R2 Archive pricing, you store a year of analytics for the cost of a few days on legacy platforms.

## When to Use Cold Tier

Use the cold tier for:

- **Dashboards** - Pre-aggregated metrics, charts
- **Time-series** - Monitoring, observability, trends
- **Historical analytics** - Year-over-year comparisons
- **Large aggregations** - Counting, summing across billions of rows

Don't use it for:

- **Point lookups** - Use hot tier
- **Recent data queries** - Use warm tier (Iceberg has it already)
- **Real-time updates** - Cold tier has eventual consistency

## Data Retention

Configure automatic tiering:

```typescript
export const storage = {
  hot: {
    // Keep last 24 hours in DO SQLite
    retention: '24h'
  },
  warm: {
    // Keep 90 days in Iceberg
    retention: '90d'
  },
  cold: {
    // Archive everything older
    retention: 'forever',
    storageClass: 'archive' // $0.002/GB-mo
  }
}
```

Data flows automatically. You set policies, not pipelines.
